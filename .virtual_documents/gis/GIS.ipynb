!pip install rasterio geopandas fiona


import rasterio
from rasterio.mask import mask
import geopandas as gpd
import os

def clip_sentinel_data_with_mask(sentinel_dir, mask_shapefile, output_dir):
    """
    Clips all Sentinel-2 raster data within a specified directory
    using a given shapefile as a mask, handling CRS mismatches.

    Args:
        sentinel_dir (str): Path to the Sentinel-2 data directory (e.g., 'Sentinel2/GRANULE/L2A_T45QZG_A039169_20221222T043927/IMG_DATA').
        mask_shapefile (str): Path to the masking shapefile (e.g., 'ThanaArea.shp').
        output_dir (str): Directory where the clipped raster files will be saved.
    """

    os.makedirs(output_dir, exist_ok=True)

    # Load the masking shapefile once
    try:
        mask_gdf_original = gpd.read_file(mask_shapefile)
        if mask_gdf_original.empty:
            print(f"Warning: Mask shapefile '{mask_shapefile}' is empty. No clipping will be performed.")
            return
        print(f"Mask shapefile loaded with CRS: {mask_gdf_original.crs}")
    except Exception as e:
        print(f"Error loading mask shapefile '{mask_shapefile}': {e}")
        return

    band_resolutions = {
        'R10m': ['B02', 'B03', 'B04', 'B08', 'AOT', 'TCI', 'WVP'],
        'R20m': ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B11', 'B12', 'B8A', 'AOT', 'SCL', 'TCI', 'WVP'],
        'R60m': ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B09', 'B11', 'B12', 'B8A', 'AOT', 'SCL', 'TCI', 'WVP']
    }

    # Iterate through resolutions (R10m, R20m, R60m)
    for res_folder, bands in band_resolutions.items():
        current_res_dir = os.path.join(sentinel_dir, res_folder)
        if not os.path.exists(current_res_dir):
            print(f"Directory not found: {current_res_dir}. Skipping.")
            continue

        print(f"\nProcessing directory: {current_res_dir}")
        for band_file_name in os.listdir(current_res_dir):
            if band_file_name.endswith('.jp2') and any(band in band_file_name for band in bands):
                input_raster_path = os.path.join(current_res_dir, band_file_name)
                output_raster_path = os.path.join(output_dir, f"clipped_{band_file_name.replace('.jp2', '.tif')}") # Change output to .tif

                try:
                    with rasterio.open(input_raster_path) as src:
                        raster_crs = src.crs
                        print(f"  Raster: {band_file_name}, CRS: {raster_crs}")

                        # Reproject shapefile to raster's CRS if necessary, in memory
                        if raster_crs != mask_gdf_original.crs:
                            print(f"  CRS mismatch. Reprojecting mask to {raster_crs} in memory.")
                            mask_gdf_reprojected = mask_gdf_original.to_crs(raster_crs)
                            geometries_to_use = [geom for geom in mask_gdf_reprojected.geometry]
                        else:
                            geometries_to_use = [geom for geom in mask_gdf_original.geometry]

                        out_image, out_transform = mask(src, geometries_to_use, crop=True)

                        out_meta = src.meta.copy()
                        out_meta.update({
                            "driver": "GTiff",
                            "height": out_image.shape[1],
                            "width": out_image.shape[2],
                            "transform": out_transform,
                            "nodata": src.nodata # Carry over nodata value
                        })

                        with rasterio.open(output_raster_path, "w", **out_meta) as dest:
                            dest.write(out_image)
                        print(f"  Successfully clipped: {band_file_name}")

                except ValueError as e:
                    if "Input shapes do not overlap raster" in str(e):
                        print(f"  Skipping {band_file_name}: Input shapes do not overlap raster. This Sentinel-2 tile does not cover your ThanaArea.")
                    else:
                        print(f"  Error clipping {band_file_name}: {e}")
                except rasterio.errors.RasterioIOError as e:
                    print(f"  Error opening {band_file_name}: {e}")
                except Exception as e:
                    print(f"  An unexpected error occurred with {band_file_name}: {e}")

# Define your paths
SENTINEL_DATA_ROOT = 'Sentinel2/GRANULE/L2A_T45QZG_A039169_20221222T043927/IMG_DATA'
THANA_AREA_SHAPEFILE = 'ThanaArea.shp'
OUTPUT_CLIPPED_DIR = 'SentinelRaster' # A new directory for batch output

# Run the batch clipping function
clip_sentinel_data_with_mask(SENTINEL_DATA_ROOT, THANA_AREA_SHAPEFILE, OUTPUT_CLIPPED_DIR)


import os
import shutil

def organize_clipped_rasters_by_resolution(clipped_data_dir):
    """
    Organizes clipped Sentinel-2 raster files into resolution-specific subfolders.

    Args:
        clipped_data_dir (str): The directory containing all the clipped raster files.
                                 (e.g., 'Clipped_Sentinel_Data_Batch')
    """

    resolutions = ['10m', '20m', '60m']

    # Create resolution subfolders if they don't exist
    for res in resolutions:
        res_folder_path = os.path.join(clipped_data_dir, res)
        os.makedirs(res_folder_path, exist_ok=True)
        print(f"Ensured directory exists: {res_folder_path}")

    # Iterate through all files in the clipped data directory
    for filename in os.listdir(clipped_data_dir):
        if filename.startswith('clipped_T45QZG_') and filename.endswith('.tif'):
            source_path = os.path.join(clipped_data_dir, filename)

            # Determine the resolution based on filename pattern
            # Example: clipped_T45QZG_20221222T043211_B04_10m.tif
            # We need to find '_10m', '_20m', or '_60m'
            if '_10m.tif' in filename:
                target_res_folder = '10m'
            elif '_20m.tif' in filename:
                target_res_folder = '20m'
            elif '_60m.tif' in filename:
                target_res_folder = '60m'
            else:
                print(f"Could not determine resolution for {filename}. Skipping.")
                continue

            destination_path = os.path.join(clipped_data_dir, target_res_folder, filename)

            try:
                shutil.move(source_path, destination_path)
                print(f"Moved: {filename} to {target_res_folder}/")
            except Exception as e:
                print(f"Error moving {filename}: {e}")

    print("\nRaster organization complete.")

# Define the directory where your clipped data is located
CLIPPED_OUTPUT_DIRECTORY = 'SentinelRaster'

# Run the organization function
organize_clipped_rasters_by_resolution(CLIPPED_OUTPUT_DIRECTORY)


# Save this code as calculate_indices.py in your Five_Rivers/gis directory
# Make sure your current working directory is Five_Rivers/gis when you run it.

import rasterio
import numpy as np
import os

def calculate_and_save_index(band_dict, index_name, formula_func, output_dir, src_profile):
    """
    Calculates a single index and saves it as a GeoTIFF.

    Args:
        band_dict (dict): Dictionary with band names (e.g., 'B02') as keys
                          and their loaded numpy arrays as values.
        index_name (str): Name of the index (e.g., 'NDVI').
        formula_func (function): A lambda or function that takes band arrays
                                 and returns the calculated index array.
        output_dir (str): Directory to save the output.
        src_profile (dict): Rasterio profile (metadata) of the source band,
                            to be used for the output index raster.
    """
    try:
        index_array = formula_func(band_dict)
        # Ensure output is float32 and handle potential NaN values (e.g., from division by zero)
        index_array = index_array.astype(np.float32)
        index_array[np.isinf(index_array)] = np.nan # Replace inf with NaN

        output_path = os.path.join(output_dir, f"{index_name.lower()}.tif")

        # Update profile for the new index raster
        index_profile = src_profile.copy()
        index_profile.update({
            "dtype": 'float32',
            "count": 1, # Index is a single band
            "nodata": np.nan # Use NaN for no data
        })

        with rasterio.open(output_path, "w", **index_profile) as dst:
            dst.write(index_array, 1)
        print(f"  Successfully calculated and saved {index_name} to: {output_path}")

    except Exception as e:
        print(f"  Error calculating or saving {index_name}: {e}")


def calculate_10m_indices(clipped_10m_dir, output_indices_dir):
    """
    Calculates various common remote sensing indices from 10m Sentinel-2 bands.

    Args:
        clipped_10m_dir (str): Path to the directory containing clipped 10m bands.
                               (e.g., 'Clipped_Sentinel_Data_Batch/10m')
        output_indices_dir (str): Directory where the calculated indices will be saved.
    """
    print(f"Starting index calculation for 10m bands in: {clipped_10m_dir}")
    os.makedirs(output_indices_dir, exist_ok=True)

    band_files = {}
    band_data = {}
    src_profile = None # To store metadata from one of the bands

    # Load all required 10m bands
    for band_code in ['B02', 'B03', 'B04', 'B08']:
        # Construct filename pattern more accurately for clipped files
        filename_pattern = f"clipped_T45QZG_20221222T043211_{band_code}_10m.tif"
        band_path = os.path.join(clipped_10m_dir, filename_pattern)

        if not os.path.exists(band_path):
            print(f"  Warning: File not found for band {band_code} at {band_path}. Skipping indices requiring this band.")
            band_data[band_code] = None # Mark as missing
            continue

        print(f"  Loading {band_path}")

        try:
            with rasterio.open(band_path) as src:
                band_data[band_code] = src.read(1).astype(np.float32)
                # Sentinel-2 Level-2A data are typically scaled by 10000
                # Convert to reflectance [0-1] for index calculations
                band_data[band_code] /= 10000.0
                if src_profile is None:
                    src_profile = src.profile # Store profile from the first band loaded
        except Exception as e:
            print(f"  Error loading band {band_code} from {band_path}: {e}")
            band_data[band_code] = None # Mark as missing


    # --- Calculate Indices ---

    # NDVI
    if band_data.get('B08') is not None and band_data.get('B04') is not None:
        calculate_and_save_index(
            band_data, 'NDVI',
            lambda b: (b['B08'] - b['B04']) / (b['B08'] + b['B04']),
            output_indices_dir, src_profile
        )
    else:
        print("  Cannot calculate NDVI: Missing B08 or B04.")

    # NDWI (McFeeters)
    if band_data.get('B03') is not None and band_data.get('B08') is not None:
        calculate_and_save_index(
            band_data, 'NDWI',
            lambda b: (b['B03'] - b['B08']) / (b['B03'] + b['B08']),
            output_indices_dir, src_profile
        )
    else:
        print("  Cannot calculate NDWI: Missing B03 or B08.")

    # GNDVI
    if band_data.get('B08') is not None and band_data.get('B03') is not None:
        calculate_and_save_index(
            band_data, 'GNDVI',
            lambda b: (b['B08'] - b['B03']) / (b['B08'] + b['B03']),
            output_indices_dir, src_profile
        )
    else:
        print("  Cannot calculate GNDVI: Missing B08 or B03.")

    # EVI
    # Check for all required bands (B08, B04, B02)
    if (band_data.get('B08') is not None and
        band_data.get('B04') is not None and
        band_data.get('B02') is not None):
        calculate_and_save_index(
            band_data, 'EVI',
            lambda b: 2.5 * (b['B08'] - b['B04']) / (b['B08'] + 6 * b['B04'] - 7.5 * b['B02'] + 1),
            output_indices_dir, src_profile
        )
    else:
        print("  Cannot calculate EVI: Missing B08, B04, or B02.")

    # SAVI (using L=0.5)
    L_savi = 0.5 # Common value for L
    if band_data.get('B08') is not None and band_data.get('B04') is not None:
        calculate_and_save_index(
            band_data, 'SAVI',
            lambda b: (1 + L_savi) * (b['B08'] - b['B04']) / (b['B08'] + b['B04'] + L_savi),
            output_indices_dir, src_profile
        )
    else:
        print("  Cannot calculate SAVI: Missing B08 or B04.")

    print("\nAll 10m index calculations attempted.")


# --- Main execution block ---
if __name__ == "__main__":
    # Ensure these paths match your actual directory structure
    # They should be relative to where you run the script (e.g., Five_Rivers/gis)
    CLIPPED_10M_DIR = 'SentinelRaster/10m'
    OUTPUT_INDICES_DIR = 'SentinelRaster/Indices_10m'

    if not os.path.exists(CLIPPED_10M_DIR):
        print(f"Error: 10m clipped bands directory not found: {CLIPPED_10M_DIR}")
        print("Please ensure you have run the clipping and organization steps correctly.")
    else:
        calculate_10m_indices(CLIPPED_10M_DIR, OUTPUT_INDICES_DIR)


import rasterio
from rasterio.enums import Resampling
from rasterio.warp import reproject # Explicitly import reproject
import numpy as np
import os
import shutil # Import shutil at the top level

def resample_band(input_path, output_path, target_resolution, target_transform, target_crs, target_shape):
    """Resamples a raster band to a target resolution, transform, and shape."""
    with rasterio.open(input_path) as src:
        # Read the band data and scale it immediately for consistency (0-1 reflectance)
        # Convert to float32 early to avoid potential dtype issues later
        source_array = src.read(1).astype(np.float32) / 10000.0

        # Check if resampling is actually needed
        # Note: Comparison of transforms should be done carefully,
        # here we rely on resolution match and a direct transform comparison which might be strict.
        # For simplicity, if resolution matches, we'll just save it, otherwise resample.
        if src.res == (target_resolution, target_resolution):
            print(f"    Band {os.path.basename(input_path)} already at target resolution ({target_resolution}m). Copying and scaling...")
            profile = src.profile.copy()
            profile.update({
                "dtype": 'float32', # Ensure output dtype is float32
                "nodata": np.nan    # Use NaN for no data
            })
            with rasterio.open(output_path, 'w', **profile) as dst:
                dst.write(source_array, 1) # Write the scaled array
            return source_array, profile # Return scaled data and profile

        print(f"    Resampling {os.path.basename(input_path)} from {src.res} to {target_resolution}...")
        
        # Prepare destination array explicitly as float32, as the final indices will be float32
        destination_array = np.empty(target_shape, dtype=np.float32)

        # Reproject and resample - source should be the actual scaled numpy array and its transform
        reproject(
            source=source_array, # Pass the actual scaled numpy array
            destination=destination_array,
            src_transform=src.transform,
            src_crs=src.crs,
            dst_transform=target_transform,
            dst_crs=target_crs,
            resampling=Resampling.bilinear # Bilinear is good for continuous data like spectral bands
        )

        # Update profile for the new output
        profile = src.profile.copy()
        profile.update({
            'height': target_shape[0],
            'width': target_shape[1],
            'transform': target_transform,
            'crs': target_crs,
            'dtype': 'float32', # Ensure the output GeoTIFF's dtype is float32
            'nodata': np.nan,    # Explicitly set nodata for float
            'driver': 'GTiff'
        })

        with rasterio.open(output_path, 'w', **profile) as dst:
            dst.write(destination_array, 1)

        return destination_array, profile

def calculate_and_save_index(band_arrays, index_name, formula_func, output_dir, reference_profile):
    """
    Calculates a single index and saves it as a GeoTIFF.
    Assumes band_arrays are already scaled to 0-1 reflectance.
    """
    try:
        index_array = formula_func(band_arrays)
        # Ensure output is float32 and handle potential NaN/Inf values
        index_array = index_array.astype(np.float32)
        index_array[np.isinf(index_array)] = np.nan # Replace inf with NaN

        output_path = os.path.join(output_dir, f"{index_name.lower()}.tif")

        # Update profile for the new index raster
        index_profile = reference_profile.copy()
        index_profile.update({
            "dtype": 'float32',
            "count": 1, # Index is a single band
            "nodata": np.nan # Use NaN for no data
        })

        with rasterio.open(output_path, "w", **index_profile) as dst:
            dst.write(index_array, 1)
        print(f"  Successfully calculated and saved {index_name} to: {output_path}")

    except Exception as e:
        print(f"  Error calculating or saving {index_name}: {e}")

def calculate_all_new_indices(base_clipped_dir, output_indices_dir):
    """
    Calculates various common remote sensing indices from 10m and resampled 20m Sentinel-2 bands.
    """
    print(f"Starting calculation of additional indices.")
    os.makedirs(output_indices_dir, exist_ok=True)

    bands_10m_dir = os.path.join(base_clipped_dir, '10m')
    bands_20m_dir = os.path.join(base_clipped_dir, '20m')

    if not os.path.exists(bands_10m_dir) or not os.path.exists(bands_20m_dir):
        print(f"Error: Required band directories (10m, 20m) not found under {base_clipped_dir}.")
        print("Please ensure you have run the clipping and organization steps correctly.")
        return

    loaded_bands = {}
    reference_profile = None # Will store profile from a 10m band to use for all outputs

    # --- Step 1: Load 10m bands and get reference profile ---
    required_10m_bands = ['B02', 'B03', 'B04', 'B08']
    for band_code in required_10m_bands:
        # Improved filename search to be more robust
        found_files = [f for f in os.listdir(bands_10m_dir) if f.startswith('clipped_T45QZG_') and f'_{band_code}_10m.tif' in f]
        if not found_files:
            print(f"  Error: No file found for band {band_code} in {bands_10m_dir}. Exiting.")
            return # Exit if crucial 10m bands are missing

        band_path = os.path.join(bands_10m_dir, found_files[0]) # Take the first match
        try:
            with rasterio.open(band_path) as src:
                loaded_bands[band_code] = src.read(1).astype(np.float32) / 10000.0 # Scale to reflectance
                if reference_profile is None:
                    reference_profile = src.profile.copy() # Store a copy of the 10m band's profile
                    # Remove nodata from reference_profile if it's not applicable to all bands equally
                    # or if we're setting new nodata later. For now, keep it as it's part of the base profile.
                    print(f"  Reference 10m profile established from {band_code}.")
        except Exception as e:
            print(f"  Error loading 10m band {band_code} from {band_path}: {e}")
            return # Exit if crucial 10m bands cannot be loaded

    # --- Step 2: Resample and Load 20m bands ---
    required_20m_bands = ['B11', 'B12'] # Core SWIR bands for these indices
    
    # Ensure a reference_profile was successfully obtained
    if reference_profile is None:
        print("  Could not establish a reference 10m profile. Cannot proceed with resampling.")
        return

    for band_code in required_20m_bands:
        # Improved filename search for 20m bands
        found_files = [f for f in os.listdir(bands_20m_dir) if f.startswith('clipped_T45QZG_') and f'_{band_code}_20m.tif' in f]
        if not found_files:
            print(f"  Error: No file found for band {band_code} in {bands_20m_dir}. Skipping indices requiring this band.")
            loaded_bands[band_code] = None # Mark as missing
            continue

        band_path = os.path.join(bands_20m_dir, found_files[0]) # Take the first match
        temp_resampled_path = os.path.join(output_indices_dir, f"resampled_{band_code}_10m_temp.tif")

        try:
            # Resample and get data array and its profile
            resampled_data, _ = resample_band( # We don't need the profile returned by resample_band here
                band_path,
                temp_resampled_path,
                target_resolution=reference_profile['transform'].a, # Get resolution from 10m band transform
                target_transform=reference_profile['transform'],
                target_crs=reference_profile['crs'],
                target_shape=(reference_profile['height'], reference_profile['width'])
            )
            loaded_bands[band_code] = resampled_data
            # You might want to remove temp_resampled_path here if you don't need them after loading
            # os.remove(temp_resampled_path)
        except Exception as e:
            print(f"  Error resampling or loading 20m band {band_code} from {band_path}: {e}")
            loaded_bands[band_code] = None # Mark as missing if error occurs


    # --- Step 3: Calculate New Indices ---
    # The lambda functions expect 'b' to be the dictionary of loaded_bands
    
    # MNDWI (Modified Normalized Difference Water Index)
    if loaded_bands.get('B03') is not None and loaded_bands.get('B11') is not None:
        calculate_and_save_index(
            loaded_bands, 'MNDWI',
            lambda b: (b['B03'] - b['B11']) / (b['B03'] + b['B11']),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate MNDWI: Missing B03 or B11.")

    # NDBI (Normalized Difference Built-up Index)
    if loaded_bands.get('B11') is not None and loaded_bands.get('B08') is not None:
        calculate_and_save_index(
            loaded_bands, 'NDBI',
            lambda b: (b['B11'] - b['B08']) / (b['B11'] + b['B08']),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate NDBI: Missing B11 or B08.")

    # NDBSI (Normalized Difference Bare Soil Index)
    if (loaded_bands.get('B11') is not None and loaded_bands.get('B04') is not None and
        loaded_bands.get('B08') is not None and loaded_bands.get('B02') is not None):
        calculate_and_save_index(
            loaded_bands, 'NDBSI',
            lambda b: ((b['B11'] + b['B04']) - (b['B08'] + b['B02'])) / ((b['B11'] + b['B04']) + (b['B08'] + b['B02'])),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate NDBSI: Missing B11, B04, B08, or B02.")

    # AWEI_NoShadow (Automated Water Extraction Index - No Shadow)
    if (loaded_bands.get('B02') is not None and loaded_bands.get('B03') is not None and
        loaded_bands.get('B08') is not None and loaded_bands.get('B11') is not None):
        calculate_and_save_index(
            loaded_bands, 'AWEI_NoShadow',
            lambda b: b['B02'] + 2.5 * b['B03'] - 1.5 * (b['B08'] + b['B11']),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate AWEI_NoShadow: Missing B02, B03, B08, or B11.")

    # NDSI (Normalized Difference Soil Index - common variant using Green and SWIR1)
    if loaded_bands.get('B11') is not None and loaded_bands.get('B03') is not None:
        calculate_and_save_index(
            loaded_bands, 'NDSI',
            lambda b: (b['B11'] - b['B03']) / (b['B11'] + b['B03']),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate NDSI: Missing B11 or B03.")

    print("\nAll new index calculations attempted.")


# --- Main execution block ---
if __name__ == "__main__":
    # Ensure this path points to your 'Clipped_Sentinel_Data_Batch' directory
    # where your '10m' and '20m' subfolders are located.
    # Based on your directory structure, it seems your 'Clipped_Sentinel_Data_Batch'
    # is actually named 'SentinelRaster' at the root of your project.
    CLIPPED_BASE_DIR = 'SentinelRaster' # Adjusted this path based on your provided structure
    
    # New directory to store all calculated 10m indices (including these new ones)
    OUTPUT_ALL_INDICES_DIR = os.path.join(CLIPPED_BASE_DIR, 'All_Calculated_Indices_10m')

    # To ensure the SentinelRaster/All_Calculated_Indices_10m directory exists correctly
    # If it was created directly under Clipped_Sentinel_Data_Batch before, it would be:
    # Clipped_Sentinel_Data_Batch/Indices_10m (old)
    # Clipped_Sentinel_Data_Batch/All_Calculated_Indices_10m (new target)
    # Your last output `SentinelRaster/SentinelRaster/All_Calculated_Indices_10m` indicates
    # `OUTPUT_ALL_INDICES_DIR` might be built on the *wrong* `CLIPPED_BASE_DIR` in the main block.
    # Let's fix that.

    # Correct path for where the *new* indices will be saved relative to your execution directory:
    # This should result in Five_Rivers/gis/SentinelRaster/All_Calculated_Indices_10m
    ACTUAL_OUTPUT_DIR = os.path.join(os.getcwd(), 'SentinelRaster', 'All_Calculated_Indices_10m')


    calculate_all_new_indices(CLIPPED_BASE_DIR, ACTUAL_OUTPUT_DIR)

    print(f"\nCheck the '{ACTUAL_OUTPUT_DIR}' folder for your new index files.")


import rasterio
from rasterio.enums import Resampling
from rasterio.warp import reproject # Explicitly import reproject
import numpy as np
import os
import shutil # Import shutil at the top level

def resample_band(input_path, output_path, target_resolution, target_transform, target_crs, target_shape):
    """Resamples a raster band to a target resolution, transform, and shape."""
    with rasterio.open(input_path) as src:
        # Read the band data and scale it immediately for consistency (0-1 reflectance)
        # Convert to float32 early to avoid potential dtype issues later
        source_array = src.read(1).astype(np.float32) / 10000.0

        # Check if resampling is actually needed
        if src.res == (target_resolution, target_resolution): # Simplified check for resolution match
            print(f"    Band {os.path.basename(input_path)} already at target resolution ({target_resolution}m). Copying and scaling...")
            profile = src.profile.copy()
            profile.update({
                "dtype": 'float32', # Ensure output dtype is float32
                "nodata": np.nan    # Use NaN for no data
            })
            with rasterio.open(output_path, 'w', **profile) as dst:
                dst.write(source_array, 1) # Write the scaled array
            return source_array, profile # Return scaled data and profile

        print(f"    Resampling {os.path.basename(input_path)} from {src.res} to {target_resolution}m...")
        
        # Prepare destination array explicitly as float32, as the final indices will be float32
        destination_array = np.empty(target_shape, dtype=np.float32)

        # Reproject and resample - source should be the actual scaled numpy array and its transform
        reproject(
            source=source_array, # Pass the actual scaled numpy array
            destination=destination_array,
            src_transform=src.transform,
            src_crs=src.crs,
            dst_transform=target_transform,
            dst_crs=target_crs,
            resampling=Resampling.bilinear # Bilinear is good for continuous data like spectral bands
        )

        # Update profile for the new output
        profile = src.profile.copy()
        profile.update({
            'height': target_shape[0],
            'width': target_shape[1],
            'transform': target_transform,
            'crs': target_crs,
            'dtype': 'float32', # Ensure the output GeoTIFF's dtype is float32
            'nodata': np.nan,    # Explicitly set nodata for float
            'driver': 'GTiff'
        })

        with rasterio.open(output_path, 'w', **profile) as dst:
            dst.write(destination_array, 1)

        return destination_array, profile

def calculate_and_save_index(band_arrays, index_name, formula_func, output_dir, reference_profile):
    """
    Calculates a single index and saves it as a GeoTIFF.
    Assumes band_arrays are already scaled to 0-1 reflectance.
    """
    try:
        index_array = formula_func(band_arrays)
        # Ensure output is float32 and handle potential NaN/Inf values
        index_array = index_array.astype(np.float32)
        index_array[np.isinf(index_array)] = np.nan # Replace inf with NaN

        output_path = os.path.join(output_dir, f"{index_name.lower()}.tif")

        # Update profile for the new index raster
        index_profile = reference_profile.copy()
        index_profile.update({
            "dtype": 'float32',
            "count": 1, # Index is a single band
            "nodata": np.nan # Use NaN for no data
        })

        with rasterio.open(output_path, "w", **index_profile) as dst:
            dst.write(index_array, 1)
        print(f"  Successfully calculated and saved {index_name} to: {output_path}")
        return index_array # Return the calculated array for further use

    except Exception as e:
        print(f"  Error calculating or saving {index_name}: {e}")
        return None # Return None if calculation fails

def calculate_all_new_indices(base_clipped_dir, output_indices_dir):
    """
    Calculates various common remote sensing indices from 10m and resampled 20m Sentinel-2 bands.
    """
    print(f"Starting calculation of additional indices.")
    os.makedirs(output_indices_dir, exist_ok=True)

    bands_10m_dir = os.path.join(base_clipped_dir, '10m')
    bands_20m_dir = os.path.join(base_clipped_dir, '20m')

    if not os.path.exists(bands_10m_dir) or not os.path.exists(bands_20m_dir):
        print(f"Error: Required band directories (10m, 20m) not found under {base_clipped_dir}.")
        print("Please ensure you have run the clipping and organization steps correctly.")
        return

    loaded_bands = {}
    reference_profile = None # Will store profile from a 10m band to use for all outputs

    # --- Step 1: Load 10m bands and get reference profile ---
    required_10m_bands = ['B02', 'B03', 'B04', 'B08']
    for band_code in required_10m_bands:
        found_files = [f for f in os.listdir(bands_10m_dir) if f.startswith('clipped_T45QZG_') and f'_{band_code}_10m.tif' in f]
        if not found_files:
            print(f"  Error: No file found for band {band_code} in {bands_10m_dir}. Exiting.")
            return

        band_path = os.path.join(bands_10m_dir, found_files[0])
        try:
            with rasterio.open(band_path) as src:
                loaded_bands[band_code] = src.read(1).astype(np.float32) / 10000.0
                if reference_profile is None:
                    reference_profile = src.profile.copy()
                    print(f"  Reference 10m profile established from {band_code}.")
        except Exception as e:
            print(f"  Error loading 10m band {band_code} from {band_path}: {e}")
            return

    # --- Step 2: Resample and Load 20m bands ---
    required_20m_bands = ['B11', 'B12'] # Core SWIR bands for these indices
    
    if reference_profile is None:
        print("  Could not establish a reference 10m profile. Cannot proceed with resampling.")
        return

    for band_code in required_20m_bands:
        found_files = [f for f in os.listdir(bands_20m_dir) if f.startswith('clipped_T45QZG_') and f'_{band_code}_20m.tif' in f]
        if not found_files:
            print(f"  Error: No file found for band {band_code} in {bands_20m_dir}. Skipping indices requiring this band.")
            loaded_bands[band_code] = None
            continue

        band_path = os.path.join(bands_20m_dir, found_files[0])
        temp_resampled_path = os.path.join(output_indices_dir, f"resampled_{band_code}_10m_temp.tif")

        try:
            resampled_data, _ = resample_band(
                band_path,
                temp_resampled_path,
                target_resolution=reference_profile['transform'].a,
                target_transform=reference_profile['transform'],
                target_crs=reference_profile['crs'],
                target_shape=(reference_profile['height'], reference_profile['width'])
            )
            loaded_bands[band_code] = resampled_data
        except Exception as e:
            print(f"  Error resampling or loading 20m band {band_code} from {band_path}: {e}")
            loaded_bands[band_code] = None


    # --- Step 3: Calculate Indices (ensure dependencies are met) ---

    # NDVI (re-calculate to ensure it's in loaded_bands for BUI)
    if loaded_bands.get('B08') is not None and loaded_bands.get('B04') is not None:
        ndvi_array = calculate_and_save_index(
            loaded_bands, 'NDVI',
            lambda b: (b['B08'] - b['B04']) / (b['B08'] + b['B04']),
            output_indices_dir, reference_profile
        )
        if ndvi_array is not None:
            loaded_bands['NDVI'] = ndvi_array # Store the calculated NDVI array
    else:
        print("  Cannot calculate NDVI: Missing B08 or B04.")

    # MNDWI (Modified Normalized Difference Water Index)
    if loaded_bands.get('B03') is not None and loaded_bands.get('B11') is not None:
        calculate_and_save_index(
            loaded_bands, 'MNDWI',
            lambda b: (b['B03'] - b['B11']) / (b['B03'] + b['B11']),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate MNDWI: Missing B03 or B11.")

    # NDBI (Normalized Difference Built-up Index)
    if loaded_bands.get('B11') is not None and loaded_bands.get('B08') is not None:
        ndbi_array = calculate_and_save_index(
            loaded_bands, 'NDBI',
            lambda b: (b['B11'] - b['B08']) / (b['B11'] + b['B08']),
            output_indices_dir, reference_profile
        )
        if ndbi_array is not None:
            loaded_bands['NDBI'] = ndbi_array # Store the calculated NDBI array
    else:
        print("  Cannot calculate NDBI: Missing B11 or B08.")

    # UI (Urban Index)
    if loaded_bands.get('B12') is not None and loaded_bands.get('B08') is not None:
        calculate_and_save_index(
            loaded_bands, 'UI',
            lambda b: (b['B12'] - b['B08']) / (b['B12'] + b['B08']),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate UI: Missing B12 or B08.")

    # NDBSI (Normalized Difference Bare Soil Index)
    if (loaded_bands.get('B11') is not None and loaded_bands.get('B04') is not None and
        loaded_bands.get('B08') is not None and loaded_bands.get('B02') is not None):
        calculate_and_save_index(
            loaded_bands, 'NDBSI',
            lambda b: ((b['B11'] + b['B04']) - (b['B08'] + b['B02'])) / ((b['B11'] + b['B04']) + (b['B08'] + b['B02'])),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate NDBSI: Missing B11, B04, B08, or B02.")

    # AWEI_NoShadow (Automated Water Extraction Index - No Shadow)
    if (loaded_bands.get('B02') is not None and loaded_bands.get('B03') is not None and
        loaded_bands.get('B08') is not None and loaded_bands.get('B11') is not None):
        calculate_and_save_index(
            loaded_bands, 'AWEI_NoShadow',
            lambda b: b['B02'] + 2.5 * b['B03'] - 1.5 * (b['B08'] + b['B11']),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate AWEI_NoShadow: Missing B02, B03, B08, or B11.")

    # NDSI (Normalized Difference Soil Index - common variant using Green and SWIR1)
    if loaded_bands.get('B11') is not None and loaded_bands.get('B03') is not None:
        calculate_and_save_index(
            loaded_bands, 'NDSI',
            lambda b: (b['B11'] - b['B03']) / (b['B11'] + b['B03']),
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate NDSI: Missing B11 or B03.")

    # BUI (Built-up Index) - Calculated LAST as it depends on NDBI and NDVI
    if loaded_bands.get('NDBI') is not None and loaded_bands.get('NDVI') is not None:
        calculate_and_save_index(
            loaded_bands, 'BUI',
            lambda b: b['NDBI'] - b['NDVI'],
            output_indices_dir, reference_profile
        )
    else:
        print("  Cannot calculate BUI: Missing NDBI or NDVI (ensure they were calculated first).")


    print("\nAll new index calculations attempted.")


# --- Main execution block ---
if __name__ == "__main__":
    # Ensure this path points to your 'SentinelRaster' directory
    # which contains '10m', '20m', and '60m' subfolders.
    CLIPPED_BASE_DIR = 'SentinelRaster' # Based on your provided directory structure
    
    # This will create a directory at Five_Rivers/gis/SentinelRaster/All_Calculated_Indices_10m
    ACTUAL_OUTPUT_DIR = os.path.join(os.getcwd(), 'SentinelRaster', 'All_Calculated_Indices_10m')

    calculate_all_new_indices(CLIPPED_BASE_DIR, ACTUAL_OUTPUT_DIR)

    print(f"\nCheck the '{ACTUAL_OUTPUT_DIR}' folder for all your 10m-resolution index files (including NDVI, NDBI, BUI, UI, etc.).")


import rasterio
import geopandas as gpd
from pysheds.grid import Grid
import numpy as np
import pandas as pd
import networkx as nx
from shapely.geometry import Point, LineString
from shapely.ops import nearest_points as shapely_nearest_points # For robust snapping

# --- Configuration ---
# File paths (UPDATE THESE TO YOUR ACTUAL FILE PATHS)
DEM_PATH = 'SentinelRaster/Indices/DEMF.tif'
SAMPLING_POINTS_PATH = 'sampling_point.shp'
BRICK_FIELDS_PATH = 'brick_field_point.shp'
INDUSTRIES_PATH = 'industry_point.shp'

# Hydrological parameters
STREAM_ACCUMULATION_THRESHOLD = 500 # Adjust this based on your DEM and desired stream density
                                    # Higher value = fewer, larger streams
# --- End Configuration ---

print("Starting data generation for hydrological and graph-based features...")

# --- 1. Load Data ---
print("\n1. Loading geospatial data...")
try:
    # Load DEM with pysheds
    grid = Grid.from_raster(DEM_PATH)
    dem = grid.read_raster(DEM_PATH) # Get the raw DEM data

    # Load point GeoDataFrames
    sampling_points = gpd.read_file(SAMPLING_POINTS_PATH)
    brick_fields = gpd.read_file(BRICK_FIELDS_PATH)
    industries = gpd.read_file(INDUSTRIES_PATH)

    # --- CRS Check and Reprojection (CRITICAL) ---
    # Get CRS from DEM directly using rasterio
    with rasterio.open(DEM_PATH) as src:
        dem_crs = src.crs
    print(f"DEM CRS: {dem_crs.to_string()}") # Use .to_string() for a common representation

    if dem_crs.is_projected:
        print("This DEM uses a Projected Coordinate System (PCS).")
        print(f"Units are typically linear (e.g., meters or feet) for PCS.")
    elif dem_crs.is_geographic:
        print("This DEM uses a Geographic Coordinate System (GCS).")
        print(f"Units are typically angular (degrees) for GCS.")
    else:
        print("CRS type unknown (neither projected nor geographic).")

    # Ensure all GeoDataFrames have the same CRS as the DEM
    if sampling_points.crs != dem_crs:
        print(f"Reprojecting sampling points from {sampling_points.crs.to_string()} to {dem_crs.to_string()}")
        sampling_points = sampling_points.to_crs(dem_crs)
    if brick_fields.crs != dem_crs:
        print(f"Reprojecting brick fields from {brick_fields.crs.to_string()} to {dem_crs.to_string()}")
        brick_fields = brick_fields.to_crs(dem_crs)
    if industries.crs != dem_crs:
        print(f"Reprojecting industries from {industries.crs.to_string()} to {dem_crs.to_string()}")
        industries = industries.to_crs(dem_crs)

    print("All data loaded and CRS checked/reprojected.")

except Exception as e:
    print(f"Error loading data: {e}")
    print("Please ensure file paths are correct and data are valid geospatial files.")
    exit()

# --- 2. Hydrological Pre-processing with pysheds ---
print("\n2. Performing hydrological analysis (filling depressions, flow direction, accumulation)...")
try:
    dem_filled = grid.fill_depressions(dem)
    dem_inflated = grid.resolve_flats(dem_filled) # Addresses flat areas

    fdir = grid.flowdir(dem_inflated)
    acc = grid.accumulation(fdir)
    streams_raster = acc > STREAM_ACCUMULATION_THRESHOLD
    branches = grid.extract_river_network(fdir, streams_raster)

    stream_lines = []
    for branch in branches['features']:
        line = LineString(branch['geometry']['coordinates'])
        stream_lines.append(line)

    stream_gdf = gpd.GeoDataFrame(geometry=stream_lines, crs=dem_crs)
    print(f"Stream network extracted with {len(stream_gdf)} segments.")

except Exception as e:
    print(f"Error during hydrological processing: {e}")
    exit()

# --- 3. Prepare All Points for Graph Analysis ---
print("\n3. Preparing and snapping points to the stream network...")

all_points_combined = pd.concat([
    sampling_points.assign(type='sampling', original_idx=sampling_points.index),
    brick_fields.assign(type='brick_field', original_idx=brick_fields.index),
    industries.assign(type='industry', original_idx=industries.index)
]).reset_index(drop=True)

snapped_geoms = []
for idx, row in all_points_combined.iterrows():
    point_geom = row.geometry
    # If stream_gdf is empty or contains no valid geometry, nearest_points will fail
    if stream_gdf.empty or not stream_gdf.geometry.is_valid.any():
        print("Warning: Stream network is empty or invalid. Cannot snap points. Keeping original geometry.")
        snapped_geoms.append(point_geom) # Keep original if cannot snap
        continue

    # Create a unary union of all stream geometries for efficient snapping
    try:
        # --- MODIFICATION: Replace unary_union attribute with union_all() method ---
        unary_stream_geometry = stream_gdf.geometry.union_all()
        # --- END MODIFICATION ---

        if unary_stream_geometry.is_empty:
            print("Warning: Union of stream geometries is empty. Cannot snap points. Keeping original geometry.")
            snapped_geoms.append(point_geom) # Keep original if cannot snap
            continue

        # Find the closest point on the nearest stream line
        _, snapped_point_on_stream = shapely_nearest_points(unary_stream_geometry, point_geom)
        snapped_geoms.append(snapped_point_on_stream)
    except Exception as e:
        print(f"Warning: Could not snap point {idx} to stream network due to: {e}. Keeping original point geometry.")
        snapped_geoms.append(point_geom)


all_points_snapped = all_points_combined.copy()
all_points_snapped['geometry'] = snapped_geoms

print(f"Total points (sampling, brick fields, industries) snapped: {len(all_points_snapped)}")


import rasterio.transform # Make sure rasterio.transform is imported for rowcol function

# --- IMPORTANT ASSUMPTION ---
# This code assumes that the following variables from previous sections are
# already defined and available in your current environment (e.g., Jupyter cell output):
# - grid (pysheds Grid object)
# - dem (raw DEM data)
# - fdir (flow direction raster data)
# - acc (flow accumulation raster data)
# - dem_crs (CRS of the DEM and reprojected points)
# - sampling_points (GeoDataFrame of sampling points, with original_idx)
# - brick_fields (GeoDataFrame of brick fields, with original_idx)
# - industries (GeoDataFrame of industries, with original_idx)
# - all_points_snapped (GeoDataFrame of all combined and snapped points)
# ----------------------------

# --- D8 Flow Direction Mapping (EXPLICITLY DEFINED) ---
# This dictionary maps flow direction values (from pysheds fdir output)
# to (row_offset, col_offset) for the next cell.
# This is crucial if grid.dirmap is not directly accessible.
D8_DIRMAP = {
    1: (0, 1),    # East
    2: (1, 1),    # Southeast
    4: (1, 0),    # South
    8: (1, -1),   # Southwest
    16: (0, -1),  # West
    32: (-1, -1), # Northwest
    64: (-1, 0),  # North
    128: (-1, 1)  # Northeast
}
# --- END EXPLICIT DIRMAP ---

# Hydrological parameter (needs to be consistent if not in global scope)
STREAM_ACCUMULATION_THRESHOLD = 500 # Ensure this matches your first cell's value

# --- 4. Construct the Hydrological Graph with NetworkX ---
print("\n4. Building the hydrological graph...")

G = nx.DiGraph()

# Add all snapped points as nodes to the graph first
for idx, row in all_points_snapped.iterrows():
    G.add_node(idx, geometry=row.geometry, type=row['type'], original_idx=row['original_idx'])

rows_dem, cols_dem = fdir.shape # Use fdir shape for raster dimensions
stream_cell_nodes_map = {}

# Get array of stream cells (row, col)
stream_r, stream_c = np.where(acc >= STREAM_ACCUMULATION_THRESHOLD)

# Add stream cells as nodes and connect them based on flow direction
for r, c in zip(stream_r, stream_c):
    cell_node_id = f'cell_{r}_{c}'
    stream_cell_nodes_map[(r, c)] = cell_node_id

    # Get the affine transform from the grid
    transform = grid.affine
    
    # Calculate the centroid coordinates of the cell (r, c)
    x_coord, y_coord = transform * (c + 0.5, r + 0.5)

    cell_point_geometry = Point(x_coord, y_coord)

    if cell_node_id not in G: # Ensure node is added only once
        G.add_node(cell_node_id, geometry=cell_point_geometry, type='stream_cell')

    # Determine the next cell in the flow direction using the explicit D8_DIRMAP
    flow_direction_value = fdir[r, c]
    if flow_direction_value in D8_DIRMAP: # Use the explicitly defined dirmap
        dr, dc = D8_DIRMAP[flow_direction_value] # Get row/col offsets
        next_r, next_c = r + dr, c + dc
    else:
        # If flow_direction_value is not in D8_DIRMAP (e.g., it's a sink or invalid value)
        # then there's no defined downstream cell from this point.
        # We can continue to the next iteration of the loop, as no edge can be formed.
        continue # Skip to the next stream cell if flow direction is invalid

    # Check if the next cell is within bounds and also part of the extracted stream network
    if (0 <= next_r < rows_dem and 0 <= next_c < cols_dem and
            acc[next_r, next_c] >= STREAM_ACCUMULATION_THRESHOLD):
        
        next_cell_node_id = f'cell_{next_r}_{next_c}'
        
        # Calculate coordinates for the next cell
        x_next_coord, y_next_coord = transform * (next_c + 0.5, next_r + 0.5)

        next_cell_point_geometry = Point(x_next_coord, y_next_coord)

        if next_cell_node_id not in G: # Add next cell node if it doesn't exist
            G.add_node(next_cell_node_id, geometry=next_cell_point_geometry, type='stream_cell')

        # Calculate weight (distance between cell centers)
        weight = np.sqrt( (x_coord - x_next_coord)**2 + (y_coord - y_next_coord)**2 )
        # Add a small epsilon weight if distance is zero (e.g., same cell, though unlikely for distinct flow cells)
        if weight == 0:
            weight = grid.resolution # Use grid resolution as a fallback or minimal distance

        G.add_edge(cell_node_id, next_cell_node_id, weight=weight)

# --- Diagnostic Prints for Graph Structure ---
print(f"Graph initialization check: Nodes={G.number_of_nodes()}, Edges={G.number_of_edges()}")
if G.number_of_nodes() > 0:
    print(f"Sample node data (first 3):")
    for i, (node_id, data) in enumerate(G.nodes(data=True)):
        if i >= 3: break
        print(f"  Node ID: {node_id}, Type: {data.get('type')}, Geometry: {data.get('geometry')}")
if G.number_of_edges() > 0:
    print(f"Sample edge data (first 3):")
    for i, (u, v, data) in enumerate(G.edges(data=True)):
        if i >= 3: break
        print(f"  Edge: {u} -> {v}, Weight: {data.get('weight')}")
# --- End Diagnostic Prints ---


# Connect the snapped points to the closest stream cell node in the graph
epsilon_weight = 0.001 # A very small weight for the connection between the snapped point and stream cell

# Pre-collect stream cell geometries for efficient distance calculation
# This creates a list of (stream_cell_node_id, stream_cell_geometry) tuples
stream_cell_graph_nodes = []
for (r, c), cell_id in stream_cell_nodes_map.items():
    if cell_id in G: # Ensure the node was actually added to G
        stream_cell_graph_nodes.append((cell_id, G.nodes[cell_id]['geometry']))


for node_idx, data in all_points_snapped.iterrows():
    point_geom = data['geometry']
    
    closest_stream_node_id = None
    min_dist_to_stream_node = float('inf')

    # Find the closest *existing* stream cell node in the graph to the snapped point
    for stream_cell_id, stream_cell_geometry in stream_cell_graph_nodes:
        dist = point_geom.distance(stream_cell_geometry)
        if dist < min_dist_to_stream_node:
            min_dist_to_stream_node = dist
            closest_stream_node_id = stream_cell_id

    if closest_stream_node_id: # If a closest stream node was found
        # Add bidirectional edges with small weight
        G.add_edge(node_idx, closest_stream_node_id, weight=epsilon_weight)
        G.add_edge(closest_stream_node_id, node_idx, weight=epsilon_weight)
        # print(f"Connected point {node_idx} (type: {data['type']}) to stream node {closest_stream_node_id} with distance {min_dist_to_stream_node:.2f}")
    else:
        # This warning is more critical now, means no stream cell node was found near the snapped point
        print(f"CRITICAL Warning: Snapped point {node_idx} (type: {data['type']}, original_idx: {data['original_idx']}) could not be connected to any stream cell node in the graph. It might be isolated.")


print(f"Hydrological graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.")
print(f"Connectivity check: Is the graph strongly connected? {nx.is_strongly_connected(G)}")
print(f"Connectivity check: Is the graph weakly connected? {nx.is_weakly_connected(G)}")


# --- 5. Calculate Graph-Based Distances and Features ---
print("\n5. Calculating graph-based distance features...")

results_df = pd.DataFrame(index=sampling_points.index)

# Filter for the graph nodes that represent sources and sampling points
source_nodes = [node for node, data in G.nodes(data=True) if data['type'] in ['brick_field', 'industry']]
sampling_node_map = {data['original_idx']: node for node, data in G.nodes(data=True) if data['type'] == 'sampling'}

print(f"Total source nodes identified in graph: {len(source_nodes)}")
print(f"Total sampling nodes identified in graph: {len(sampling_node_map)}")

for original_sampling_idx, s_node_graph_id in sampling_node_map.items():
    
    dist_to_nearest_bf = np.nan
    num_upstream_bf = 0
    dist_to_nearest_ind = np.nan
    num_upstream_ind = 0

    print(f"\nProcessing sampling point (original_idx): {original_sampling_idx}, Graph Node ID: {s_node_graph_id}")

    for source_graph_id in source_nodes:
        source_type = G.nodes[source_graph_id]['type']
        
        print(f"  Checking path from source {source_graph_id} (type: {source_type}) to sampling {s_node_graph_id}...")

        try:
            # Check if a directed path exists from the source to the sampling node.
            if nx.has_path(G, source_graph_id, s_node_graph_id):
                path_length = nx.shortest_path_length(G, source=source_graph_id, target=s_node_graph_id, weight='weight')
                print(f"    Path EXISTS! Length: {path_length:.2f}")
                
                if source_type == 'brick_field':
                    if np.isnan(dist_to_nearest_bf) or path_length < dist_to_nearest_bf:
                        dist_to_nearest_bf = path_length
                    num_upstream_bf += 1 # Count if path exists, regardless of whether it's the *nearest*
                elif source_type == 'industry':
                    if np.isnan(dist_to_nearest_ind) or path_length < dist_to_nearest_ind:
                        dist_to_nearest_ind = path_length
                    num_upstream_ind += 1 # Count if path exists, regardless of whether it's the *nearest*
            else:
                print(f"    No directed path from {source_graph_id} to {s_node_graph_id}.")
        except nx.NetworkXNoPath:
            # This 'except' block handles cases where shortest_path_length might raise an error
            # even if has_path returns True in some edge cases (e.g., disconnected components if graph is malformed)
            print(f"    NetworkXNoPath error for path from {source_graph_id} to {s_node_graph_id}. This shouldn't happen if has_path is True.")
        except Exception as e:
            print(f"    An unexpected error occurred while checking path from {source_graph_id} to {s_node_graph_id}: {e}")

    results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_BF'] = dist_to_nearest_bf
    results_df.loc[original_sampling_idx, 'num_upstream_BF'] = num_upstream_bf
    results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_IND'] = dist_to_nearest_ind
    results_df.loc[original_sampling_idx, 'num_upstream_IND'] = num_upstream_ind

print("\nGenerated Features (first 5 rows):")
print(results_df.head())

# --- Optional: Save the generated features to a CSV or Parquet file ---
output_csv_path = 'hydrological_features.csv'
results_df.to_csv(output_csv_path)
print(f"\nGenerated features saved to: {output_csv_path}")

print("\nData generation complete!")


results_df


import numpy as np
import pandas as pd
import geopandas as gpd
import networkx as nx
from shapely.geometry import Point, LineString
from shapely.ops import nearest_points
import rasterio
from rasterio.warp import reproject, Resampling # Import reproject and Resampling enum
from rasterio.crs import CRS # Import CRS to explicitly set it
import rasterio.transform # For rowcol

# Ensure pysheds is imported if you're running this as a standalone script
try:
    from pysheds.grid import Grid
except ImportError:
    print("pysheds not found. Please install it using: pip install pysheds")
    exit()

# --- IMPORTANT ASSUMPTION ---
# This code assumes that the following variables from previous sections are
# already defined and available in your current environment (e.g., Jupyter cell output),
# or that you will load them if running as a single script:
# - dem (raw DEM data array)
# - fdir (flow direction raster data array)
# - acc (flow accumulation raster data array)
# - sampling_points (GeoDataFrame)
# - brick_fields (GeoDataFrame)
# - industries (GeoDataFrame)
# - all_points_snapped (GeoDataFrame)
#
# If running this as a complete script, you'd need to load these first, e.g.:
# grid = Grid.from_raster("path/to/your/dem.tif")
# dem = grid.read_raster("path/to/your/dem.tif")
# fdir = grid.flowdir(dem)
# acc = grid.accumulation(fdir)
# dem_crs = grid.crs
# sampling_points = gpd.read_file("path/to/sampling_points.shp").to_crs(dem_crs)
# brick_fields = gpd.read_file("path/to/brick_fields.shp").to_crs(dem_crs)
# industries = gpd.read_file("path/to/industries.shp").to_crs(dem_crs)
# (and then combine/snap points as in previous sections)
# ----------------------------

# --- NEW PARAMETERS FOR ALIGNMENT AND NDWI/GNDWI INTEGRATION ---
DEM_RASTER_PATH = './SentinelRaster/Indices/DEMF.tif' # <--- !!! SET THIS PATH TO YOUR DEM !!!
NDWI_RASTER_PATH = './SentinelRaster/Indices_10m/ndwi.tif' # <--- !!! SET THIS PATH TO YOUR NDWI/GNDWI RASTER !!!
NDWI_WATER_THRESHOLD = 0.1 # <--- !!! TUNE THIS VALUE !!! (e.g., 0.0 to 0.3 are common for water)
# -----------------------------------------------------------------

# --- D8 Flow Direction Mapping (EXPLICITLY DEFINED) ---
D8_DIRMAP = {
    1: (0, 1),    # East
    2: (1, 1),    # Southeast
    4: (1, 0),    # South
    8: (1, -1),   # Southwest
    16: (0, -1),  # West
    32: (-1, -1), # Northwest
    64: (-1, 0),  # North
    128: (-1, 1)  # Northeast
}
# --- END EXPLICIT DIRMAP ---

# Hydrological parameter
STREAM_ACCUMULATION_THRESHOLD = 500

# --- 4. Construct the Hydrological Graph with NetworkX ---
print("\n4. Building the hydrological graph...")

# Load DEM profile for target alignment and initial DEM processing
dem_profile = None
try:
    with rasterio.open(DEM_RASTER_PATH) as dem_src:
        dem_profile = dem_src.profile
        dem_crs = dem_profile['crs']
        rows_dem, cols_dem = dem_profile['height'], dem_profile['width']
        dem_transform = dem_profile['transform'] # Store transform

        # Load DEM data and perform PySheds processing here if not done already
        # Assuming `grid` is already initialized with DEM_RASTER_PATH from previous steps
        # If running standalone, you'd initialize grid here:
        grid = Grid.from_raster(DEM_RASTER_PATH)
        dem = grid.read_raster(DEM_RASTER_PATH)

        # Ensure DEM is filled and flow processed
        # This part should ideally be done once at the beginning of your overall workflow
        print("Performing DEM hydrological processing (fill, flowdir, accumulation)...")
        dem = grid.fill_depressions(dem)
        fdir = grid.flowdir(dem)
        acc = grid.accumulation(fdir)
        print("DEM hydrological processing complete.")

except FileNotFoundError:
    print(f"Error: DEM raster not found at {DEM_RASTER_PATH}. Cannot proceed with hydrological graph building.")
    exit() # Exit if DEM is not found, as it's fundamental
except Exception as e:
    print(f"Error during DEM loading or initial hydrological processing: {e}. Cannot proceed.")
    exit()

# If dem_crs from rasterio is different from grid.crs, prioritize rasterio's
# (This scenario is rare if grid is correctly initialized from the same raster)
if grid.crs != dem_crs:
    print(f"Warning: PySheds Grid CRS ({grid.crs}) differs from Rasterio DEM CRS ({dem_crs}). Using Rasterio's DEM CRS.")
    grid.crs = dem_crs


# Ensure sampling_points, brick_fields, industries are loaded and reprojected to dem_crs
# This part assumes you've done this in previous steps. If running standalone:
try:
    # Example paths - replace with your actual paths
    if 'sampling_points' not in locals():
        sampling_points = gpd.read_file("sampling_point.shp").to_crs(dem_crs)
    if 'brick_fields' not in locals():
        brick_fields = gpd.read_file("brick_field_point.shp").to_crs(dem_crs)
    if 'industries' not in locals():
        industries = gpd.read_file("industry_point.shp").to_crs(dem_crs)

    # Combine all points into one GeoDataFrame for snapping
    all_points = pd.concat([
        sampling_points.assign(type='sampling'),
        brick_fields.assign(type='brick_field'),
        industries.assign(type='industry')
    ], ignore_index=True)

    # Snap points to the stream network
    # Extract stream network lines from pysheds grid for snapping
    # This part should be updated to use `grid.extract_river_network`
    # if you want to snap to derived lines.
    # For robust snapping, we'll continue snapping to the closest point on any stream line geometry,
    # then connect that snapped point to the *closest stream cell node* in the graph.
    print("Snapping points to the hydrological network...")
    all_points_snapped = all_points.copy()
    stream_lines = grid.extract_river_network(fdir, acc >= STREAM_ACCUMULATION_THRESHOLD)

    # Convert shapely LineStrings to MultiLineString for single object to snap to
    all_stream_lines_geom = LineString([ls for ls in stream_lines.geometry if ls.is_empty == False])

    for idx, row in all_points_snapped.iterrows():
        point_geom = row.geometry
        snapped_point = nearest_points(all_stream_lines_geom, point_geom)[0]
        all_points_snapped.loc[idx, 'geometry'] = snapped_point
    print("Points snapping complete.")

except Exception as e:
    print(f"Error preparing points or snapping: {e}. Ensure all input geospatial data is correctly loaded and paths are set.")
    exit()


G = nx.DiGraph()

# Add all snapped points as nodes to the graph first
for idx, row in all_points_snapped.iterrows():
    G.add_node(idx, geometry=row.geometry, type=row['type'], original_idx=row['original_idx'])

stream_cell_nodes_map = {}

# 1. Get initial DEM-derived stream cells based on flow accumulation
stream_r_dem, stream_c_dem = np.where(acc >= STREAM_ACCUMULATION_THRESHOLD)
dem_stream_cells = set(zip(stream_r_dem, stream_c_dem))
print(f"Identified {len(dem_stream_cells)} stream cells from DEM (threshold: {STREAM_ACCUMULATION_THRESHOLD}).")

# 2. Process and align NDWI/GNDWI raster to find additional water cells
ndwi_water_cells = set()
try:
    with rasterio.open(NDWI_RASTER_PATH) as ndwi_src:
        print(f"Loading NDWI/GNDWI raster from: {NDWI_RASTER_PATH}")
        
        # Read the original NDWI data (first band)
        ndwi_data_original = ndwi_src.read(1, masked=True)
        
        # Prepare an empty array for the reprojected NDWI data
        # It will have the same dimensions as the DEM
        ndwi_reprojected = np.empty(shape=(rows_dem, cols_dem), dtype=ndwi_data_original.dtype)
        
        print(f"Reprojecting and resampling NDWI/GNDWI from {ndwi_src.crs} to {dem_profile['crs']}...")
        print(f"Source shape: {ndwi_src.shape}, Target shape: {ndwi_reprojected.shape}")
        
        reproject(
            source=ndwi_data_original,
            src_crs=ndwi_src.crs,
            src_transform=ndwi_src.transform,
            destination=ndwi_reprojected,
            dst_crs=dem_profile['crs'],
            dst_transform=dem_profile['transform'],
            resampling=Resampling.bilinear, # Bilinear for continuous data like NDWI
            src_nodata=ndwi_src.nodata, # Handle source nodata
            dst_nodata=np.nan # Set destination nodata to NaN
        )
        print("NDWI/GNDWI reprojection complete.")

        # Create a boolean mask for water from the REPROJECTED data
        # Filter out NaN values which are now nodata
        water_mask = (ndwi_reprojected > NDWI_WATER_THRESHOLD) & (~np.isnan(ndwi_reprojected))
        
        # Get row, col indices of identified water cells
        r_ndwi_water, c_ndwi_water = np.where(water_mask)
        ndwi_water_cells = set(zip(r_ndwi_water, c_ndwi_water))
        print(f"Found {len(ndwi_water_cells)} water cells from ALIGNED NDWI/GNDWI raster (threshold: {NDWI_WATER_THRESHOLD}).")

except FileNotFoundError:
    print(f"Error: NDWI/GNDWI raster not found at {NDWI_RASTER_PATH}. Proceeding with DEM-only stream network.")
except Exception as e:
    print(f"Error processing NDWI/GNDWI raster during reprojection: {e}. Proceeding with DEM-only stream network.")


# 3. Combine DEM-derived stream cells and NDWI-derived water cells
# Use a union to get all unique cells from both sources
all_stream_cells_rc = sorted(list(dem_stream_cells.union(ndwi_water_cells)))
print(f"Total unique stream/water cells for graph: {len(all_stream_cells_rc)}")


# Add stream cells as nodes and connect them based on flow direction
# This loop will now use the combined set of (r,c) cells
for r, c in all_stream_cells_rc:
    cell_node_id = f'cell_{r}_{c}'
    
    # Ensure a cell is not added if it's outside the bounds of fdir
    # This might happen if NDWI had a slightly different extent, even after reproject.
    if not (0 <= r < rows_dem and 0 <= c < cols_dem):
        continue # Skip cells outside the primary DEM grid

    stream_cell_nodes_map[(r, c)] = cell_node_id

    # Calculate the centroid coordinates of the cell (r, c)
    x_coord, y_coord = grid.affine * (c + 0.5, r + 0.5) # Use grid.affine

    cell_point_geometry = Point(x_coord, y_coord)

    if cell_node_id not in G: # Ensure node is added only once
        G.add_node(cell_node_id, geometry=cell_point_geometry, type='stream_cell')

    # Determine the next cell in the flow direction using the explicit D8_DIRMAP
    # Note: fdir is derived from DEM only. NDWI adds nodes but doesn't change DEM flow direction.
    # We still use fdir for connectivity between stream cells.
    flow_direction_value = fdir[r, c] # Get flow direction from DEM-derived fdir array
    if flow_direction_value in D8_DIRMAP:
        dr, dc = D8_DIRMAP[flow_direction_value]
        next_r, next_c = r + dr, c + dc
    else:
        continue # Skip to the next stream cell if flow direction is invalid

    # Check if the next cell is within bounds AND part of our *combined* set of stream/water cells
    # This is crucial: an edge only forms if the downstream cell is also a valid "waterway" node.
    if (0 <= next_r < rows_dem and 0 <= next_c < cols_dem and
            (next_r, next_c) in stream_cell_nodes_map): # Check if the next cell is a *node we added*
        
        next_cell_node_id = f'cell_{next_r}_{next_c}'
        
        # Calculate coordinates for the next cell
        x_next_coord, y_next_coord = grid.affine * (next_c + 0.5, next_r + 0.5)

        next_cell_point_geometry = Point(x_next_coord, y_next_coord)

        if next_cell_node_id not in G: # Add next cell node if it doesn't exist
            G.add_node(next_cell_node_id, geometry=next_cell_point_geometry, type='stream_cell')

        # Calculate weight (distance between cell centers)
        weight = np.sqrt( (x_coord - x_next_coord)**2 + (y_coord - y_next_coord)**2 )
        if weight == 0:
            weight = grid.resolution

        G.add_edge(cell_node_id, next_cell_node_id, weight=weight)

# --- Diagnostic Prints for Graph Structure ---
print(f"Graph initialization check: Nodes={G.number_of_nodes()}, Edges={G.number_of_edges()}")
if G.number_of_nodes() > 0:
    print(f"Sample node data (first 3):")
    for i, (node_id, data) in enumerate(G.nodes(data=True)):
        if i >= 3: break
        print(f"  Node ID: {node_id}, Type: {data.get('type')}, Geometry: {data.get('geometry')}")
if G.number_of_edges() > 0:
    print(f"Sample edge data (first 3):")
    for i, (u, v, data) in enumerate(G.edges(data=True)):
        if i >= 3: break
        print(f"  Edge: {u} -> {v}, Weight: {data.get('weight')}")
# --- End Diagnostic Prints ---


# Connect the snapped points to the closest stream cell node in the graph
epsilon_weight = 0.001 # A very small weight for the connection between the snapped point and stream cell

# Pre-collect stream cell geometries for efficient distance calculation
stream_cell_graph_nodes = []
for (r, c), cell_id in stream_cell_nodes_map.items():
    if cell_id in G: # Ensure the node was actually added to G
        stream_cell_graph_nodes.append((cell_id, G.nodes[cell_id]['geometry']))


for node_idx, data in all_points_snapped.iterrows():
    point_geom = data['geometry']
    
    closest_stream_node_id = None
    min_dist_to_stream_node = float('inf')

    # Find the closest *existing* stream cell node in the graph to the snapped point
    for stream_cell_id, stream_cell_geometry in stream_cell_graph_nodes:
        dist = point_geom.distance(stream_cell_geometry)
        if dist < min_dist_to_stream_node:
            min_dist_to_stream_node = dist
            closest_stream_node_id = stream_cell_id

    if closest_stream_node_id: # If a closest stream node was found
        # Add bidirectional edges with small weight
        G.add_edge(node_idx, closest_stream_node_id, weight=epsilon_weight)
        G.add_edge(closest_stream_node_id, node_idx, weight=epsilon_weight)
    else:
        print(f"CRITICAL Warning: Snapped point {node_idx} (type: {data['type']}, original_idx: {data['original_idx']}) could not be connected to any stream cell node in the graph. It might be isolated.")


print(f"Hydrological graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.")
print(f"Connectivity check: Is the graph strongly connected? {nx.is_strongly_connected(G)}")
print(f"Connectivity check: Is the graph weakly connected? {nx.is_weakly_connected(G)}")


# --- 5. Calculate Graph-Based Distances and Features ---
print("\n5. Calculating graph-based distance features...")

results_df = pd.DataFrame(index=sampling_points.index)

# Filter for the graph nodes that represent sources and sampling points
source_nodes = [node for node, data in G.nodes(data=True) if data['type'] in ['brick_field', 'industry']]
sampling_node_map = {data['original_idx']: node for node, data in G.nodes(data=True) if data['type'] == 'sampling'}

print(f"Total source nodes identified in graph: {len(source_nodes)}")
print(f"Total sampling nodes identified in graph: {len(sampling_node_map)}")

for original_sampling_idx, s_node_graph_id in sampling_node_map.items():
    
    dist_to_nearest_bf = np.nan # Initialize as NaN
    num_upstream_bf = 0
    dist_to_nearest_ind = np.nan # Initialize as NaN
    num_upstream_ind = 0

    print(f"\nProcessing sampling point (original_idx): {original_sampling_idx}, Graph Node ID: {s_node_graph_id}")

    for source_graph_id in source_nodes:
        source_type = G.nodes[source_graph_id]['type']
        
        print(f"  Checking path from source {source_graph_id} (type: {source_type}) to sampling {s_node_graph_id}...")

        try:
            # Check if a directed path exists from the source to the sampling node.
            if nx.has_path(G, source_graph_id, s_node_graph_id):
                path_length = nx.shortest_path_length(G, source=source_graph_id, target=s_node_graph_id, weight='weight')
                print(f"    Path EXISTS! Length: {path_length:.2f}")
                
                if source_type == 'brick_field':
                    if np.isnan(dist_to_nearest_bf) or path_length < dist_to_nearest_bf:
                        dist_to_nearest_bf = path_length
                    num_upstream_bf += 1
                elif source_type == 'industry':
                    if np.isnan(dist_to_nearest_ind) or path_length < dist_to_nearest_ind:
                        dist_to_nearest_ind = path_length
                    num_upstream_ind += 1
            else:
                print(f"    No directed path from {source_graph_id} to {s_node_graph_id}.")
        except nx.NetworkXNoPath:
            print(f"    NetworkXNoPath error for path from {source_graph_id} to {s_node_graph_id}.")
        except Exception as e:
            print(f"    An unexpected error occurred while checking path from {source_graph_id} to {s_node_graph_id}: {e}")

    # Replace NaN distances with infinity if no upstream source was found
    if np.isnan(dist_to_nearest_bf):
        dist_to_nearest_bf = float('inf')
    if np.isnan(dist_to_nearest_ind):
        dist_to_nearest_ind = float('inf')

    results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_BF'] = dist_to_nearest_bf
    results_df.loc[original_sampling_idx, 'num_upstream_BF'] = num_upstream_bf
    results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_IND'] = dist_to_nearest_ind
    results_df.loc[original_sampling_idx, 'num_upstream_IND'] = num_upstream_ind

print("\nGenerated Features (first 5 rows):")
print(results_df.head())

# --- Optional: Save the generated features to a CSV or Parquet file ---
output_csv_path = 'hydrological_features1.csv'
results_df.to_csv(output_csv_path)
print(f"\nGenerated features saved to: {output_csv_path}")

print("\nData generation complete!")


import numpy as np
import pandas as pd
import geopandas as gpd
import networkx as nx
from shapely.geometry import Point, LineString
from shapely.ops import nearest_points
import rasterio
from rasterio.warp import reproject, Resampling # Import reproject and Resampling enum
from rasterio.crs import CRS # Import CRS to explicitly set it
import rasterio.transform # For rowcol

# Ensure pysheds is imported if you're running this as a standalone script
try:
    from pysheds.grid import Grid
except ImportError:
    print("pysheds not found. Please install it using: pip install pysheds")
    exit()

# --- IMPORTANT ASSUMPTION ---
# This code assumes that the following variables from previous sections are
# already defined and available in your current environment (e.g., Jupyter cell output),
# or that you will load them if running as a single script:
# - dem (raw DEM data array)
# - fdir (flow direction raster data array)
# - acc (flow accumulation raster data array)
# - sampling_points (GeoDataFrame)
# - brick_fields (GeoDataFrame)
# - industries (GeoDataFrame)
# - all_points_snapped (GeoDataFrame)
#
# If running this as a complete script, you'd need to load these first, e.g.:
# grid = Grid.from_raster("path/to/your/dem.tif")
# dem = grid.read_raster("path/to/your/dem.tif")
# fdir = grid.flowdir(dem)
# acc = grid.accumulation(fdir)
# dem_crs = grid.crs
# sampling_points = gpd.read_file("path/to/sampling_points.shp").to_crs(dem_crs)
# brick_fields = gpd.read_file("path/to/brick_fields.shp").to_crs(dem_crs)
# industries = gpd.read_file("path/to/industries.shp").to_crs(dem_crs)
# (and then combine/snap points as in previous sections)
# ----------------------------

# --- NEW PARAMETERS FOR ALIGNMENT AND NDWI/GNDWI INTEGRATION ---
DEM_RASTER_PATH = './SentinelRaster/Indices/DEMF.tif' # <--- !!! SET THIS PATH TO YOUR DEM !!!
NDWI_RASTER_PATH = './SentinelRaster/Indices_10m/ndwi.tif' # <--- !!! SET THIS PATH TO YOUR NDWI/GNDWI RASTER !!!
NDWI_WATER_THRESHOLD = 0.1 # <--- !!! TUNE THIS VALUE !!! (e.g., 0.0 to 0.3 are common for water)
# -----------------------------------------------------------------

# --- D8 Flow Direction Mapping (EXPLICITLY DEFINED) ---
D8_DIRMAP = {
    1: (0, 1),    # East
    2: (1, 1),    # Southeast
    4: (1, 0),    # South
    8: (1, -1),   # Southwest
    16: (0, -1),  # West
    32: (-1, -1), # Northwest
    64: (-1, 0),  # North
    128: (-1, 1)  # Northeast
}
# --- END EXPLICIT DIRMAP ---

# Hydrological parameter
STREAM_ACCUMULATION_THRESHOLD = 500

# --- 4. Construct the Hydrological Graph with NetworkX ---
print("\n4. Building the hydrological graph...")

# Load DEM profile for target alignment and initial DEM processing
dem_profile = None
try:
    with rasterio.open(DEM_RASTER_PATH) as dem_src:
        dem_profile = dem_src.profile
        dem_crs = dem_profile['crs']
        rows_dem, cols_dem = dem_profile['height'], dem_profile['width']
        dem_transform = dem_profile['transform'] # Store transform

        # Load DEM data and perform PySheds processing here if not done already
        # Assuming `grid` is already initialized with DEM_RASTER_PATH from previous steps
        # If running standalone, you'd initialize grid here:
        grid = Grid.from_raster(DEM_RASTER_PATH)
        dem = grid.read_raster(DEM_RASTER_PATH)

        # Ensure DEM is filled and flow processed
        # This part should ideally be done once at the beginning of your overall workflow
        print("Performing DEM hydrological processing (fill, flowdir, accumulation)...")
        dem = grid.fill_depressions(dem)
        fdir = grid.flowdir(dem)
        acc = grid.accumulation(fdir)
        print("DEM hydrological processing complete.")

except FileNotFoundError:
    print(f"Error: DEM raster not found at {DEM_RASTER_PATH}. Cannot proceed with hydrological graph building.")
    exit() # Exit if DEM is not found, as it's fundamental
except Exception as e:
    print(f"Error during DEM loading or initial hydrological processing: {e}. Cannot proceed.")
    exit()

# If dem_crs from rasterio is different from grid.crs, prioritize rasterio's
# (This scenario is rare if grid is correctly initialized from the same raster)
if grid.crs != dem_crs:
    print(f"Warning: PySheds Grid CRS ({grid.crs}) differs from Rasterio DEM CRS ({dem_crs}). Using Rasterio's DEM CRS.")
    grid.crs = dem_crs


# Ensure sampling_points, brick_fields, industries are loaded and reprojected to dem_crs
# This part assumes you've done this in previous steps. If running standalone:
try:
    # Example paths - replace with your actual paths
    if 'sampling_points' not in locals():
        sampling_points = gpd.read_file("sampling_point.shp").to_crs(dem_crs)
    if 'brick_fields' not in locals():
        brick_fields = gpd.read_file("brick_field_point.shp").to_crs(dem_crs)
    if 'industries' not in locals():
        industries = gpd.read_file("industry_point.shp").to_crs(dem_crs)

    # Combine all points into one GeoDataFrame for snapping
    # Ensure 'original_idx' is created from the original index before concatenation
    all_points = pd.concat([
        sampling_points.assign(type='sampling').reset_index().rename(columns={'index': 'original_idx'}),
        brick_fields.assign(type='brick_field').reset_index().rename(columns={'index': 'original_idx'}),
        industries.assign(type='industry').reset_index().rename(columns={'index': 'original_idx'})
    ], ignore_index=True)

    # Snap points to the stream network
    print("Snapping points to the hydrological network...")
    all_points_snapped = all_points.copy()
    stream_lines = grid.extract_river_network(fdir, acc >= STREAM_ACCUMULATION_THRESHOLD)

    # Convert shapely LineStrings to MultiLineString for single object to snap to
    # Filter out empty geometries to avoid errors
    all_stream_lines_geom = LineString([ls for ls in stream_lines.geometry if not ls.is_empty])

    for idx, row in all_points_snapped.iterrows():
        point_geom = row.geometry
        snapped_point = nearest_points(all_stream_lines_geom, point_geom)[0]
        all_points_snapped.loc[idx, 'geometry'] = snapped_point
    print("Points snapping complete.")

except Exception as e:
    print(f"Error preparing points or snapping: {e}. Ensure all input geospatial data is correctly loaded and paths are set.")
    exit()


G = nx.DiGraph()

# Add all snapped points as nodes to the graph first
for idx, row in all_points_snapped.iterrows():
    # 'original_idx' is now guaranteed to exist due to the modification above
    G.add_node(idx, geometry=row.geometry, type=row['type'], original_idx=row['original_idx'])

stream_cell_nodes_map = {}

# 1. Get initial DEM-derived stream cells based on flow accumulation
stream_r_dem, stream_c_dem = np.where(acc >= STREAM_ACCUMULATION_THRESHOLD)
dem_stream_cells = set(zip(stream_r_dem, stream_c_dem))
print(f"Identified {len(dem_stream_cells)} stream cells from DEM (threshold: {STREAM_ACCUMULATION_THRESHOLD}).")

# 2. Process and align NDWI/GNDWI raster to find additional water cells
ndwi_water_cells = set()
try:
    with rasterio.open(NDWI_RASTER_PATH) as ndwi_src:
        print(f"Loading NDWI/GNDWI raster from: {NDWI_RASTER_PATH}")
        
        # Read the original NDWI data (first band)
        ndwi_data_original = ndwi_src.read(1, masked=True)
        
        # Prepare an empty array for the reprojected NDWI data
        # It will have the same dimensions as the DEM
        ndwi_reprojected = np.empty(shape=(rows_dem, cols_dem), dtype=ndwi_data_original.dtype)
        
        print(f"Reprojecting and resampling NDWI/GNDWI from {ndwi_src.crs} to {dem_profile['crs']}...")
        print(f"Source shape: {ndwi_src.shape}, Target shape: {ndwi_reprojected.shape}")
        
        reproject(
            source=ndwi_data_original,
            src_crs=ndwi_src.crs,
            src_transform=ndwi_src.transform,
            destination=ndwi_reprojected,
            dst_crs=dem_profile['crs'],
            dst_transform=dem_profile['transform'],
            resampling=Resampling.bilinear, # Bilinear for continuous data like NDWI
            src_nodata=ndwi_src.nodata, # Handle source nodata
            dst_nodata=np.nan # Set destination nodata to NaN
        )
        print("NDWI/GNDWI reprojection complete.")

        # Create a boolean mask for water from the REPROJECTED data
        # Filter out NaN values which are now nodata
        water_mask = (ndwi_reprojected > NDWI_WATER_THRESHOLD) & (~np.isnan(ndwi_reprojected))
        
        # Get row, col indices of identified water cells
        r_ndwi_water, c_ndwi_water = np.where(water_mask)
        ndwi_water_cells = set(zip(r_ndwi_water, c_ndwi_water))
        print(f"Found {len(ndwi_water_cells)} water cells from ALIGNED NDWI/GNDWI raster (threshold: {NDWI_WATER_THRESHOLD}).")

except FileNotFoundError:
    print(f"Error: NDWI/GNDWI raster not found at {NDWI_RASTER_PATH}. Proceeding with DEM-only stream network.")
except Exception as e:
    print(f"Error processing NDWI/GNDWI raster during reprojection: {e}. Proceeding with DEM-only stream network.")


# 3. Combine DEM-derived stream cells and NDWI-derived water cells
# Use a union to get all unique cells from both sources
all_stream_cells_rc = sorted(list(dem_stream_cells.union(ndwi_water_cells)))
print(f"Total unique stream/water cells for graph: {len(all_stream_cells_rc)}")


# Add stream cells as nodes and connect them based on flow direction
# This loop will now use the combined set of (r,c) cells
for r, c in all_stream_cells_rc:
    cell_node_id = f'cell_{r}_{c}'
    
    # Ensure a cell is not added if it's outside the bounds of fdir
    # This might happen if NDWI had a slightly different extent, even after reproject.
    if not (0 <= r < rows_dem and 0 <= c < cols_dem):
        continue # Skip cells outside the primary DEM grid

    stream_cell_nodes_map[(r, c)] = cell_node_id

    # Calculate the centroid coordinates of the cell (r, c)
    x_coord, y_coord = grid.affine * (c + 0.5, r + 0.5) # Use grid.affine

    cell_point_geometry = Point(x_coord, y_coord)

    if cell_node_id not in G: # Ensure node is added only once
        G.add_node(cell_node_id, geometry=cell_point_geometry, type='stream_cell')

    # Determine the next cell in the flow direction using the explicit D8_DIRMAP
    # Note: fdir is derived from DEM only. NDWI adds nodes but doesn't change DEM flow direction.
    # We still use fdir for connectivity between stream cells.
    flow_direction_value = fdir[r, c] # Get flow direction from DEM-derived fdir array
    if flow_direction_value in D8_DIRMAP:
        dr, dc = D8_DIRMAP[flow_direction_value]
        next_r, next_c = r + dr, c + dc
    else:
        continue # Skip to the next stream cell if flow direction is invalid

    # Check if the next cell is within bounds AND part of our *combined* set of stream/water cells
    # This is crucial: an edge only forms if the downstream cell is also a valid "waterway" node.
    if (0 <= next_r < rows_dem and 0 <= next_c < cols_dem and
            (next_r, next_c) in stream_cell_nodes_map): # Check if the next cell is a *node we added*
        
        next_cell_node_id = f'cell_{next_r}_{next_c}'
        
        # Calculate coordinates for the next cell
        x_next_coord, y_next_coord = grid.affine * (next_c + 0.5, next_r + 0.5)

        next_cell_point_geometry = Point(x_next_coord, y_next_coord)

        if next_cell_node_id not in G: # Add next cell node if it doesn't exist
            G.add_node(next_cell_node_id, geometry=next_cell_point_geometry, type='stream_cell')

        # Calculate weight (distance between cell centers)
        weight = np.sqrt( (x_coord - x_next_coord)**2 + (y_coord - y_next_coord)**2 )
        if weight == 0:
            weight = grid.resolution

        G.add_edge(cell_node_id, next_cell_node_id, weight=weight)

# --- Diagnostic Prints for Graph Structure ---
print(f"Graph initialization check: Nodes={G.number_of_nodes()}, Edges={G.number_of_edges()}")
if G.number_of_nodes() > 0:
    print(f"Sample node data (first 3):")
    for i, (node_id, data) in enumerate(G.nodes(data=True)):
        if i >= 3: break
        print(f"  Node ID: {node_id}, Type: {data.get('type')}, Geometry: {data.get('geometry')}")
if G.number_of_edges() > 0:
    print(f"Sample edge data (first 3):")
    for i, (u, v, data) in enumerate(G.edges(data=True)):
        if i >= 3: break
        print(f"  Edge: {u} -> {v}, Weight: {data.get('weight')}")
# --- End Diagnostic Prints ---


# Connect the snapped points to the closest stream cell node in the graph
epsilon_weight = 0.001 # A very small weight for the connection between the snapped point and stream cell

# Pre-collect stream cell geometries for efficient distance calculation
stream_cell_graph_nodes = []
for (r, c), cell_id in stream_cell_nodes_map.items():
    if cell_id in G: # Ensure the node was actually added to G
        stream_cell_graph_nodes.append((cell_id, G.nodes[cell_id]['geometry']))


for node_idx, data in all_points_snapped.iterrows():
    point_geom = data['geometry']
    
    closest_stream_node_id = None
    min_dist_to_stream_node = float('inf')

    # Find the closest *existing* stream cell node in the graph to the snapped point
    for stream_cell_id, stream_cell_geometry in stream_cell_graph_nodes:
        dist = point_geom.distance(stream_cell_geometry)
        if dist < min_dist_to_stream_node:
            min_dist_to_stream_node = dist
            closest_stream_node_id = stream_cell_id

    if closest_stream_node_id: # If a closest stream node was found
        # Add bidirectional edges with small weight
        G.add_edge(node_idx, closest_stream_node_id, weight=epsilon_weight)
        # --- FIX START: Corrected typo 'node_node_idx' to 'node_idx' ---
        G.add_edge(closest_stream_node_id, node_idx, weight=epsilon_weight) 
        # --- FIX END ---
    else:
        print(f"CRITICAL Warning: Snapped point {node_idx} (type: {data['type']}, original_idx: {data['original_idx']}) could not be connected to any stream cell node in the graph. It might be isolated.")


print(f"Hydrological graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.")
print(f"Connectivity check: Is the graph strongly connected? {nx.is_strongly_connected(G)}")
print(f"Connectivity check: Is the graph weakly connected? {nx.is_weakly_connected(G)}")


# --- 5. Calculate Graph-Based Distances and Features ---
print("\n5. Calculating graph-based distance features...")

results_df = pd.DataFrame(index=sampling_points.index)

# Filter for the graph nodes that represent sources and sampling points
source_nodes = [node for node, data in G.nodes(data=True) if data['type'] in ['brick_field', 'industry']]
sampling_node_map = {data['original_idx']: node for node, data in G.nodes(data=True) if data['type'] == 'sampling'}

print(f"Total source nodes identified in graph: {len(source_nodes)}")
print(f"Total sampling nodes identified in graph: {len(sampling_node_map)}")

for original_sampling_idx, s_node_graph_id in sampling_node_map.items():
    
    dist_to_nearest_bf = np.nan # Initialize as NaN
    num_upstream_bf = 0
    dist_to_nearest_ind = np.nan # Initialize as NaN
    num_upstream_ind = 0

    print(f"\nProcessing sampling point (original_idx): {original_sampling_idx}, Graph Node ID: {s_node_graph_id}")

    for source_graph_id in source_nodes:
        source_type = G.nodes[source_graph_id]['type']
        
        print(f"  Checking path from source {source_graph_id} (type: {source_type}) to sampling {s_node_graph_id}...")

        try:
            # Check if a directed path exists from the source to the sampling node.
            if nx.has_path(G, source_graph_id, s_node_graph_id):
                path_length = nx.shortest_path_length(G, source=source_graph_id, target=s_node_graph_id, weight='weight')
                print(f"    Path EXISTS! Length: {path_length:.2f}")
                
                if source_type == 'brick_field':
                    if np.isnan(dist_to_nearest_bf) or path_length < dist_to_nearest_bf:
                        dist_to_nearest_bf = path_length
                    num_upstream_bf += 1
                elif source_type == 'industry':
                    if np.isnan(dist_to_nearest_ind) or path_length < dist_to_nearest_ind:
                        dist_to_nearest_ind = path_length
                    num_upstream_ind += 1
            else:
                print(f"    No directed path from {source_graph_id} to {s_node_graph_id}.")
        except nx.NetworkXNoPath:
            print(f"    NetworkXNoPath error for path from {source_graph_id} to {s_node_graph_id}.")
        except Exception as e:
            print(f"    An unexpected error occurred while checking path from {source_graph_id} to {s_node_graph_id}: {e}")

    # Replace NaN distances with infinity if no upstream source was found
    if np.isnan(dist_to_nearest_bf):
        dist_to_nearest_bf = float('inf')
    if np.isnan(dist_to_nearest_ind):
        dist_to_nearest_ind = float('inf')

    results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_BF'] = dist_to_nearest_bf
    results_df.loc[original_sampling_idx, 'num_upstream_BF'] = num_upstream_bf
    # --- FIX START: Corrected assignment of hydrological_dist_to_nearest_IND ---
    results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_IND'] = dist_to_nearest_ind 
    results_df.loc[original_sampling_idx, 'num_upstream_IND'] = num_upstream_ind 
    # --- FIX END ---

print("\nGenerated Features (first 5 rows):")
print(results_df.head())

# --- Optional: Save the generated features to a CSV or Parquet file ---
output_csv_path = 'hydrological_features2.csv'
results_df.to_csv(output_csv_path)
print(f"\nGenerated features saved to: {output_csv_path}")

print("\nData generation complete!")


import numpy as np
import pandas as pd
import geopandas as gpd
import networkx as nx
from shapely.geometry import Point, LineString
from shapely.ops import nearest_points, unary_union # ADDED unary_union
import rasterio
from rasterio.warp import reproject, Resampling
from rasterio.crs import CRS
import rasterio.transform
import geojson # Import geojson to check for its types

# It's Tuesday, July 29, 2025 at 1:32:29 PM +06 in Dhaka, Bangladesh.
# No specific time-sensitive data paths are provided for this context, so file paths are general.

try:
    from pysheds.grid import Grid
except ImportError:
    print("pysheds not found. Please install it using: pip install pysheds")
    exit()

# --- IMPORTANT ASSUMPTION & INITIAL DATA LOADING ---
# This script assumes you have your raster and shapefile data ready.
# If running as a standalone script, ensure these paths are correct and data exists.

try:
    DEM_RASTER_PATH = './SentinelRaster/Indices/DEMF.tif' # !!! VERIFY THIS PATH IS CORRECT AND FILE EXISTS !!!
    NDWI_RASTER_PATH = './SentinelRaster/Indices_10m/ndwi.tif' # !!! VERIFY THIS PATH IS CORRECT AND FILE EXISTS !!!

    # Load DEM and get its CRS and transform
    with rasterio.open(DEM_RASTER_PATH) as dem_src:
        dem_profile = dem_src.profile
        dem_crs = dem_profile['crs']
        rows_dem, cols_dem = dem_profile['height'], dem_profile['width']
        dem_transform = dem_profile['transform']

    # Initialize PySheds Grid and process DEM
    grid = Grid.from_raster(DEM_RASTER_PATH)
    dem = grid.read_raster(DEM_RASTER_PATH)
    print("Performing DEM hydrological processing (fill, flowdir, accumulation)...")
    dem = grid.fill_depressions(dem)
    fdir = grid.flowdir(dem)
    acc = grid.accumulation(fdir)
    print("DEM hydrological processing complete.")

    # Ensure grid.crs matches dem_crs from rasterio
    if grid.crs != dem_crs:
        print(f"Warning: PySheds Grid CRS ({grid.crs}) differs from Rasterio DEM CRS ({dem_crs}). Using Rasterio's DEM CRS.")
        grid.crs = dem_crs

    # Load and reproject points if necessary
    sampling_points = gpd.read_file("sampling_point.shp").to_crs(dem_crs) # !!! VERIFY THIS PATH !!!
    brick_fields = gpd.read_file("brick_field_point.shp").to_crs(dem_crs) # !!! VERIFY THIS PATH !!!
    industries = gpd.read_file("industry_point.shp").to_crs(dem_crs)     # !!! VERIFY THIS PATH !!!

except FileNotFoundError as e:
    print(f"Error: Required file not found. Please check paths. {e}")
    exit()
except Exception as e:
    print(f"An unexpected error occurred during initial data loading or processing: {e}")
    exit()

# --- PARAMETERS FOR ACCURACY TUNING ---
# Adjust these values based on your specific study area and desired stream network density.
# Lowering STREAM_ACCUMULATION_THRESHOLD generally increases network connectivity.
STREAM_ACCUMULATION_THRESHOLD = 50 # <-- ADJUSTED: Try a much lower value if you're getting empty streams
                                    # Experiment with values like 50, 20, 10, or even 5.
                                    # Tune this based on visual inspection of the resulting network
                                    # and whether it connects your points.

NDWI_WATER_THRESHOLD = 0.1          # Adjust this threshold (e.g., 0.05 to 0.2)
                                    # to refine what is considered 'water' from NDWI.
                                    # Visually compare with satellite imagery if possible.

# D8 Flow Direction Mapping (Standard)
D8_DIRMAP = {
    1: (0, 1),    # East
    2: (1, 1),    # Southeast
    4: (1, 0),    # South
    8: (1, -1),   # Southwest
    16: (0, -1),  # West
    32: (-1, -1), # Northwest
    64: (-1, 0),  # North
    128: (-1, 1)  # Northeast
}

print("\n4. Building the hydrological graph...")

# Combine all points into one GeoDataFrame for snapping
# Ensure 'original_idx' is created from the original index before concatenation
all_points = pd.concat([
    sampling_points.assign(type='sampling').reset_index().rename(columns={'index': 'original_idx'}),
    brick_fields.assign(type='brick_field').reset_index().rename(columns={'index': 'original_idx'}),
    industries.assign(type='industry').reset_index().rename(columns={'index': 'original_idx'})
], ignore_index=True)

print("Snapping points to the hydrological network...")
stream_lines = grid.extract_river_network(fdir, acc >= STREAM_ACCUMULATION_THRESHOLD)

# --- Ensure stream_lines is a GeoDataFrame (Existing Fix) ---
# Check if stream_lines is a geojson.FeatureCollection and convert it
if isinstance(stream_lines, geojson.FeatureCollection):
    try:
        # Assuming the features are in the 'features' key and CRS is available from grid
        stream_lines = gpd.GeoDataFrame.from_features(stream_lines['features'], crs=grid.crs)
        print("Successfully converted geojson.FeatureCollection to GeoDataFrame.")
    except Exception as e:
        print(f"Error converting geojson.FeatureCollection to GeoDataFrame: {e}")
        print("Proceeding with an empty GeoDataFrame for stream lines, snapping will likely fail.")
        stream_lines = gpd.GeoDataFrame({'geometry': []}, crs=grid.crs) # Create an empty GeoDataFrame
elif not isinstance(stream_lines, gpd.GeoDataFrame):
    print(f"Warning: stream_lines is of unexpected type ({type(stream_lines)}). Attempting generic conversion to GeoDataFrame.")
    try:
        stream_lines = gpd.GeoDataFrame(stream_lines, crs=grid.crs)
    except Exception as e:
        print(f"Error converting unexpected type to GeoDataFrame: {e}. Skipping stream line processing.")
        stream_lines = gpd.GeoDataFrame({'geometry': []}, crs=grid.crs) # Create an empty GeoDataFrame

# Ensure 'geometry' column exists after potential conversions
if 'geometry' not in stream_lines.columns:
    print("Error: 'geometry' column not found in stream_lines after processing. Cannot proceed with snapping.")
    exit()

# Filter out empty geometries
valid_geometries = [ls for ls in stream_lines.geometry if not ls.is_empty]

print(f"Number of raw stream line geometries extracted: {len(stream_lines.geometry)}")
print(f"Number of VALID (non-empty) stream line geometries: {len(valid_geometries)}")


# Create a single geometry object (LineString or MultiLineString or GeometryCollection)
if not valid_geometries:
    print("CRITICAL: No valid stream line geometries found after extraction and filtering.")
    print("This means no stream network could be delineated using the current parameters.")
    print("RECOMMENDATION: Try lowering 'STREAM_ACCUMULATION_THRESHOLD' further or checking your DEM/NDWI data for issues.")
    all_stream_lines_geom = LineString() # Create an empty LineString
else:
    # --- FIX START: Use unary_union to combine valid geometries ---
    # Use unary_union to combine all valid LineString objects into a single MultiLineString
    # or GeometryCollection. This is the correct way to handle a list of geometries.
    all_stream_lines_geom = unary_union(valid_geometries)
    # --- FIX END ---
    
    # Check if unary_union resulted in an empty geometry (e.g., if valid_geometries were unexpectedly problematic)
    if all_stream_lines_geom.is_empty:
        print("WARNING: unary_union resulted in an empty geometry even when valid_geometries was not empty. This is unexpected.")
        print("This might indicate an issue with the geometries themselves. Proceeding as if no streams found.")
        all_stream_lines_geom = LineString() # Fallback to empty LineString

# Explicit check for empty geometry object before attempting to snap points
if all_stream_lines_geom.is_empty:
    print("FINAL CHECK: 'all_stream_lines_geom' is empty. Cannot proceed with point snapping.")
    print("This usually means the stream network could not be formed.")
    exit() # Exit here if no streams are available for snapping

# This block will only be reached if all_stream_lines_geom is NOT empty
all_points_snapped = all_points.copy()
for idx, row in all_points_snapped.iterrows():
    point_geom = row.geometry
    snapped_point = nearest_points(all_stream_lines_geom, point_geom)[0]
    all_points_snapped.loc[idx, 'geometry'] = snapped_point
print("Points snapping complete.")


G = nx.DiGraph()

# Add all snapped points as nodes to the graph first
for idx, row in all_points_snapped.iterrows():
    G.add_node(idx, geometry=row.geometry, type=row['type'], original_idx=row['original_idx'])

stream_cell_nodes_map = {}

# 1. Get initial DEM-derived stream cells based on flow accumulation
stream_r_dem, stream_c_dem = np.where(acc >= STREAM_ACCUMULATION_THRESHOLD)
dem_stream_cells = set(zip(stream_r_dem, stream_c_dem))
print(f"Identified {len(dem_stream_cells)} stream cells from DEM (threshold: {STREAM_ACCUMULATION_THRESHOLD}).")

# 2. Process and align NDWI/GNDWI raster to find additional water cells
ndwi_water_cells = set()
try:
    with rasterio.open(NDWI_RASTER_PATH) as ndwi_src:
        print(f"Loading NDWI/GNDWI raster from: {NDWI_RASTER_PATH}")
        
        ndwi_data_original = ndwi_src.read(1, masked=True)
        ndwi_reprojected = np.empty(shape=(rows_dem, cols_dem), dtype=ndwi_data_original.dtype)
        
        print(f"Reprojecting and resampling NDWI/GNDWI from {ndwi_src.crs} to {dem_profile['crs']}...")
        reproject(
            source=ndwi_data_original,
            src_crs=ndwi_src.crs,
            src_transform=ndwi_src.transform,
            destination=ndwi_reprojected,
            dst_crs=dem_profile['crs'],
            dst_transform=dem_profile['transform'],
            resampling=Resampling.bilinear,
            src_nodata=ndwi_src.nodata,
            dst_nodata=np.nan
        )
        print("NDWI/GNDWI reprojection complete.")

        water_mask = (ndwi_reprojected > NDWI_WATER_THRESHOLD) & (~np.isnan(ndwi_reprojected))
        r_ndwi_water, c_ndwi_water = np.where(water_mask)
        ndwi_water_cells = set(zip(r_ndwi_water, c_ndwi_water))
        print(f"Found {len(ndwi_water_cells)} water cells from ALIGNED NDWI/GNDWI raster (threshold: {NDWI_WATER_THRESHOLD}).")

except FileNotFoundError:
    print(f"Error: NDWI/GNDWI raster not found at {NDWI_RASTER_PATH}. Proceeding with DEM-only stream network for water cells.")
except Exception as e:
    print(f"Error processing NDWI/GNDWI raster during reprojection: {e}. Proceeding with DEM-only stream network for water cells.")


# 3. Combine DEM-derived stream cells and NDWI-derived water cells
all_stream_cells_rc = sorted(list(dem_stream_cells.union(ndwi_water_cells)))
print(f"Total unique stream/water cells for graph: {len(all_stream_cells_rc)}")


# Add stream cells as nodes and connect them based on flow direction
for r, c in all_stream_cells_rc:
    cell_node_id = f'cell_{r}_{c}'
    
    if not (0 <= r < rows_dem and 0 <= c < cols_dem):
        continue

    stream_cell_nodes_map[(r, c)] = cell_node_id

    x_coord, y_coord = grid.affine * (c + 0.5, r + 0.5)
    cell_point_geometry = Point(x_coord, y_coord)

    if cell_node_id not in G:
        G.add_node(cell_node_id, geometry=cell_point_geometry, type='stream_cell')

    flow_direction_value = fdir[r, c]
    if flow_direction_value in D8_DIRMAP:
        dr, dc = D8_DIRMAP[flow_direction_value]
        next_r, next_c = r + dr, c + dc
    else:
        continue

    if (0 <= next_r < rows_dem and 0 <= next_c < cols_dem and
            (next_r, next_c) in stream_cell_nodes_map):
        
        next_cell_node_id = f'cell_{next_r}_{next_c}'
        
        x_next_coord, y_next_coord = grid.affine * (next_c + 0.5, next_r + 0.5)
        next_cell_point_geometry = Point(x_next_coord, y_next_coord)

        if next_cell_node_id not in G:
            G.add_node(next_cell_node_id, geometry=next_cell_point_geometry, type='stream_cell')

        weight = np.sqrt( (x_coord - x_next_coord)**2 + (y_coord - y_next_coord)**2 )
        if weight == 0:
            weight = grid.resolution

        G.add_edge(cell_node_id, next_cell_node_id, weight=weight)


print(f"Graph initialization check: Nodes={G.number_of_nodes()}, Edges={G.number_of_edges()}")
if G.number_of_nodes() == 0:
    print("Error: No nodes were added to the graph. Cannot proceed with connectivity analysis.")
    exit()

# Connect the snapped points to the closest stream cell node in the graph
epsilon_weight = 0.001

stream_cell_graph_nodes = []
for (r, c), cell_id in stream_cell_nodes_map.items():
    if cell_id in G:
        stream_cell_graph_nodes.append((cell_id, G.nodes[cell_id]['geometry']))

if not stream_cell_graph_nodes:
    print("Warning: No stream cell nodes found in the graph. Snapped points cannot be connected.")


for node_idx, data in all_points_snapped.iterrows():
    point_geom = data['geometry']
    
    closest_stream_node_id = None
    min_dist_to_stream_node = float('inf')

    if not stream_cell_graph_nodes:
        print(f"Skipping connection for point {node_idx}: No stream cell nodes available.")
        continue

    for stream_cell_id, stream_cell_geometry in stream_cell_graph_nodes:
        dist = point_geom.distance(stream_cell_geometry)
        if dist < min_dist_to_stream_node:
            min_dist_to_stream_node = dist
            closest_stream_node_id = stream_cell_id

    if closest_stream_node_id:
        G.add_edge(node_idx, closest_stream_node_id, weight=epsilon_weight)
        G.add_edge(closest_stream_node_id, node_idx, weight=epsilon_weight)
    else:
        print(f"CRITICAL Warning: Snapped point {node_idx} (type: {data['type']}, original_idx: {data['original_idx']}) could not be connected to any stream cell node in the graph. It might be isolated.")


print(f"Hydrological graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.")
print(f"Connectivity check: Is the graph strongly connected? {nx.is_strongly_connected(G)}")
print(f"Connectivity check: Is the graph weakly connected? {nx.is_weakly_connected(G)}")


# --- 5. Calculate Graph-Based Distances and Features ---
print("\n5. Calculating graph-based distance features...")

if sampling_points.empty:
    print("Error: 'sampling_points' GeoDataFrame is empty. Cannot generate features.")
    exit()

results_df = pd.DataFrame(index=sampling_points.index)

source_nodes = [node for node, data in G.nodes(data=True) if data['type'] in ['brick_field', 'industry']]
sampling_node_map = {data['original_idx']: node for node, data in G.nodes(data=True) if data['type'] == 'sampling'}

if not source_nodes:
    print("Warning: No source nodes (brick fields or industries) found in the graph. Distance calculations will be trivial.")
if not sampling_node_map:
    print("Warning: No sampling point nodes found in the graph. No features to calculate.")


print(f"Total source nodes identified in graph: {len(source_nodes)}")
print(f"Total sampling nodes identified in graph: {len(sampling_node_map)}")

for original_sampling_idx, s_node_graph_id in sampling_node_map.items():
    
    dist_to_nearest_bf = np.nan
    num_upstream_bf = 0
    dist_to_nearest_ind = np.nan
    num_upstream_ind = 0

    if s_node_graph_id not in G:
        print(f"Skipping original sampling index {original_sampling_idx}: Graph node {s_node_graph_id} not found in graph G.")
        results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_BF'] = float('inf')
        results_df.loc[original_sampling_idx, 'num_upstream_BF'] = 0
        results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_IND'] = float('inf')
        results_df.loc[original_sampling_idx, 'num_upstream_IND'] = 0
        continue

    for source_graph_id in source_nodes:
        source_type = G.nodes[source_graph_id]['type']
        
        if source_graph_id not in G:
            continue
        
        try:
            if nx.has_path(G, source_graph_id, s_node_graph_id):
                path_length = nx.shortest_path_length(G, source=source_graph_id, target=s_node_graph_id, weight='weight')
                
                if source_type == 'brick_field':
                    if np.isnan(dist_to_nearest_bf) or path_length < dist_to_nearest_bf:
                        dist_to_nearest_bf = path_length
                    num_upstream_bf += 1
                elif source_type == 'industry':
                    if np.isnan(dist_to_nearest_ind) or path_length < dist_to_nearest_ind:
                        dist_to_nearest_ind = path_length
                    num_upstream_ind += 1
        except nx.NetworkXNoPath:
            pass
        except Exception as e:
            print(f"    An unexpected error occurred while checking path from {source_graph_id} to {s_node_graph_id}: {e}")

    if np.isnan(dist_to_nearest_bf):
        dist_to_nearest_bf = float('inf')
    if np.isnan(dist_to_nearest_ind):
        dist_to_nearest_ind = float('inf')

    results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_BF'] = dist_to_nearest_bf
    results_df.loc[original_sampling_idx, 'num_upstream_BF'] = num_upstream_bf
    results_df.loc[original_sampling_idx, 'hydrological_dist_to_nearest_IND'] = dist_to_nearest_ind
    results_df.loc[original_sampling_idx, 'num_upstream_IND'] = num_upstream_ind

print("\nGenerated Features (first 5 rows):")
print(results_df.head())

output_csv_path = 'hydrological_features3.csv'
results_df.to_csv(output_csv_path)
print(f"\nGenerated features saved to: {output_csv_path}")

print("\nData generation complete!")





f1 = pd.read_csv("hydrological_features.csv")
f2 = pd.read_csv("hydrological_features3.csv")
f1, f2


sum(f2.num_upstream_BF)


f = pd.read_csv("hydrologicalF.csv")


f


import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
import contextily as cx # For adding a basemap

# --- 1. Define File Paths ---
SAMPLING_POINTS_SHP = 'sampling_point.shp' # Path to your original sampling points shapefile
# Assuming your "final values" are in this file,
# or you can change this to 'final_merged_hydrological_features.csv'
# if you want to visualize the output of the very last merge step.
FEATURES_CSV = 'hydrologicalF.csv' # Path to your processed features CSV

# --- 2. Load Data ---
try:
    # Load the original sampling points GeoDataFrame
    sampling_points_gdf = gpd.read_file(SAMPLING_POINTS_SHP)
    print(f"Loaded {len(sampling_points_gdf)} sampling points from '{SAMPLING_POINTS_SHP}'.")
    print(f"CRS of sampling points: {sampling_points_gdf.crs}")

    # Load the features CSV
    features_df = pd.read_csv(FEATURES_CSV)
    # Set the 'Unnamed: 0' column as the index for merging
    if 'Unnamed: 0' in features_df.columns:
        features_df = features_df.set_index('Unnamed: 0')
        features_df.index.name = None # Clean up index name
    print(f"Loaded {len(features_df)} feature rows from '{FEATURES_CSV}'.")

    # --- 3. Merge Features with Spatial Data ---
    # Merge the features DataFrame with the GeoDataFrame based on their indices.
    # We assume the index of features_df corresponds to the index of sampling_points_gdf.
    # If your indices don't match or are not unique, you might need to merge on a specific ID column.
    
    # Ensure original index is preserved in sampling_points_gdf if it was not the default
    # Or, if sampling_points_gdf uses a default integer index, ensure features_df also uses that.
    # For now, let's assume default integer indices align, or the 'Unnamed: 0' aligns.
    
    # If your sampling_points_gdf also has an 'Unnamed: 0' or 'original_idx' that matches features_df's index:
    # You might need to adjust this depending on how your sampling_points_gdf was originally loaded.
    # Example: If sampling_points_gdf has an 'id_column' matching features_df's index:
    # merged_gdf = sampling_points_gdf.merge(features_df, left_on='id_column', right_index=True, how='left')

    # Assuming default integer index alignment for now, common if not explicitly set
    merged_gdf = sampling_points_gdf.merge(features_df, left_index=True, right_index=True, how='left')

    print("\nMerged GeoDataFrame (first 5 rows with new features):")
    print(merged_gdf.head())
    print(f"Total columns in merged GeoDataFrame: {merged_gdf.shape[1]}")

    # --- 4. Choose Features to Visualize ---
    # Pick a few key features to plot. You can change these.
    # Make sure the chosen columns exist in your FEATURES_CSV.
    
    # Check if the combined NDWI-based features are present
    if 'hydrological_dist_to_nearest_ANY' in merged_gdf.columns:
        feature_to_plot_1 = 'hydrological_dist_to_nearest_ANY'
        feature_to_plot_2 = 'num_upstream_ANY'
    else:
        # Fallback if _ANY columns aren't in the loaded CSV (e.g., if you loaded hydrological_features2.csv)
        feature_to_plot_1 = 'hydrological_dist_to_nearest_BF'
        feature_to_plot_2 = 'num_upstream_BF'
        print(f"Note: '{feature_to_plot_1}' and '{feature_to_plot_2}' are used as _ANY columns not found.")


    # --- 5. Create Spatial Visualizations ---

    # Plot 1: Hydrological Distance to Nearest Any Source
    fig1, ax1 = plt.subplots(1, 1, figsize=(12, 12))
    merged_gdf.plot(column=feature_to_plot_1, cmap='viridis_r', markersize=50,
                    legend=True, legend_kwds={'label': f"Hydrological Distance ({feature_to_plot_1})"},
                    edgecolor='black', linewidth=0.5, ax=ax1) # viridis_r reverses colors (darker = shorter dist)
    ax1.set_title(f'Sampling Points: {feature_to_plot_1}', fontsize=15)
    ax1.set_axis_off() # Hide axes for cleaner map
    # Add a basemap for geographical context
    cx.add_basemap(ax1, crs=merged_gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)
    plt.tight_layout()
    plt.savefig(f'map_{feature_to_plot_1}.png', dpi=300)
    plt.show()

    # Plot 2: Number of Upstream Any Sources
    fig2, ax2 = plt.subplots(1, 1, figsize=(12, 12))
    merged_gdf.plot(column=feature_to_plot_2, cmap='YlOrRd', markersize=50,
                    legend=True, legend_kwds={'label': f"Number of Upstream Sources ({feature_to_plot_2})"},
                    edgecolor='black', linewidth=0.5, ax=ax2) # YlOrRd for count (darker = more)
    ax2.set_title(f'Sampling Points: {feature_to_plot_2}', fontsize=15)
    ax2.set_axis_off()
    cx.add_basemap(ax2, crs=merged_gdf.crs.to_string(), source=cx.providers.CartoDB.Positron)
    plt.tight_layout()
    plt.savefig(f'map_{feature_to_plot_2}.png', dpi=300)
    plt.show()

    print(f"\nMaps generated and saved as 'map_{feature_to_plot_1}.png' and 'map_{feature_to_plot_2}.png'.")

except FileNotFoundError as e:
    print(f"Error: One of the required files was not found. Please check paths: {e}")
    print(f"Ensure '{SAMPLING_POINTS_SHP}' and '{FEATURES_CSV}' exist in the correct directory.")
except KeyError as e:
    print(f"Error: A column expected for plotting or merging was not found: {e}")
    print("Please check the column names in your CSV file and ensure the correct feature names are selected.")
except Exception as e:
    print(f"An unexpected error occurred during spatial visualization: {e}")


merged_gdf.head()


merged_gdf.to_csv("merged_df.csv",index= False)


import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import griddata # For interpolation
import contextily as cx # For basemaps

# --- 1. Define File Paths ---
SAMPLING_POINTS_SHP = 'sampling_point.shp'
FEATURES_CSV = 'hydrologicalF.csv' # Or 'final_merged_hydrological_features.csv'
                                                    # or 'final_transformed_df.csv' if you applied scaling

# --- 2. Load Data ---
try:
    sampling_points_gdf = gpd.read_file(SAMPLING_POINTS_SHP)
    # Ensure a consistent CRS for spatial operations and basemaps
    if sampling_points_gdf.crs is None:
        print("Warning: Sampling points GeoDataFrame has no CRS. Assuming WGS84 (EPSG:4326).")
        sampling_points_gdf = sampling_points_gdf.set_crs("EPSG:4326", allow_override=True)
    
    # Reproject to a suitable projected CRS for accurate distance calculations if not already
    # (e.g., a UTM zone for your area, or a local projection)
    # We'll use a common web mercator (EPSG:3857) for contextily compatibility if CRS is geographic.
    if sampling_points_gdf.crs.is_geographic:
        projected_crs = "EPSG:3857" # Web Mercator, good for Basemaps
        sampling_points_gdf_proj = sampling_points_gdf.to_crs(projected_crs)
    else:
        projected_crs = sampling_points_gdf.crs
        sampling_points_gdf_proj = sampling_points_gdf

    print(f"Sampling points CRS (projected for interpolation/plot): {sampling_points_gdf_proj.crs}")


    features_df = pd.read_csv(FEATURES_CSV)
    if 'Unnamed: 0' in features_df.columns:
        features_df = features_df.set_index('Unnamed: 0')
        features_df.index.name = None

    # Merge features with the projected GeoDataFrame
    merged_gdf = sampling_points_gdf_proj.merge(features_df, left_index=True, right_index=True, how='left')

    # Ensure all required columns are numeric and handle NaNs if any
    for col in merged_gdf.columns:
        if pd.api.types.is_numeric_dtype(merged_gdf[col]):
            # Fill NaN for interpolation. Common strategies: mean, median, or 0.
            # For "distance", a NaN might imply infinite distance, so max might be better, or treat as missing.
            # For "counts", NaN might imply 0.
            # For demonstration, let's fill with the mean for now, but be careful with this!
            merged_gdf[col] = merged_gdf[col].fillna(merged_gdf[col].mean())


    print("\nMerged GeoDataFrame for interpolation (first 5 rows):")
    print(merged_gdf.head())

    # --- 3. Choose Feature for Interpolation ---
    # IMPORTANT: Select the column you want to interpolate.
    # Examples: 'hydrological_dist_to_nearest_ANY', 'num_upstream_ANY', 'log_num_upstream_BF_scaled', 'BF_Impact_Score'
    
    # Check if a combined feature exists, otherwise default to a basic one.
    if 'hydrological_dist_to_nearest_ANY' in merged_gdf.columns:
        feature_to_interpolate = 'hydrological_dist_to_nearest_ANY'
        # Or 'num_upstream_ANY'
        # Or if you created the composite score: 'BF_Impact_Score'
    elif 'hydrological_dist_to_nearest_BF' in merged_gdf.columns:
        feature_to_interpolate = 'hydrological_dist_to_nearest_BF'
    else:
        # Fallback to the first numeric column if specific ones aren't found
        numeric_cols = merged_gdf.select_dtypes(include=np.number).columns.tolist()
        if numeric_cols:
            feature_to_interpolate = numeric_cols[0]
            print(f"Warning: Specific features not found. Using '{feature_to_interpolate}' for interpolation.")
        else:
            raise ValueError("No numeric columns found for interpolation.")

    print(f"\nProceeding with interpolation for feature: '{feature_to_interpolate}'")

    # --- 4. Prepare Data for Interpolation ---
    # Extract coordinates (X, Y) and Z values from your merged GeoDataFrame
    # FIX: Access the 'geometry' column directly as a Series
    points = np.array(merged_gdf['geometry'].apply(lambda geom: (geom.x, geom.y)).tolist())
    values = merged_gdf[feature_to_interpolate].values

    # Define the interpolation grid
    # Get the bounding box of your points
    minx, miny, maxx, maxy = merged_gdf.total_bounds
    
    # Create a grid of points over the bounding box
    # Adjust resolution (num_rows, num_cols) for desired detail vs performance
    grid_resolution = 200 # Number of pixels in x and y direction
    grid_x, grid_y = np.mgrid[minx:maxx:complex(0, grid_resolution),
                              miny:maxy:complex(0, grid_resolution)]

    # --- 5. Perform Interpolation (Inverse Distance Weighting-like using griddata) ---
    print(f"Interpolating '{feature_to_interpolate}' using griddata (method='linear')...")
    grid_z = griddata(points, values, (grid_x, grid_y), method='linear')

    # Handle NaNs in the interpolated grid (areas outside convex hull of points for 'linear')
    grid_z[np.isnan(grid_z)] = np.nan # Or fill with a background value, e.g., mean, 0, or specific NoData value


    # --- 6. Visualize the Interpolated Surface ---
    fig, ax = plt.subplots(1, 1, figsize=(12, 12))

    # Plot the interpolated raster
    cmap_choice = 'viridis_r' if 'dist' in feature_to_interpolate else 'YlOrRd'

    im = ax.imshow(grid_z.T, extent=(minx, maxx, miny, maxy), origin='lower',
                   cmap=cmap_choice, aspect='auto', interpolation='bilinear', alpha=0.8) # alpha for transparency

    # Optionally, overlay the original sampling points for context
    merged_gdf.plot(ax=ax, color='black', marker='o', markersize=10, zorder=2, label='Sampling Points')


    ax.set_title(f'Interpolated Surface: {feature_to_interpolate}', fontsize=15)
    ax.set_xlabel('Longitude' if sampling_points_gdf.crs.is_geographic else 'X Coordinate')
    ax.set_ylabel('Latitude' if sampling_points_gdf.crs.is_geographic else 'Y Coordinate')
    
    # Add a colorbar
    cbar = fig.colorbar(im, ax=ax, orientation='vertical', shrink=0.7)
    cbar.set_label(feature_to_interpolate)

    # Add a basemap for geographical context
    cx.add_basemap(ax, crs=projected_crs.to_string(), source=cx.providers.CartoDB.Positron)

    plt.tight_layout()
    plt.savefig(f'interpolated_map_{feature_to_interpolate}.png', dpi=300)
    plt.show()

    print(f"\nInterpolated map generated and saved as 'interpolated_map_{feature_to_interpolate}.png'.")

except FileNotFoundError as e:
    print(f"Error: One of the required files was not found. Please check paths: {e}")
    print(f"Ensure '{SAMPLING_POINTS_SHP}' and '{FEATURES_CSV}' exist in the correct directory.")
except KeyError as e:
    print(f"Error: A column expected for plotting or merging was not found: {e}")
    print("Please check the column names in your CSV file and ensure the correct feature is selected.")
except Exception as e:
    print(f"An unexpected error occurred during spatial interpolation: {e}")


import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import griddata
import contextily as cx
from shapely.geometry import Point # For checking points within polygon
from shapely.ops import unary_union # To combine multiple polygons into one

# --- 1. Define File Paths ---
SAMPLING_POINTS_SHP = 'sampling_point.shp'
FEATURES_CSV = 'hydrologicalF.csv' # Or your most recent merged/transformed CSV
STUDY_AREA_SHP = 'Area2.shp' # Path to your study area shapefile

# --- 2. Load Data ---
try:
    # Load the original sampling points GeoDataFrame
    sampling_points_gdf = gpd.read_file(SAMPLING_POINTS_SHP)
    
    # Load the study area GeoDataFrame
    study_area_gdf = gpd.read_file(STUDY_AREA_SHP)

    # --- Ensure Consistent CRS for all GeoDataFrames ---
    # We will reproject everything to Web Mercator (EPSG:3857) for good basemap compatibility and projected units.
    # Adjust this CRS if you have a specific local projected CRS (e.g., UTM zone) in mind.
    TARGET_CRS = "EPSG:3857"

    if sampling_points_gdf.crs != TARGET_CRS:
        sampling_points_gdf_proj = sampling_points_gdf.to_crs(TARGET_CRS)
        print(f"Reprojected sampling points to {TARGET_CRS}")
    else:
        sampling_points_gdf_proj = sampling_points_gdf
    
    if study_area_gdf.crs != TARGET_CRS:
        study_area_gdf_proj = study_area_gdf.to_crs(TARGET_CRS)
        print(f"Reprojected study area to {TARGET_CRS}")
    else:
        study_area_gdf_proj = study_area_gdf

    print(f"All spatial data are now in CRS: {TARGET_CRS}")

    # Load the features CSV
    features_df = pd.read_csv(FEATURES_CSV)
    if 'Unnamed: 0' in features_df.columns:
        features_df = features_df.set_index('Unnamed: 0')
        features_df.index.name = None

    # Merge features with the projected sampling points GeoDataFrame
    merged_gdf = sampling_points_gdf_proj.merge(features_df, left_index=True, right_index=True, how='left')

    # Ensure all required columns are numeric and handle NaNs if any
    for col in merged_gdf.columns:
        if pd.api.types.is_numeric_dtype(merged_gdf[col]):
            merged_gdf[col] = merged_gdf[col].fillna(merged_gdf[col].mean()) # Fill NaNs for interpolation

    print("\nMerged GeoDataFrame for interpolation (first 5 rows):")
    print(merged_gdf.head())

    # --- 3. Choose Feature for Interpolation ---
    feature_to_interpolate = 'hydrological_dist_to_nearest_IND' # Example: Change as needed
    
    if feature_to_interpolate not in merged_gdf.columns:
        raise ValueError(f"Error: The column '{feature_to_interpolate}' was not found in the merged data.")

    print(f"\nProceeding with interpolation for feature: '{feature_to_interpolate}'")

    # --- 4. Prepare Data for Interpolation ---
    points = np.array(merged_gdf['geometry'].apply(lambda geom: (geom.x, geom.y)).tolist())
    values = merged_gdf[feature_to_interpolate].values

    # Define the interpolation grid using the bounds of the study area
    minx, miny, maxx, maxy = study_area_gdf_proj.total_bounds
    grid_resolution = 200 # Number of pixels in x and y direction
    grid_x, grid_y = np.mgrid[minx:maxx:complex(0, grid_resolution),
                              miny:maxy:complex(0, grid_resolution)]

    # --- 5. Perform Interpolation ---
    print(f"Interpolating '{feature_to_interpolate}' using griddata (method='linear')...")
    grid_z = griddata(points, values, (grid_x, grid_y), method='linear')

    # --- 6. Mask the Interpolated Surface by the Study Area Polygon ---
    print("Masking interpolated surface to study area boundary...")
    
    # Combine all polygons in study_area_gdf_proj into a single polygon object for efficient checking
    study_area_polygon = unary_union(study_area_gdf_proj.geometry)

    # Create a boolean mask: True for points inside the polygon, False otherwise
    mask = np.zeros(grid_x.shape, dtype=bool)
    for i in range(grid_x.shape[0]):
        for j in range(grid_x.shape[1]):
            # Check if the center of the grid cell is within the study area polygon
            if study_area_polygon.contains(Point(grid_x[i, j], grid_y[i, j])):
                mask[i, j] = True
    
    # Apply the mask: set values outside the polygon to NaN
    grid_z[~mask] = np.nan # Invert mask to set values outside to NaN


    # --- 7. Visualize the Interpolated Surface ---
    fig, ax = plt.subplots(1, 1, figsize=(12, 12))

    # For distance, 'viridis_r' or 'plasma_r' makes shorter distances (higher impact) darker/more prominent.
    cmap_choice = 'viridis_r' if 'dist' in feature_to_interpolate else 'YlOrRd'

    im = ax.imshow(grid_z.T, extent=(minx, maxx, miny, maxy), origin='lower',
                   cmap=cmap_choice, aspect='auto', interpolation='bilinear', alpha=0.8)

    # Overlay the original sampling points
    merged_gdf.plot(ax=ax, color='black', marker='o', markersize=10, zorder=3, label='Sampling Points')
    
    # Overlay the boundary of the study area
    study_area_gdf_proj.plot(ax=ax, facecolor='none', edgecolor='blue', linewidth=2, zorder=4, label='Study Area Boundary')


    ax.set_title(f'Interpolated Surface: {feature_to_interpolate} (Masked to Study Area)', fontsize=15)
    ax.set_xlabel('X Coordinate')
    ax.set_ylabel('Y Coordinate')
    
    cbar = fig.colorbar(im, ax=ax, orientation='vertical', shrink=0.7)
    cbar.set_label(feature_to_interpolate)

    # Add a basemap for geographical context using the projected CRS
    cx.add_basemap(ax, crs=TARGET_CRS, source=cx.providers.CartoDB.Positron)

    plt.tight_layout()
    plt.savefig(f'interpolated_map_{feature_to_interpolate}_masked.png', dpi=300)
    plt.show()

    print(f"\nInterpolated map for '{feature_to_interpolate}' masked to study area generated and saved.")

except FileNotFoundError as e:
    print(f"Error: One of the required files was not found. Please check paths: {e}")
    print(f"Ensure '{SAMPLING_POINTS_SHP}', '{FEATURES_CSV}', and '{STUDY_AREA_SHP}' exist.")
except KeyError as e:
    print(f"Error: A column expected for plotting or merging was not found: {e}")
    print("Please check the column names in your CSV file and ensure the correct feature is selected.")
except Exception as e:
    print(f"An unexpected error occurred during spatial interpolation: {e}")


merged_gdf.River


merged_gdf.River = merged_gdf.River.astype(str).str.rstrip(')')
merged_gdf.head()


merged_gdf.to_csv("merged_df.csv", index=False)


!pip install earthengine-api


! earthengine authenticate --force


import requests
import os
import time

def download_lulc_tile(year, tile_id="46R", output_dir="LULC_Downloads_46R_Tile"):
    """
    Downloads a specific LULC GeoTIFF tile for a given year.

    Args:
        year (int): The year of the LULC data (e.g., 2017).
        tile_id (str): The Sentinel-2 MGRS tile ID (e.g., "46R").
        output_dir (str): The directory where the downloaded files will be saved.
    """
    
    base_url = "https://lulctimeseries.blob.core.windows.net/lulctimeseriesv003/"
    
    # Construct the year-specific directory and filename
    year_dir = f"lc{year}/"
    filename = f"{tile_id}_{year}0101-{year+1}0101.tif" # Assumes YYYY0101-(YYYY+1)0101 pattern

    full_url = f"{base_url}{year_dir}{filename}"
    output_filepath = os.path.join(output_dir, filename)

    # Create the output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    print(f"Attempting to download: {full_url}")
    print(f"Saving to: {output_filepath}")

    try:
        # Use stream=True to handle large files efficiently
        with requests.get(full_url, stream=True) as r:
            r.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)
            with open(output_filepath, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192): # Iterate in chunks
                    f.write(chunk)
        print(f"Successfully downloaded {filename}")
    except requests.exceptions.HTTPError as e:
        print(f"Error downloading {filename}: HTTP Error {e.response.status_code} - {e.response.reason}")
        if e.response.status_code == 404:
            print("This usually means the file was not found at the specified URL for this year/tile.")
    except requests.exceptions.ConnectionError as e:
        print(f"Error downloading {filename}: Connection Error - {e}")
    except requests.exceptions.Timeout as e:
        print(f"Error downloading {filename}: Timeout Error - {e}")
    except requests.exceptions.RequestException as e:
        print(f"An unexpected error occurred while downloading {filename}: {e}")
    
    # Add a small delay to avoid overwhelming the server
    time.sleep(1) 


# --- Main execution part ---
if __name__ == "__main__":
    start_year = 2017
    end_year = 2023
    
    # The specific MGRS tile ID you provided
    target_tile_id = "46R" 
    
    # Output directory for your downloads
    download_directory = "LULC_Annual_Tiles1"

    print(f"Starting download of LULC tiles for MGRS tile '{target_tile_id}' from {start_year} to {end_year}...")
    
    for year in range(start_year, end_year + 1):
        download_lulc_tile(year, target_tile_id, download_directory)
        
    print("\nDownload process completed.")
    print(f"Check the '{download_directory}' folder for your GeoTIFF files.")

    print("\n--- Important Note on Coverage ---")
    print(f"The downloaded files are only for the Sentinel-2 MGRS tile '{target_tile_id}'.")
    print("Bangladesh is covered by multiple MGRS tiles (e.g., 45Q, 46Q, 45R, 46R).")
    print("If you need full coverage of Bangladesh, you will need to find the corresponding links for other tiles.")


# --- Main execution part ---
if __name__ == "__main__":
    start_year = 2017
    end_year = 2022
    
    # The specific MGRS tile ID you provided
    target_tile_id = "46Q" 
    
    # Output directory for your downloads
    download_directory = "LULC_Annual_Tiles4"

    print(f"Starting download of LULC tiles for MGRS tile '{target_tile_id}' from {start_year} to {end_year}...")
    
    for year in range(start_year, end_year + 1):
        download_lulc_tile(year, target_tile_id, download_directory)
        
    print("\nDownload process completed.")
    print(f"Check the '{download_directory}' folder for your GeoTIFF files.")

    print("\n--- Important Note on Coverage ---")
    print(f"The downloaded files are only for the Sentinel-2 MGRS tile '{target_tile_id}'.")
    print("Bangladesh is covered by multiple MGRS tiles (e.g., 45Q, 46Q, 45R, 46R).")
    print("If you need full coverage of Bangladesh, you will need to find the corresponding links for other tiles.")


! pwd


import os
import subprocess
import tempfile

# --- Configuration ---
# Set the root directory where your LULC_Annual_Tiles folders are located
# Make sure this path is correct for your system.
ROOT_LULC_PARENT_DIR = os.path.expanduser("~/Research/Five_Rivers") # This is now correctly expanded internally

# Define the output directory for your merged annual LULC files
OUTPUT_MERGED_LULC_DIR = os.path.join(ROOT_LULC_PARENT_DIR, "gis", "Merged_Annual_LULC")

# Path to your gdal_merge.py executable
GDAL_MERGE_PATH = "/opt/local/Library/Frameworks/Python.framework/Versions/3.12/bin/gdal_merge.py"
PYTHON_EXECUTABLE = "/opt/local/Library/Frameworks/Python.framework/Versions/3.12/bin/python3"

# Define the years you want to process
YEARS_TO_PROCESS = range(2017, 2023)

# Define the structure of your tile folders and their corresponding MGRS prefixes
LULC_TILE_STRUCTURE = [
    ('LULC_Annual_Tiles1', '46R'),
    ('LULC_Annual_Tiles2', '45R'),
    ('LULC_Annual_Tiles3', '45Q'),
    ('LULC_Annual_Tiles4', '46Q')
]

# --- Create Output Directory if it doesn't exist ---
os.makedirs(OUTPUT_MERGED_LULC_DIR, exist_ok=True)
print(f"Merged LULC files will be saved to: {OUTPUT_MERGED_LULC_DIR}")
print("-" * 50)

# --- Loop through each year to perform the merge ---
for year in YEARS_TO_PROCESS:
    print(f"Processing year: {year}")
    
    input_files_for_year = []
    
    # Collect all input TIFF files for the current year
    for folder_name, tile_prefix in LULC_TILE_STRUCTURE:
        # Construct the filename pattern like '46R_20170101-20180101.tif'
        filename = f"{tile_prefix}_{year}0101-{year+1}0101.tif"
        full_tile_path = os.path.join(ROOT_LULC_PARENT_DIR, folder_name, filename)
        
        # --- DEBUG PRINT: Print the exact path being checked ---
        print(f"  Checking for file: {full_tile_path}")
        
        if os.path.exists(full_tile_path):
            input_files_for_year.append(full_tile_path)
        else:
            print(f"  Warning: File not found for {year} in {folder_name}: {full_tile_path}. Skipping.")
            
    if not input_files_for_year:
        print(f"  No input files found for {year}. Skipping merge for this year.")
        print("-" * 50)
        continue

    # Create a temporary file to store the list of input TIFFs for gdal_merge.py
    temp_optfile = None
    try:
        temp_optfile = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')
        temp_optfile.write('\n'.join(input_files_for_year))
        temp_optfile.close()
        optfile_path = temp_optfile.name
        
        print(f"  Temporary optfile created at: {optfile_path}")

        # Define the output file path for the merged annual LULC
        output_merged_file = os.path.join(OUTPUT_MERGED_LULC_DIR, f"LULC{year}.tif")

        # Construct the gdal_merge.py command as a list of arguments
        command = [
            PYTHON_EXECUTABLE,
            GDAL_MERGE_PATH,
            '-ot', 'Float32',
            '-of', 'GTiff',
            '-o', output_merged_file,
            '--optfile', optfile_path
        ]

        print(f"  Merging files for {year} using command: {' '.join(command)}")
        
        process = subprocess.run(command, capture_output=True, text=True, check=False)

        if process.returncode == 0:
            print(f"  Successfully merged {year} LULC to: {output_merged_file}")
        else:
            print(f"  ERROR merging {year} LULC:")
            print(f"    Stdout: {process.stdout}")
            print(f"    Stderr: {process.stderr}")
            print(f"    Command: {' '.join(command)}")

    except Exception as e:
        print(f"  An error occurred during merge for {year}: {e}")
    finally:
        if temp_optfile and os.path.exists(optfile_path):
            os.remove(optfile_path)
            print(f"  Cleaned up temporary optfile: {optfile_path}")
            
    print("-" * 50)

print("\nAll annual LULC merging tasks completed.")
print(f"Check the '{os.path.basename(OUTPUT_MERGED_LULC_DIR)}' folder inside '{os.path.dirname(OUTPUT_MERGED_LULC_DIR)}' for your merged files.")


!tree


















