import numpy as np
import pandas as pd
import rasterio
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
import geopandas as gpd
from shapely.geometry import Point, mapping
from rasterio.mask import mask
from rasterio.warp import transform_geom
import os

# Define directories
lulc_dir = "LULCMerged"
calindices_dir = "CalIndices"
idw_dir = "IDW"
csv_file = "data/sampling_features_with_hydro_lulc.csv"

# Load data
csv_data = pd.read_csv(csv_file)
y_data = csv_data["AsR"].values
sample_points = gpd.read_file("sampling_point100.shp")

# Define buffer radius (in meters)
buffer_radius = 4300

# ==============================================
# CORRECT CRS FIX: Align all data to a single projected CRS
# ==============================================

# First, determine the correct CRS from one of the rasters.
# Assuming LULC raster is representative.
try:
    # Filter for .tif files and get the first one
    lulc_files = [f for f in os.listdir(lulc_dir) if f.endswith('.tif')]
    if not lulc_files:
        raise FileNotFoundError("No .tif files found in the LULC directory.")
    
    first_raster_path = os.path.join(lulc_dir, lulc_files[0])
    with rasterio.open(first_raster_path) as src:
        target_crs = src.crs
        print(f"Target CRS from raster: {target_crs}")
except IndexError:
    raise FileNotFoundError("Could not find any raster files in the LULC directory.")

# Now, re-project the sample points to this target CRS.
sample_points = sample_points.to_crs(target_crs)

def extract_raster_data_at_point(point_geometry, raster_path, buffer_radius):
    """Extract buffered area from raster for a point geometry."""
    with rasterio.open(raster_path) as src:
        # Get the original nodata value from the source raster
        raster_nodata = src.nodata
        
        # If the raster has no existing nodata value, or if it's float,
        # we need to provide a suitable integer value for integer dtypes.
        if src.dtypes[0] in ['uint8', 'int16', 'int32'] and (raster_nodata is None or np.isnan(raster_nodata)):
            # Use a common integer value for nodata for integer rasters
            nodata_value = 0
        else:
            # Otherwise, use the raster's existing nodata value
            nodata_value = raster_nodata
            
        geometry = point_geometry.buffer(buffer_radius)

        try:
            out_image, out_transform = mask(
                src,
                [mapping(geometry)],
                crop=True,
                nodata=nodata_value, # Use the corrected nodata value
                all_touched=True
            )
            return out_image[0]
        except ValueError as e:
            return None


            
def process_point(point, buffer_radius):
    """Process all rasters for a single point"""
    raster_data_list = []
    
    # Process LULC rasters
    for lulc_raster in os.listdir(lulc_dir):
        if lulc_raster.endswith(('.tif', '.tiff')):
            lulc_path = os.path.join(lulc_dir, lulc_raster)
            data = extract_raster_data_at_point(point.geometry, lulc_path, buffer_radius)
            if data is not None:
                if data.shape != (64, 64):
                    data = tf.image.resize(np.expand_dims(data, axis=-1), (64, 64)).numpy().squeeze()
                raster_data_list.append(data)
    
    # Process CalIndices rasters
    for calindex in os.listdir(calindices_dir):
        if calindex.endswith(('.tif', '.tiff')):
            calindex_path = os.path.join(calindices_dir, calindex)
            data = extract_raster_data_at_point(point.geometry, calindex_path, buffer_radius)
            if data is not None:
                if data.shape != (64, 64):
                    data = tf.image.resize(np.expand_dims(data, axis=-1), (64, 64)).numpy().squeeze()
                raster_data_list.append(data)
    
    # Process IDW rasters
    for idw_raster in os.listdir(idw_dir):
        if idw_raster.endswith(('.tif', '.tiff')):
            idw_path = os.path.join(idw_dir, idw_raster)
            data = extract_raster_data_at_point(point.geometry, idw_path, buffer_radius)
            if data is not None:
                if data.shape != (64, 64):
                    data = tf.image.resize(np.expand_dims(data, axis=-1), (64, 64)).numpy().squeeze()
                raster_data_list.append(data)
    
    if raster_data_list:
        return np.stack(raster_data_list, axis=-1)
    return None

# Main processing loop
X_rasters = []
valid_indices = []

for i, point in sample_points.iterrows():
    combined_data = process_point(point, buffer_radius)
    
    if combined_data is not None:
        X_rasters.append(combined_data)
        valid_indices.append(i)

# Filter y_data to only include valid points
y_data = y_data[valid_indices]

# Convert to numpy arrays
X_rasters = np.array(X_rasters)
y_data = np.array(y_data)

# Check shapes
print(f"X shape: {X_rasters.shape}, y shape: {y_data.shape}")

# Normalize data
# Assuming raster values are from 0-255 for simplicity. You might need to adjust this.
X_rasters = X_rasters / 255.0
scaler = StandardScaler()
y_scaled = scaler.fit_transform(y_data.reshape(-1, 1)).flatten()

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_rasters, y_scaled, test_size=0.25, random_state=42
)

print(f"Train shapes - X: {X_train.shape}, y: {y_train.shape}")
print(f"Test shapes - X: {X_test.shape}, y: {y_test.shape}")


import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

# Assuming X_train, X_test, y_train, y_test are already loaded and preprocessed

def create_cnn_model(input_shape):
    """Creates a more regularized CNN model for raster data."""
    model_input = Input(shape=input_shape)

    # Convolutional layers with L2 regularization
    x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01))(model_input)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01))(x)
    x = MaxPooling2D((2, 2))(x)

    # Flatten the output for the Dense layers
    x = Flatten()(x)

    # Dense layers with L2 regularization and reduced Dropout
    x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)
    x = Dropout(0.3)(x)
    output = Dense(1)(x)

    model = Model(inputs=model_input, outputs=output)
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

def create_gnn_model(input_shape):
    """Creates a simplified, regularized GNN-like model."""
    model_input = Input(shape=input_shape)
    x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01))(model_input)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01))(x)
    x = MaxPooling2D((2, 2))(x)
    x = Flatten()(x)
    x = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(x)
    x = Dropout(0.3)(x)
    output = Dense(1)(x)
    model = Model(inputs=model_input, outputs=output)
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

def create_mlp_model(input_shape):
    """Creates a simple, regularized Multi-Layer Perceptron model."""
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.01)),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# 1. Create the models with the correct input shapes
cnn_model = create_cnn_model(X_train.shape[1:])
gnn_model = create_gnn_model(X_train.shape[1:])

X_train_mlp = X_train.reshape(X_train.shape[0], -1)
X_test_mlp = X_test.reshape(X_test.shape[0], -1)
mlp_model = create_mlp_model(X_train_mlp.shape[1])

# 2. Train the models with Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

print("Training CNN Model...")
cnn_model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])

print("Training GNN-like Model...")
gnn_model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])

print("Training MLP Model...")
mlp_model.fit(X_train_mlp, y_train, epochs=200, batch_size=16, validation_data=(X_test_mlp, y_test), callbacks=[early_stopping])

# 3. Stack ensemble model
def ensemble_predictions(cnn_preds, gnn_preds, mlp_preds, weights):
    return (cnn_preds.flatten() * weights[0] + 
            gnn_preds.flatten() * weights[1] + 
            mlp_preds.flatten() * weights[2])

# Get predictions from each model
cnn_preds_train = cnn_model.predict(X_train)
gnn_preds_train = gnn_model.predict(X_train)
mlp_preds_train = mlp_model.predict(X_train_mlp)

cnn_preds_test = cnn_model.predict(X_test)
gnn_preds_test = gnn_model.predict(X_test)
mlp_preds_test = mlp_model.predict(X_test_mlp)

# Final ensemble predictions
ensemble_preds_train = ensemble_predictions(cnn_preds_train, gnn_preds_train, mlp_preds_train, [0.4, 0.3, 0.3])
ensemble_preds_test = ensemble_predictions(cnn_preds_test, gnn_preds_test, mlp_preds_test, [0.4, 0.3, 0.3])

# Evaluate ensemble model
r2_train = r2_score(y_train, ensemble_preds_train)
rmse_train = np.sqrt(mean_squared_error(y_train, ensemble_preds_train))

r2_test = r2_score(y_test, ensemble_preds_test)
rmse_test = np.sqrt(mean_squared_error(y_test, ensemble_preds_test))

print(f"Train R²: {r2_train:.4f}, Train RMSE: {rmse_train:.4f}")
print(f"Test R²: {r2_test:.4f}, Test RMSE: {rmse_test:.4f}")











import pandas as pd
import numpy as np
import glob
import os
import rasterio
from rasterio.windows import Window
from scipy.spatial import distance_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import Sequence
import gc # Import garbage collector

# ==================== 1. Load Data ==================== #
orig = pd.read_csv("../data/RainySeason.csv")
river_100 = pd.read_csv("data/river_200_samples_rainy.csv")

drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('AsR')

# Train-test split
train_orig = orig.sample(10, random_state=42)
test_orig = orig.drop(train_orig.index)
train_combined = pd.concat([river_100, train_orig], ignore_index=True)

# ==================== 2. Collect ALL Rasters ==================== #
raster_paths = []
raster_paths += glob.glob("CalIndices/*.tif")
raster_paths += glob.glob("LULCMerged/*.tif")
raster_paths += glob.glob("IDW/*.tif")

print(f"Using {len(raster_paths)} raster layers for CNN input.")
for r in raster_paths:
    print("  -", os.path.basename(r))

# ==================== 3. Create a Custom Data Generator ==================== #
def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    """
    Extracts a batch of patches from rasters for a given set of coordinates.
    This function is optimized to be called by the data generator for each batch.
    """
    patches = []
    # Loop through each coordinate pair in the batch
    for lon, lat in coords:
        channels = []
        # Loop through each raster file to get a single patch for each raster
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)

                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)

class DataGenerator(Sequence):
    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, batch_size=4, shuffle=True, buffer_meters=1000, **kwargs):
        super().__init__(**kwargs)
        self.coords = coords
        self.mlp_data = mlp_data
        self.gnn_data = gnn_data
        self.y = y
        self.raster_paths = raster_paths
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indices = np.arange(len(self.y))
        self.buffer_meters = buffer_meters

        # Pre-calculate patch size from the first raster
        with rasterio.open(raster_paths[0]) as src:
            res_x, res_y = src.res
            self.buffer_pixels_x = int(self.buffer_meters / res_x)
            self.buffer_pixels_y = int(self.buffer_meters / res_y)
            self.patch_width = 2 * self.buffer_pixels_x
            self.patch_height = 2 * self.buffer_pixels_y

        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.y) / self.batch_size))

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
            
    def __getitem__(self, index):
        # Get batch indices
        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]

        # Get batch data
        batch_coords = self.coords[batch_indices]
        batch_mlp = self.mlp_data[batch_indices]
        
        # Slice the GNN adjacency matrix for the current batch
        # The GNN input is now a (batch_size, num_train_samples) matrix
        batch_gnn = self.gnn_data[batch_indices, :]

        batch_y = self.y[batch_indices]

        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            self.raster_paths,
            self.buffer_pixels_x,
            self.buffer_pixels_y,
            self.patch_width,
            self.patch_height
        )

        # Return a tuple of inputs and the target, which Keras expects
        return (batch_cnn, batch_mlp, batch_gnn), batch_y


# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #
coords_train = train_combined[['Long','Lat']].values
coords_test = test_orig[['Long','Lat']].values
dist_mat_train = distance_matrix(coords_train, coords_train)
gnn_train = np.exp(-dist_mat_train/10)
dist_mat_test_train = distance_matrix(coords_test, coords_train)
gnn_test = np.exp(-dist_mat_test_train/10)

scaler = StandardScaler()
mlp_train = scaler.fit_transform(train_combined[numeric_cols])
mlp_test = scaler.transform(test_orig[numeric_cols])
y_train = train_combined['AsR'].values
y_test = test_orig['AsR'].values


# ==================== 5. Define Enhanced CNN–GNN–MLP Model ==================== #
def build_fusion_model(patch_shape, gnn_dim, mlp_dim):
    # CNN branch (for raster data)
    cnn_input = Input(shape=patch_shape, name="cnn_input")
    x = Conv2D(32, (3,3), activation="relu")(cnn_input)
    x = MaxPooling2D((2,2))(x)
    x = Conv2D(64, (3,3), activation="relu")(x)
    x = MaxPooling2D((2,2))(x)
    x = Flatten()(x)
    cnn_out = Dense(128, activation="relu", name="cnn_out")(x)

    # MLP branch (for numerical site features)
    mlp_input = Input(shape=(mlp_dim,), name="mlp_input")
    m = Dense(64, activation="relu")(mlp_input)
    mlp_out = Dense(32, activation="relu", name="mlp_out")(m)

    # GNN branch (for spatial connectivity)
    # The GNN input dimension is now the number of training samples
    gnn_input = Input(shape=(gnn_dim,), name="gnn_input")
    g = Dense(64, activation="relu")(gnn_input)
    gnn_out = Dense(32, activation="relu", name="gnn_out")(g)

    # Fusion Layer
    combined = Concatenate()([cnn_out, mlp_out, gnn_out])
    f = Dense(128, activation="relu")(combined)
    f = Dropout(0.4)(f)
    f = Dense(64, activation="relu")(f)
    output = Dense(1, activation="linear", name="final_output")(f)

    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss="mse")
    return model

# We need to determine the final GNN input dimension for the model
# It's the total number of training samples
batch_size = 4
gnn_input_dim = len(coords_train)
cnn_patch_shape = (2*int(1000/rasterio.open(raster_paths[0]).res[0]), 2*int(1000/rasterio.open(raster_paths[0]).res[0]), len(raster_paths))
model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])
model.summary()

# ==================== 6. Create Data Generators ==================== #
# We create a separate generator for the validation data.
train_generator = DataGenerator(
    coords=coords_train,
    mlp_data=mlp_train,
    gnn_data=gnn_train,
    y=y_train,
    raster_paths=raster_paths,
    batch_size=batch_size,
    shuffle=True
)

# For evaluation, we will create a generator for the full test set.
# The shuffle is set to False for consistent evaluation results.
def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, batch_size=4):
    num_samples = len(y_test)
    y_pred_list = []
    
    # Pre-calculate patch size from the first raster for evaluation
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(1000 / res_x)
        buffer_pixels_y = int(1000 / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    for i in range(0, num_samples, batch_size):
        batch_coords = coords_test[i:i+batch_size]
        batch_mlp = mlp_test[i:i+batch_size]
        
        # Get the corresponding slice of the test GNN input matrix
        batch_gnn = gnn_test_matrix[i:i+batch_size, :]
        batch_y = y_test[i:i+batch_size]

        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            raster_paths,
            buffer_pixels_x,
            buffer_pixels_y,
            patch_width,
            patch_height
        )
        
        # Make predictions on the batch and append to list
        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())
        
    y_pred = np.concatenate(y_pred_list)
    
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    return r2, rmse


# ==================== 7. Train Model ==================== #
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

history = model.fit(
    train_generator,
    epochs=100,
    verbose=1,
    callbacks=[early_stopping],
    validation_data=train_generator # Using the same generator for validation for this example
)


# ==================== 8. Evaluate ==================== #
# Re-create a data generator without shuffling for evaluation on the training set
train_eval_generator = DataGenerator(
    coords=coords_train,
    mlp_data=mlp_train,
    gnn_data=gnn_train,
    y=y_train,
    raster_paths=raster_paths,
    batch_size=batch_size,
    shuffle=False
)

y_pred_train = model.predict(train_eval_generator).flatten()
r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)
rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))

r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, batch_size)


print(f"\n Enhanced CNN–GNN–MLP Model Performance (All Rasters):")
print(f"R² Train: {r2_train:.4f} | RMSE Train: {rmse_test:.4f}")
print(f"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}")











import pandas as pd
import numpy as np
import glob
import os
import rasterio
from rasterio.windows import Window
from scipy.spatial import distance_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import Sequence
import gc # Import garbage collector

# List of buffer sizes to test
BUFFER_SIZES_TO_TEST = [1000, 2000, 3000, 5000]

# ==================== 1. Load Data ==================== #
orig = pd.read_csv("../data/RainySeason.csv")
river_100 = pd.read_csv("data/river_200_samples_rainy.csv")

drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('AsR')

# Train-test split
train_orig = orig.sample(10, random_state=42)
test_orig = orig.drop(train_orig.index)
train_combined = pd.concat([river_100, train_orig], ignore_index=True)

# ==================== 2. Collect ALL Rasters ==================== #
raster_paths = []
raster_paths += glob.glob("CalIndices/*.tif")
raster_paths += glob.glob("LULCMerged/*.tif")
raster_paths += glob.glob("IDW/*.tif")

print(f"Using {len(raster_paths)} raster layers for CNN input.")
for r in raster_paths:
    print("  -", os.path.basename(r))

# ==================== 3. Create a Custom Data Generator ==================== #
def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    """
    Extracts a batch of patches from rasters for a given set of coordinates.
    This function is optimized to be called by the data generator for each batch.
    """
    patches = []
    # Loop through each coordinate pair in the batch
    for lon, lat in coords:
        channels = []
        # Loop through each raster file to get a single patch for each raster
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)

                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)

class DataGenerator(Sequence):
    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):
        super().__init__(**kwargs)
        self.coords = coords
        self.mlp_data = mlp_data
        self.gnn_data = gnn_data
        self.y = y
        self.raster_paths = raster_paths
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indices = np.arange(len(self.y))
        self.buffer_meters = buffer_meters

        # Pre-calculate patch size from the first raster
        with rasterio.open(raster_paths[0]) as src:
            res_x, res_y = src.res
            self.buffer_pixels_x = int(self.buffer_meters / res_x)
            self.buffer_pixels_y = int(self.buffer_meters / res_y)
            self.patch_width = 2 * self.buffer_pixels_x
            self.patch_height = 2 * self.buffer_pixels_y

        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.y) / self.batch_size))

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
            
    def __getitem__(self, index):
        # Get batch indices
        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]

        # Get batch data
        batch_coords = self.coords[batch_indices]
        batch_mlp = self.mlp_data[batch_indices]
        
        # Slice the GNN adjacency matrix for the current batch
        # The GNN input is now a (batch_size, num_train_samples) matrix
        batch_gnn = self.gnn_data[batch_indices, :]

        batch_y = self.y[batch_indices]

        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            self.raster_paths,
            self.buffer_pixels_x,
            self.buffer_pixels_y,
            self.patch_width,
            self.patch_height
        )

        # Return a tuple of inputs and the target, which Keras expects
        return (batch_cnn, batch_mlp, batch_gnn), batch_y


# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #
coords_train = train_combined[['Long','Lat']].values
coords_test = test_orig[['Long','Lat']].values
dist_mat_train = distance_matrix(coords_train, coords_train)
gnn_train = np.exp(-dist_mat_train/10)
dist_mat_test_train = distance_matrix(coords_test, coords_train)
gnn_test = np.exp(-dist_mat_test_train/10)

scaler = StandardScaler()
mlp_train = scaler.fit_transform(train_combined[numeric_cols])
mlp_test = scaler.transform(test_orig[numeric_cols])
y_train = train_combined['AsR'].values
y_test = test_orig['AsR'].values


# ==================== 5. Define Enhanced CNN–GNN–MLP Model ==================== #
def build_fusion_model(patch_shape, gnn_dim, mlp_dim):
    # CNN branch (for raster data)
    cnn_input = Input(shape=patch_shape, name="cnn_input")
    x = Conv2D(32, (3,3), activation="relu")(cnn_input)
    x = MaxPooling2D((2,2))(x)
    x = Conv2D(64, (3,3), activation="relu")(x)
    x = MaxPooling2D((2,2))(x)
    x = Flatten()(x)
    cnn_out = Dense(128, activation="relu", name="cnn_out")(x)

    # MLP branch (for numerical site features)
    mlp_input = Input(shape=(mlp_dim,), name="mlp_input")
    m = Dense(64, activation="relu")(mlp_input)
    mlp_out = Dense(32, activation="relu", name="mlp_out")(m)

    # GNN branch (for spatial connectivity)
    # The GNN input dimension is now the number of training samples
    gnn_input = Input(shape=(gnn_dim,), name="gnn_input")
    g = Dense(64, activation="relu")(gnn_input)
    gnn_out = Dense(32, activation="relu", name="gnn_out")(g)

    # Fusion Layer
    combined = Concatenate()([cnn_out, mlp_out, gnn_out])
    f = Dense(128, activation="relu")(combined)
    f = Dropout(0.4)(f)
    f = Dense(64, activation="relu")(f)
    output = Dense(1, activation="linear", name="final_output")(f)

    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss="mse")
    return model

def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters, batch_size=4, return_preds=False):
    num_samples = len(y_test)
    y_pred_list = []
    
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(buffer_meters / res_x)
        buffer_pixels_y = int(buffer_meters / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    for i in range(0, num_samples, batch_size):
        batch_coords = coords_test[i:i+batch_size]
        batch_mlp = mlp_test[i:i+batch_size]
        
        batch_gnn = gnn_test_matrix[i:i+batch_size, :]
        batch_y = y_test[i:i+batch_size]

        batch_cnn = extract_patch_for_generator(
            batch_coords,
            raster_paths,
            buffer_pixels_x,
            buffer_pixels_y,
            patch_width,
            patch_height
        )
        
        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())
        
    y_pred = np.concatenate(y_pred_list)
    
    if return_preds:
        return y_pred
    else:
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        return r2, rmse


# ==================== Loop through buffer sizes for analysis ==================== #
for BUFFER_METERS in BUFFER_SIZES_TO_TEST:
    print("\n" + "="*80)
    print(f"Analyzing for BUFFER_METERS = {BUFFER_METERS}m")
    print("="*80)

    # We need to determine the final GNN input dimension for the model
    # It's the total number of training samples
    batch_size = 4
    gnn_input_dim = len(coords_train)
    
    # Calculate CNN patch shape based on the current buffer size
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(BUFFER_METERS / res_x)
        patch_width = 2 * buffer_pixels_x
        cnn_patch_shape = (patch_width, patch_width, len(raster_paths))

    model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])
    model.summary()

    # ==================== 6. Create Data Generators ==================== #
    train_generator = DataGenerator(
        coords=coords_train,
        mlp_data=mlp_train,
        gnn_data=gnn_train,
        y=y_train,
        raster_paths=raster_paths,
        buffer_meters=BUFFER_METERS,
        batch_size=batch_size,
        shuffle=True
    )

    # ==================== 7. Train Model ==================== #
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )

    history = model.fit(
        train_generator,
        epochs=100,
        verbose=1,
        callbacks=[early_stopping],
        validation_data=train_generator
    )

    # ==================== 8. Evaluate ==================== #
    y_pred_train = model.predict(train_generator).flatten()
    r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)
    rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))
    
    r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)

    print(f"\n Enhanced CNN–GNN–MLP Model Performance ({BUFFER_METERS}m):")
    print(f"R² Train: {r2_train:.4f} | RMSE Train: {rmse_test:.4f}")
    print(f"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}")

    # ==================== 9. Feature Importance Analysis ==================== #
    print("\n" + "-"*50)
    print(f"Feature Importance Analysis for {BUFFER_METERS}m")
    print("-"*50)

    # --- 9.1 Combined Feature Importance (by Model Branch) ---
    y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)
    baseline_r2 = r2_score(y_test, y_pred_baseline)

    print(f"\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}")

    # Ablate CNN branch
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(BUFFER_METERS / res_x)
        buffer_pixels_y = int(BUFFER_METERS / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    cnn_test_ablated = np.zeros_like(extract_patch_for_generator(
        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
    ))
    y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test)).flatten()
    r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)
    importance_cnn = baseline_r2 - r2_cnn_ablated

    # Ablate MLP branch
    mlp_test_ablated = np.zeros_like(mlp_test)
    y_pred_mlp_ablated = model.predict((extract_patch_for_generator(
        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
    ), mlp_test_ablated, gnn_test)).flatten()
    r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)
    importance_mlp = baseline_r2 - r2_mlp_ablated

    # Ablate GNN branch
    gnn_test_ablated = np.zeros_like(gnn_test)
    y_pred_gnn_ablated = model.predict((extract_patch_for_generator(
        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
    ), mlp_test, gnn_test_ablated)).flatten()
    r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)
    importance_gnn = baseline_r2 - r2_gnn_ablated

    print("\n--- Combined Feature Importance (by Model Branch) ---")
    print(f"CNN Branch Importance (R² drop): {importance_cnn:.4f}")
    print(f"MLP Branch Importance (R² drop): {importance_mlp:.4f}")
    print(f"GNN Branch Importance (R² drop): {importance_gnn:.4f}")

    # --- 9.2 MLP Feature Importance (Permutation-based) ---
    mlp_feature_importance = {}
    for i, feature_name in enumerate(numeric_cols):
        mlp_test_shuffled = np.copy(mlp_test)
        np.random.shuffle(mlp_test_shuffled[:, i])
        
        y_pred_shuffled = model.predict((extract_patch_for_generator(
            coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
        ), mlp_test_shuffled, gnn_test)).flatten()
        r2_shuffled = r2_score(y_test, y_pred_shuffled)
        
        importance = baseline_r2 - r2_shuffled
        mlp_feature_importance[feature_name] = importance

    print("\n--- MLP Feature Importance (Permutation-based) ---")
    sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)
    for feature, importance in sorted_importance:
        print(f"{feature:<20}: {importance:.4f}")
    
    # Garbage collect to free up memory before the next loop iteration
    del model, history, train_generator
    gc.collect()
















import pandas as pd
import numpy as np
import glob
import os
import rasterio
from rasterio.windows import Window
from scipy.spatial import distance_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import Sequence
import gc # Import garbage collector

# Define the buffer size in meters
BUFFER_METERS = 500

# ==================== 1. Load Data ==================== #
orig = pd.read_csv("../data/RainySeason.csv")
river_100 = pd.read_csv("data/river_200_samples_rainy.csv")

drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('AsR')

# Train-test split
train_orig = orig.sample(10, random_state=42)
test_orig = orig.drop(train_orig.index)
train_combined = pd.concat([river_100, train_orig], ignore_index=True)

# ==================== 2. Collect ALL Rasters ==================== #
raster_paths = []
raster_paths += glob.glob("CalIndices/*.tif")
raster_paths += glob.glob("LULCMerged/*.tif")
raster_paths += glob.glob("IDW/*.tif")

print(f"Using {len(raster_paths)} raster layers for CNN input.")
for r in raster_paths:
    print("  -", os.path.basename(r))

# ==================== 3. Create a Custom Data Generator ==================== #
def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    """
    Extracts a batch of patches from rasters for a given set of coordinates.
    This function is optimized to be called by the data generator for each batch.
    """
    patches = []
    # Loop through each coordinate pair in the batch
    for lon, lat in coords:
        channels = []
        # Loop through each raster file to get a single patch for each raster
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)

                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)

class DataGenerator(Sequence):
    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, batch_size=4, shuffle=True, buffer_meters=BUFFER_METERS, **kwargs):
        super().__init__(**kwargs)
        self.coords = coords
        self.mlp_data = mlp_data
        self.gnn_data = gnn_data
        self.y = y
        self.raster_paths = raster_paths
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indices = np.arange(len(self.y))
        self.buffer_meters = buffer_meters

        # Pre-calculate patch size from the first raster
        with rasterio.open(raster_paths[0]) as src:
            res_x, res_y = src.res
            self.buffer_pixels_x = int(self.buffer_meters / res_x)
            self.buffer_pixels_y = int(self.buffer_meters / res_y)
            self.patch_width = 2 * self.buffer_pixels_x
            self.patch_height = 2 * self.buffer_pixels_y

        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.y) / self.batch_size))

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
            
    def __getitem__(self, index):
        # Get batch indices
        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]

        # Get batch data
        batch_coords = self.coords[batch_indices]
        batch_mlp = self.mlp_data[batch_indices]
        
        # Slice the GNN adjacency matrix for the current batch
        # The GNN input is now a (batch_size, num_train_samples) matrix
        batch_gnn = self.gnn_data[batch_indices, :]

        batch_y = self.y[batch_indices]

        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            self.raster_paths,
            self.buffer_pixels_x,
            self.buffer_pixels_y,
            self.patch_width,
            self.patch_height
        )

        # Return a tuple of inputs and the target, which Keras expects
        return (batch_cnn, batch_mlp, batch_gnn), batch_y


# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #
coords_train = train_combined[['Long','Lat']].values
coords_test = test_orig[['Long','Lat']].values
dist_mat_train = distance_matrix(coords_train, coords_train)
gnn_train = np.exp(-dist_mat_train/10)
dist_mat_test_train = distance_matrix(coords_test, coords_train)
gnn_test = np.exp(-dist_mat_test_train/10)

scaler = StandardScaler()
mlp_train = scaler.fit_transform(train_combined[numeric_cols])
mlp_test = scaler.transform(test_orig[numeric_cols])
y_train = train_combined['AsR'].values
y_test = test_orig['AsR'].values


# ==================== 5. Define Enhanced CNN–GNN–MLP Model ==================== #
def build_fusion_model(patch_shape, gnn_dim, mlp_dim):
    # CNN branch (for raster data)
    cnn_input = Input(shape=patch_shape, name="cnn_input")
    x = Conv2D(32, (3,3), activation="relu")(cnn_input)
    x = MaxPooling2D((2,2))(x)
    x = Conv2D(64, (3,3), activation="relu")(x)
    x = MaxPooling2D((2,2))(x)
    x = Flatten()(x)
    cnn_out = Dense(128, activation="relu", name="cnn_out")(x)

    # MLP branch (for numerical site features)
    mlp_input = Input(shape=(mlp_dim,), name="mlp_input")
    m = Dense(64, activation="relu")(mlp_input)
    mlp_out = Dense(32, activation="relu", name="mlp_out")(m)

    # GNN branch (for spatial connectivity)
    # The GNN input dimension is now the number of training samples
    gnn_input = Input(shape=(gnn_dim,), name="gnn_input")
    g = Dense(64, activation="relu")(gnn_input)
    gnn_out = Dense(32, activation="relu", name="gnn_out")(g)

    # Fusion Layer
    combined = Concatenate()([cnn_out, mlp_out, gnn_out])
    f = Dense(128, activation="relu")(combined)
    f = Dropout(0.4)(f)
    f = Dense(64, activation="relu")(f)
    output = Dense(1, activation="linear", name="final_output")(f)

    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss="mse")
    return model

# We need to determine the final GNN input dimension for the model
# It's the total number of training samples
batch_size = 4
gnn_input_dim = len(coords_train)
cnn_patch_shape = (2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), len(raster_paths))
model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])
model.summary()

# ==================== 6. Create Data Generators ==================== #
# We create a separate generator for the validation data.
train_generator = DataGenerator(
    coords=coords_train,
    mlp_data=mlp_train,
    gnn_data=gnn_train,
    y=y_train,
    raster_paths=raster_paths,
    batch_size=batch_size,
    shuffle=True,
    buffer_meters=BUFFER_METERS
)

# For evaluation, we will create a generator for the full test set.
# The shuffle is set to False for consistent evaluation results.
def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=4):
    num_samples = len(y_test)
    y_pred_list = []
    
    # Pre-calculate patch size from the first raster for evaluation
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(buffer_meters / res_x)
        buffer_pixels_y = int(buffer_meters / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    for i in range(0, num_samples, batch_size):
        batch_coords = coords_test[i:i+batch_size]
        batch_mlp = mlp_test[i:i+batch_size]
        
        # Get the corresponding slice of the test GNN input matrix
        batch_gnn = gnn_test_matrix[i:i+batch_size, :]
        batch_y = y_test[i:i+batch_size]

        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            raster_paths,
            buffer_pixels_x,
            buffer_pixels_y,
            patch_width,
            patch_height
        )
        
        # Make predictions on the batch and append to list
        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())
        
    y_pred = np.concatenate(y_pred_list)
    
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    return r2, rmse


# ==================== 7. Train Model ==================== #
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

history = model.fit(
    train_generator,
    epochs=100,
    verbose=1,
    callbacks=[early_stopping],
    validation_data=train_generator # Using the same generator for validation for this example
)


# ==================== 8. Evaluate ==================== #
# Re-create a data generator without shuffling for evaluation on the training set
train_eval_generator = DataGenerator(
    coords=coords_train,
    mlp_data=mlp_train,
    gnn_data=gnn_train,
    y=y_train,
    raster_paths=raster_paths,
    batch_size=batch_size,
    shuffle=False,
    buffer_meters=BUFFER_METERS
)

y_pred_train = model.predict(train_eval_generator).flatten()
r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)
rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))

r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)


print(f"\n Enhanced CNN–GNN–MLP Model Performance (All Rasters):")
print(f"R² Train: {r2_train:.4f} | RMSE Train: {rmse_test:.4f}")
print(f"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}")



import pandas as pd
import numpy as np
import glob
import os
import rasterio
from rasterio.windows import Window
from scipy.spatial import distance_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import Sequence
import gc # Import garbage collector

# Define the buffer size in meters
BUFFER_METERS = 500

# ==================== 1. Load Data ==================== #
orig = pd.read_csv("../data/RainySeason.csv")
river_100 = pd.read_csv("data/river_200_samples_rainy.csv")

drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('AsR')

# Train-test split
train_orig = orig.sample(10, random_state=42)
test_orig = orig.drop(train_orig.index)
train_combined = pd.concat([river_100, train_orig], ignore_index=True)

# ==================== 2. Collect ALL Rasters ==================== #
raster_paths = []
raster_paths += glob.glob("CalIndices/*.tif")
raster_paths += glob.glob("LULCMerged/*.tif")
raster_paths += glob.glob("IDW/*.tif")

print(f"Using {len(raster_paths)} raster layers for CNN input.")
for r in raster_paths:
    print("  -", os.path.basename(r))

# ==================== 3. Create a Custom Data Generator ==================== #
def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    """
    Extracts a batch of patches from rasters for a given set of coordinates.
    This function is optimized to be called by the data generator for each batch.
    """
    patches = []
    # Loop through each coordinate pair in the batch
    for lon, lat in coords:
        channels = []
        # Loop through each raster file to get a single patch for each raster
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)

                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)

class DataGenerator(Sequence):
    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, batch_size=4, shuffle=True, buffer_meters=BUFFER_METERS, **kwargs):
        super().__init__(**kwargs)
        self.coords = coords
        self.mlp_data = mlp_data
        self.gnn_data = gnn_data
        self.y = y
        self.raster_paths = raster_paths
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indices = np.arange(len(self.y))
        self.buffer_meters = buffer_meters

        # Pre-calculate patch size from the first raster
        with rasterio.open(raster_paths[0]) as src:
            res_x, res_y = src.res
            self.buffer_pixels_x = int(self.buffer_meters / res_x)
            self.buffer_pixels_y = int(self.buffer_meters / res_y)
            self.patch_width = 2 * self.buffer_pixels_x
            self.patch_height = 2 * self.buffer_pixels_y

        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.y) / self.batch_size))

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
            
    def __getitem__(self, index):
        # Get batch indices
        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]

        # Get batch data
        batch_coords = self.coords[batch_indices]
        batch_mlp = self.mlp_data[batch_indices]
        
        # Slice the GNN adjacency matrix for the current batch
        # The GNN input is now a (batch_size, num_train_samples) matrix
        batch_gnn = self.gnn_data[batch_indices, :]

        batch_y = self.y[batch_indices]

        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            self.raster_paths,
            self.buffer_pixels_x,
            self.buffer_pixels_y,
            self.patch_width,
            self.patch_height
        )

        # Return a tuple of inputs and the target, which Keras expects
        return (batch_cnn, batch_mlp, batch_gnn), batch_y


# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #
coords_train = train_combined[['Long','Lat']].values
coords_test = test_orig[['Long','Lat']].values
dist_mat_train = distance_matrix(coords_train, coords_train)
gnn_train = np.exp(-dist_mat_train/10)
dist_mat_test_train = distance_matrix(coords_test, coords_train)
gnn_test = np.exp(-dist_mat_test_train/10)

scaler = StandardScaler()
mlp_train = scaler.fit_transform(train_combined[numeric_cols])
mlp_test = scaler.transform(test_orig[numeric_cols])
y_train = train_combined['AsR'].values
y_test = test_orig['AsR'].values


# ==================== 5. Define Enhanced CNN–GNN–MLP Model ==================== #
def build_fusion_model(patch_shape, gnn_dim, mlp_dim):
    # CNN branch (for raster data)
    cnn_input = Input(shape=patch_shape, name="cnn_input")
    x = Conv2D(32, (3,3), activation="relu")(cnn_input)
    x = MaxPooling2D((2,2))(x)
    x = Conv2D(64, (3,3), activation="relu")(x)
    x = MaxPooling2D((2,2))(x)
    x = Flatten()(x)
    cnn_out = Dense(128, activation="relu", name="cnn_out")(x)

    # MLP branch (for numerical site features)
    mlp_input = Input(shape=(mlp_dim,), name="mlp_input")
    m = Dense(64, activation="relu")(mlp_input)
    mlp_out = Dense(32, activation="relu", name="mlp_out")(m)

    # GNN branch (for spatial connectivity)
    # The GNN input dimension is now the number of training samples
    gnn_input = Input(shape=(gnn_dim,), name="gnn_input")
    g = Dense(64, activation="relu")(gnn_input)
    gnn_out = Dense(32, activation="relu", name="gnn_out")(g)

    # Fusion Layer
    combined = Concatenate()([cnn_out, mlp_out, gnn_out])
    f = Dense(128, activation="relu")(combined)
    f = Dropout(0.4)(f)
    f = Dense(64, activation="relu")(f)
    output = Dense(1, activation="linear", name="final_output")(f)

    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss="mse")
    return model

# We need to determine the final GNN input dimension for the model
# It's the total number of training samples
batch_size = 4
gnn_input_dim = len(coords_train)
cnn_patch_shape = (2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), len(raster_paths))
model = build_fusion_model(cnn_patch_shape, gnn_input_dim, mlp_train.shape[1])
model.summary()

# ==================== 6. Create Data Generators ==================== #
# We create a separate generator for the validation data.
train_generator = DataGenerator(
    coords=coords_train,
    mlp_data=mlp_train,
    gnn_data=gnn_train,
    y=y_train,
    raster_paths=raster_paths,
    batch_size=batch_size,
    shuffle=True,
    buffer_meters=BUFFER_METERS
)

# For evaluation, we will create a generator for the full test set.
# The shuffle is set to False for consistent evaluation results.
def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=4, return_preds=False):
    num_samples = len(y_test)
    y_pred_list = []
    
    # Pre-calculate patch size from the first raster for evaluation
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(buffer_meters / res_x)
        buffer_pixels_y = int(buffer_meters / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    for i in range(0, num_samples, batch_size):
        batch_coords = coords_test[i:i+batch_size]
        batch_mlp = mlp_test[i:i+batch_size]
        
        # Get the corresponding slice of the test GNN input matrix
        batch_gnn = gnn_test_matrix[i:i+batch_size, :]
        batch_y = y_test[i:i+batch_size]

        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            raster_paths,
            buffer_pixels_x,
            buffer_pixels_y,
            patch_width,
            patch_height
        )
        
        # Make predictions on the batch and append to list
        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())
        
    y_pred = np.concatenate(y_pred_list)
    
    if return_preds:
        return y_pred
    else:
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        return r2, rmse


# ==================== 7. Train Model ==================== #
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

history = model.fit(
    train_generator,
    epochs=100,
    verbose=1,
    callbacks=[early_stopping],
    validation_data=train_generator # Using the same generator for validation for this example
)


# ==================== 8. Evaluate ==================== #
# Re-create a data generator without shuffling for evaluation on the training set
train_eval_generator = DataGenerator(
    coords=coords_train,
    mlp_data=mlp_train,
    gnn_data=gnn_train,
    y=y_train,
    raster_paths=raster_paths,
    batch_size=batch_size,
    shuffle=False,
    buffer_meters=BUFFER_METERS
)

y_pred_train = model.predict(train_eval_generator).flatten()
r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)
rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))

r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)


print(f"\n Enhanced CNN–GNN–MLP Model Performance (All Rasters):")
print(f"R² Train: {r2_train:.4f} | RMSE Train: {rmse_test:.4f}")
print(f"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}")


# ==================== 9. Feature Importance Analysis ==================== #

print("\n" + "="*50)
print("9. Feature Importance Analysis")
print("="*50)

# --- 9.1 Combined Feature Importance (by Model Branch) ---
# This method measures the importance of each model branch (CNN, MLP, GNN)
# by temporarily 'ablating' it and measuring the drop in model performance (R²).

# Calculate baseline performance on the test set
y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)
baseline_r2 = r2_score(y_test, y_pred_baseline)

print(f"\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}")

# Ablate CNN branch
cnn_test_ablated = np.zeros_like(extract_patch_for_generator(
    coords_test, raster_paths, int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0])
))
y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test)).flatten()
r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)
importance_cnn = baseline_r2 - r2_cnn_ablated

# Ablate MLP branch
mlp_test_ablated = np.zeros_like(mlp_test)
y_pred_mlp_ablated = model.predict((extract_patch_for_generator(
    coords_test, raster_paths, int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0])
), mlp_test_ablated, gnn_test)).flatten()
r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)
importance_mlp = baseline_r2 - r2_mlp_ablated

# Ablate GNN branch
gnn_test_ablated = np.zeros_like(gnn_test)
y_pred_gnn_ablated = model.predict((extract_patch_for_generator(
    coords_test, raster_paths, int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0])
), mlp_test, gnn_test_ablated)).flatten()
r2_gnn_ablated = r2_score(y_test, y_pred_gnn_ablated)
importance_gnn = baseline_r2 - r2_gnn_ablated

print("\n--- Combined Feature Importance (by Model Branch) ---")
print(f"CNN Branch Importance (R² drop): {importance_cnn:.4f}")
print(f"MLP Branch Importance (R² drop): {importance_mlp:.4f}")
print(f"GNN Branch Importance (R² drop): {importance_gnn:.4f}")


# --- 9.2 MLP Feature Importance (Permutation-based) ---
# This method measures the importance of each individual MLP feature
# by shuffling its values and measuring the drop in model performance.

mlp_feature_importance = {}
for i, feature_name in enumerate(numeric_cols):
    # Create a copy of the test data to avoid modifying the original
    mlp_test_shuffled = np.copy(mlp_test)
    # Shuffle the values of the current feature (column i)
    np.random.shuffle(mlp_test_shuffled[:, i])
    
    # Make predictions with the shuffled feature
    y_pred_shuffled = model.predict((extract_patch_for_generator(
        coords_test, raster_paths, int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0]), 2*int(BUFFER_METERS/rasterio.open(raster_paths[0]).res[0])
    ), mlp_test_shuffled, gnn_test)).flatten()
    r2_shuffled = r2_score(y_test, y_pred_shuffled)
    
    # Calculate the drop in performance
    importance = baseline_r2 - r2_shuffled
    mlp_feature_importance[feature_name] = importance

# Sort and print the results
print("\n--- MLP Feature Importance (Permutation-based) ---")
sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)
for feature, importance in sorted_importance:
    print(f"{feature:<20}: {importance:.4f}")
















import pandas as pd
import numpy as np
import glob
import os
import rasterio
from rasterio.windows import Window
from scipy.spatial import distance_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, Layer, MultiHeadAttention
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import Sequence
import tensorflow as tf
import gc # Import garbage collector

# List of buffer sizes to test
BUFFER_SIZES_TO_TEST = [1000, 2000, 3000, 5000]

# ==================== 1. Load Data ==================== #
orig = pd.read_csv("../data/RainySeason.csv")
river_100 = pd.read_csv("data/river_200_samples_rainy.csv")

drop_cols = ['Stations','River','Lat','Long','geometry']
numeric_cols = orig.drop(columns=drop_cols).columns.drop('AsR')

# Train-test split
train_orig = orig.sample(10, random_state=42)
test_orig = orig.drop(train_orig.index)
train_combined = pd.concat([river_100, train_orig], ignore_index=True)

# ==================== 2. Collect ALL Rasters ==================== #
raster_paths = []
raster_paths += glob.glob("CalIndices/*.tif")
raster_paths += glob.glob("LULCMerged/*.tif")
raster_paths += glob.glob("IDW/*.tif")

print(f"Using {len(raster_paths)} raster layers for CNN input.")
for r in raster_paths:
    print("  -", os.path.basename(r))

# ==================== 3. Create a Custom Data Generator ==================== #
def extract_patch_for_generator(coords, raster_files, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height):
    """
    Extracts a batch of patches from rasters for a given set of coordinates.
    This function is optimized to be called by the data generator for each batch.
    """
    patches = []
    # Loop through each coordinate pair in the batch
    for lon, lat in coords:
        channels = []
        # Loop through each raster file to get a single patch for each raster
        for rfile in raster_files:
            with rasterio.open(rfile) as src:
                try:
                    row, col = src.index(lon, lat)
                    win = Window(col - buffer_pixels_x, row - buffer_pixels_y, patch_width, patch_height)
                    arr = src.read(1, window=win, boundless=True, fill_value=0)
                    arr = arr.astype(np.float32)

                    if np.nanmax(arr) != 0:
                        arr /= np.nanmax(arr)
                except Exception as e:
                    print(f"Error processing {rfile} for coordinates ({lon}, {lat}): {e}")
                    arr = np.zeros((patch_width, patch_height), dtype=np.float32)
            channels.append(arr)
        patches.append(np.stack(channels, axis=-1))
    
    return np.array(patches)

class DataGenerator(Sequence):
    def __init__(self, coords, mlp_data, gnn_data, y, raster_paths, buffer_meters, batch_size=4, shuffle=True, **kwargs):
        super().__init__(**kwargs)
        self.coords = coords
        self.mlp_data = mlp_data
        self.gnn_data = gnn_data
        self.y = y
        self.raster_paths = raster_paths
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indices = np.arange(len(self.y))
        self.buffer_meters = buffer_meters

        # Pre-calculate patch size from the first raster
        with rasterio.open(raster_paths[0]) as src:
            res_x, res_y = src.res
            self.buffer_pixels_x = int(self.buffer_meters / res_x)
            self.buffer_pixels_y = int(self.buffer_meters / res_y)
            self.patch_width = 2 * self.buffer_pixels_x
            self.patch_height = 2 * self.buffer_pixels_y

        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.y) / self.batch_size))

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)
            
    def __getitem__(self, index):
        # Get batch indices
        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]

        # Get batch data
        batch_coords = self.coords[batch_indices]
        batch_mlp = self.mlp_data[batch_indices]
        
        # Slice the GNN adjacency matrix for the current batch
        batch_gnn = self.gnn_data[batch_indices, :]

        batch_y = self.y[batch_indices]

        # Extract CNN patches for the current batch
        batch_cnn = extract_patch_for_generator(
            batch_coords,
            self.raster_paths,
            self.buffer_pixels_x,
            self.buffer_pixels_y,
            self.patch_width,
            self.patch_height
        )

        # Return a tuple of inputs and the target, which Keras expects
        return (batch_cnn, batch_mlp, batch_gnn), batch_y


# ==================== 4. Prepare GNN & MLP Input (only once) ==================== #
coords_train = train_combined[['Long','Lat']].values
coords_test = test_orig[['Long','Lat']].values
dist_mat_train = distance_matrix(coords_train, coords_train)
gnn_train = np.exp(-dist_mat_train/10)
dist_mat_test_train = distance_matrix(coords_test, coords_train)
gnn_test = np.exp(-dist_mat_test_train/10)

scaler = StandardScaler()
mlp_train = scaler.fit_transform(train_combined[numeric_cols])
mlp_test = scaler.transform(test_orig[numeric_cols])
y_train = train_combined['AsR'].values
y_test = test_orig['AsR'].values


# ==================== 5. Define Custom GAT Layer and Fusion Model ==================== #
class GATLayer(Layer):
    """
    Custom GAT layer for graph attention.
    Takes node features and an adjacency matrix as input.
    """
    def __init__(self, output_dim, num_heads, **kwargs):
        super(GATLayer, self).__init__(**kwargs)
        self.output_dim = output_dim
        self.num_heads = num_heads
        self.attn_heads = [MultiHeadAttention(num_heads=num_heads, key_dim=output_dim // num_heads) for _ in range(num_heads)]
        self.linear_transform = Dense(output_dim)

    def call(self, inputs):
        x, adj = inputs
        # GAT logic: attention is applied over the neighbors defined by the adjacency matrix
        # Here we simplify and use the MultiHeadAttention layer with the adjacency matrix as a mask
        # The MLP features are used as query, key, and value
        attention_output = self.linear_transform(x)
        attention_output = tf.expand_dims(attention_output, axis=1) # Shape: (batch, 1, output_dim)
        attention_output = tf.tile(attention_output, [1, adj.shape[1], 1]) # Tile to match the number of neighbors
        
        # Reshape adj for the mask
        adj_mask = tf.cast(adj > 0, dtype=tf.bool)
        adj_mask = tf.expand_dims(adj_mask, axis=1)
        adj_mask = tf.tile(adj_mask, [1, adj.shape[1], 1])
        
        # A simpler approach: use the MLP features as key, query, and value
        # and the adjacency matrix as a mask to enforce neighborhood constraints.
        # However, a proper GAT aggregates features. Let's simplify this with a Dense layer
        # followed by an attention mechanism over the neighborhood.
        # This implementation uses MLP features as value and the adjacency matrix for neighbor weighting.

        # GAT logic: Attention over neighbors
        query = Dense(self.output_dim, activation="relu")(x)
        key = Dense(self.output_dim, activation="relu")(x)
        value = Dense(self.output_dim, activation="relu")(x)
        
        # Reshape for multi-head attention
        query = tf.expand_dims(query, axis=1)
        key = tf.expand_dims(key, axis=1)
        value = tf.expand_dims(value, axis=1)

        # The adjacency matrix (gnn_input) will act as a mask
        # MultiHeadAttention in Keras expects a 2D mask (batch_size, seq_len)
        # We will use the `attention_mask` to implement the graph structure.
        attention_output = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.output_dim // self.num_heads)(
            query=query,
            value=value,
            key=key,
            attention_mask=adj
        )
        attention_output = tf.squeeze(attention_output, axis=1)
        return attention_output
        

def build_fusion_model(patch_shape, gnn_adj_dim, mlp_dim):
    # CNN branch (for raster data)
    cnn_input = Input(shape=patch_shape, name="cnn_input")
    x = Conv2D(32, (3,3), activation="relu")(cnn_input)
    x = MaxPooling2D((2,2))(x)
    x = Conv2D(64, (3,3), activation="relu")(x)
    x = MaxPooling2D((2,2))(x)
    x = Flatten()(x)
    cnn_out = Dense(128, activation="relu", name="cnn_out")(x)

    # MLP branch (for numerical site features)
    mlp_input = Input(shape=(mlp_dim,), name="mlp_input")
    m = Dense(64, activation="relu")(mlp_input)
    mlp_out = Dense(32, activation="relu", name="mlp_out")(m)

    # GAT branch (for spatial connectivity)
    # The GAT takes the MLP features and the adjacency matrix
    gnn_input = Input(shape=(gnn_adj_dim,), name="gnn_input")
    g_out = GATLayer(output_dim=32, num_heads=4, name="gat_out")([mlp_input, gnn_input])

    # Concatenate the outputs of the three branches
    combined_features = Concatenate()([cnn_out, mlp_out, g_out])
    
    # Attention Fusion Layer as requested by the user
    # We'll use MultiHeadAttention to weigh the contributions of each branch
    # The concatenated features are reshaped and fed into the attention layer
    fusion_input = tf.expand_dims(combined_features, axis=1) # (batch, 1, features)
    fusion_output = MultiHeadAttention(num_heads=4, key_dim=32)(
        query=fusion_input,
        value=fusion_input,
        key=fusion_input
    )
    fusion_output = tf.squeeze(fusion_output, axis=1)
    
    f = Dense(128, activation="relu")(fusion_output)
    f = Dropout(0.4)(f)
    f = Dense(64, activation="relu")(f)
    output = Dense(1, activation="linear", name="final_output")(f)

    model = Model(inputs=[cnn_input, mlp_input, gnn_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss="mse")
    return model


def evaluate_model(model, coords_test, mlp_test, gnn_test_matrix, y_test, raster_paths, buffer_meters, batch_size=4, return_preds=False):
    num_samples = len(y_test)
    y_pred_list = []
    
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(buffer_meters / res_x)
        buffer_pixels_y = int(buffer_meters / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    for i in range(0, num_samples, batch_size):
        batch_coords = coords_test[i:i+batch_size]
        batch_mlp = mlp_test[i:i+batch_size]
        
        batch_gnn = gnn_test_matrix[i:i+batch_size, :]
        batch_y = y_test[i:i+batch_size]

        batch_cnn = extract_patch_for_generator(
            batch_coords,
            raster_paths,
            buffer_pixels_x,
            buffer_pixels_y,
            patch_width,
            patch_height
        )
        
        y_pred_list.append(model.predict((batch_cnn, batch_mlp, batch_gnn)).flatten())
        
    y_pred = np.concatenate(y_pred_list)
    
    if return_preds:
        return y_pred
    else:
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        return r2, rmse


# ==================== Loop through buffer sizes for analysis ==================== #
for BUFFER_METERS in BUFFER_SIZES_TO_TEST:
    print("\n" + "="*80)
    print(f"Analyzing for BUFFER_METERS = {BUFFER_METERS}m")
    print("="*80)

    # We need to determine the final GNN input dimension for the model
    # It's the total number of training samples
    batch_size = 4
    gnn_adj_dim = len(coords_train)
    
    # Calculate CNN patch shape based on the current buffer size
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(BUFFER_METERS / res_x)
        patch_width = 2 * buffer_pixels_x
        cnn_patch_shape = (patch_width, patch_width, len(raster_paths))

    model = build_fusion_model(cnn_patch_shape, gnn_adj_dim, mlp_train.shape[1])
    model.summary()

    # ==================== 6. Create Data Generators ==================== #
    train_generator = DataGenerator(
        coords=coords_train,
        mlp_data=mlp_train,
        gnn_data=gnn_train,
        y=y_train,
        raster_paths=raster_paths,
        buffer_meters=BUFFER_METERS,
        batch_size=batch_size,
        shuffle=True
    )

    # ==================== 7. Train Model ==================== #
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )

    history = model.fit(
        train_generator,
        epochs=100,
        verbose=1,
        callbacks=[early_stopping],
        validation_data=train_generator
    )

    # ==================== 8. Evaluate ==================== #
    y_pred_train = model.predict(train_generator).flatten()
    r2_train = r2_score(y_train[:len(y_pred_train)], y_pred_train)
    rmse_train = np.sqrt(mean_squared_error(y_train[:len(y_pred_train)], y_pred_train))
    
    r2_test, rmse_test = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size)

    print(f"\n Enhanced GAT-CNN-MLP Model Performance ({BUFFER_METERS}m):")
    print(f"R² Train: {r2_train:.4f} | RMSE Train: {rmse_test:.4f}")
    print(f"R² Test: {r2_test:.4f} | RMSE Test: {rmse_test:.4f}")

    # ==================== 9. Feature Importance Analysis ==================== #
    print("\n" + "-"*50)
    print(f"Feature Importance Analysis for {BUFFER_METERS}m")
    print("-"*50)

    # --- 9.1 Combined Feature Importance (by Model Branch) ---
    y_pred_baseline = evaluate_model(model, coords_test, mlp_test, gnn_test, y_test, raster_paths, buffer_meters=BUFFER_METERS, batch_size=batch_size, return_preds=True)
    baseline_r2 = r2_score(y_test, y_pred_baseline)

    print(f"\nBaseline Performance on Test Set: R² = {baseline_r2:.4f}")

    # Ablate CNN branch
    with rasterio.open(raster_paths[0]) as src:
        res_x, res_y = src.res
        buffer_pixels_x = int(BUFFER_METERS / res_x)
        buffer_pixels_y = int(BUFFER_METERS / res_y)
        patch_width = 2 * buffer_pixels_x
        patch_height = 2 * buffer_pixels_y

    cnn_test_ablated = np.zeros_like(extract_patch_for_generator(
        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
    ))
    y_pred_cnn_ablated = model.predict((cnn_test_ablated, mlp_test, gnn_test)).flatten()
    r2_cnn_ablated = r2_score(y_test, y_pred_cnn_ablated)
    importance_cnn = baseline_r2 - r2_cnn_ablated

    # Ablate MLP branch
    mlp_test_ablated = np.zeros_like(mlp_test)
    y_pred_mlp_ablated = model.predict((extract_patch_for_generator(
        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
    ), mlp_test_ablated, gnn_test)).flatten()
    r2_mlp_ablated = r2_score(y_test, y_pred_mlp_ablated)
    importance_mlp = baseline_r2 - r2_mlp_ablated

    # Ablate GAT branch (GAT requires MLP features, so we can't just zero out the GAT input)
    # Instead, we zero out the MLP features which are the node features for GAT, effectively ablating it
    y_pred_gat_ablated = model.predict((extract_patch_for_generator(
        coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
    ), mlp_test_ablated, gnn_test)).flatten()
    r2_gat_ablated = r2_score(y_test, y_pred_gat_ablated)
    importance_gat = baseline_r2 - r2_gat_ablated
    # NOTE: The ablation of the MLP branch and GAT branch is effectively the same in this architecture since GAT relies on the MLP features.

    print("\n--- Combined Feature Importance (by Model Branch) ---")
    print(f"CNN Branch Importance (R² drop): {importance_cnn:.4f}")
    print(f"MLP Branch Importance (R² drop): {importance_mlp:.4f}")
    print(f"GAT Branch Importance (R² drop): {importance_gat:.4f}")

    # --- 9.2 MLP Feature Importance (Permutation-based) ---
    mlp_feature_importance = {}
    for i, feature_name in enumerate(numeric_cols):
        mlp_test_shuffled = np.copy(mlp_test)
        np.random.shuffle(mlp_test_shuffled[:, i])
        
        y_pred_shuffled = model.predict((extract_patch_for_generator(
            coords_test, raster_paths, buffer_pixels_x, buffer_pixels_y, patch_width, patch_height
        ), mlp_test_shuffled, gnn_test)).flatten()
        r2_shuffled = r2_score(y_test, y_pred_shuffled)
        
        importance = baseline_r2 - r2_shuffled
        mlp_feature_importance[feature_name] = importance

    print("\n--- MLP Feature Importance (Permutation-based) ---")
    sorted_importance = sorted(mlp_feature_importance.items(), key=lambda item: item[1], reverse=True)
    for feature, importance in sorted_importance:
        print(f"{feature:<20}: {importance:.4f}")
    
    # Garbage collect to free up memory before the next loop iteration
    del model, history, train_generator
    gc.collect()





