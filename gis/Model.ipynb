{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf0bd4d-7c63-4a57-9053-a2e12feb9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import rasterio.windows\n",
    "import rasterio.features \n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from scipy.spatial.distance import cdist # For calculating distances between points\n",
    "import networkx as nx \n",
    "\n",
    "# --- Configuration ---\n",
    "# Base directory for all your GIS data files\n",
    "GIS_BASE_DIR = \"/Users/rakibhhridoy/Five_Rivers/gis\" # <--- IMPORTANT: Update this path to your GIS data directory\n",
    "\n",
    "# Subdirectories for LULC and CalIndices TIFF files\n",
    "LULC_DIR = os.path.join(GIS_BASE_DIR, \"LULCMerged\")\n",
    "CAL_INDICES_DIR = os.path.join(GIS_BASE_DIR, \"CalIndices\")\n",
    "\n",
    "# Paths to your point shapefiles\n",
    "SAMPLING_POINTS_PATH = os.path.join(GIS_BASE_DIR, \"sampling_point.shp\")\n",
    "BRICK_FIELD_POINTS_PATH = os.path.join(GIS_BASE_DIR, \"brick_field_point.shp\")\n",
    "INDUSTRY_POINTS_PATH = os.path.join(GIS_BASE_DIR, \"industry_point.shp\")\n",
    "\n",
    "# Output CSV file paths for intermediate and final processed data\n",
    "OUTPUT_LULC_VARIATIONS_CSV = os.path.join(GIS_BASE_DIR, \"LULC_5km_Variations.csv\")\n",
    "OUTPUT_HYDRO_PROPERTIES_CSV = os.path.join(GIS_BASE_DIR, \"Hydrological_Properties.csv\")\n",
    "OUTPUT_RASTER_FEATURES_CSV = os.path.join(GIS_BASE_DIR, \"Raster_Derived_Features.csv\")\n",
    "OUTPUT_COMBINED_FEATURES_CSV = os.path.join(GIS_BASE_DIR, \"Combined_Features_Scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6caba50a-8f61-43aa-bc03-9ef7bd03093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LULC_YEARS = list(range(2017, 2023)) # Covers 2017, 2018, 2019, 2020, 2021, 2022\n",
    "UNIFORM_BUFFER_RADIUS_METERS = 5000 # 5 km\n",
    "\n",
    "# CNN Patch Size (pixels) - for extracting image patches around points for CNN branch\n",
    "# This is a pixel dimension, not a real-world radius.\n",
    "CNN_PATCH_SIZE = 32 # e.g., 32x32 pixels\n",
    "\n",
    "# GNN Edge Definition: Distance threshold for connecting nodes (sampling stations)\n",
    "# This creates a proximity graph as an approximation of hydrological connectivity.\n",
    "GNN_EDGE_DISTANCE_THRESHOLD_METERS = 5000 # 5 km for connecting stations\n",
    "\n",
    "# List of raster indices to extract neighborhood statistics from\n",
    "RASTER_INDICES_TO_EXTRACT = [\n",
    "    \"awei.tif\", \"bui.tif\", \"evi.tif\", \"mndwi.tif\", \"ndbi.tif\",\n",
    "    \"ndbsi.tif\", \"ndsi.tif\", \"ndvi.tif\", \"ndwi.tif\", \"savi.tif\", \"ui.tif\"\n",
    "]\n",
    "# Path to DEM file\n",
    "DEM_PATH = os.path.join(GIS_BASE_DIR, \"DEMF.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30da99db-19b4-4f9d-90b8-0c35e77d0b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "# These functions are designed to encapsulate common geospatial operations,\n",
    "# making the main pipeline cleaner and more modular.\n",
    "\n",
    "def load_raster(filepath, reference_crs):\n",
    "    \"\"\"\n",
    "    Loads a single raster file. If its CRS does not match the reference_crs,\n",
    "    it reprojects the raster to the reference_crs.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Raster file not found: {filepath}\")\n",
    "    \n",
    "    with rasterio.open(filepath) as src:\n",
    "        if src.crs != reference_crs:\n",
    "            print(f\"Reprojecting {os.path.basename(filepath)} from {src.crs} to {reference_crs}...\")\n",
    "            from rasterio.warp import reproject, Resampling\n",
    "            \n",
    "            # Calculate new transform and dimensions for reprojection\n",
    "            transform, width, height = rasterio.windows.calculate_default_transform(\n",
    "                src.crs, reference_crs, src.width, src.height, *src.bounds\n",
    "            )\n",
    "            reprojected_array = np.empty((height, width), dtype=src.dtypes)\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=reprojected_array,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=reference_crs,\n",
    "                resampling=Resampling.nearest # Use nearest for categorical LULC, bilinear for continuous DEM/indices\n",
    "            )\n",
    "            profile = src.profile.copy()\n",
    "            profile.update({\n",
    "                'crs': reference_crs,\n",
    "                'transform': transform,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "            return reprojected_array, profile\n",
    "        else:\n",
    "            return src.read(1), src.profile\n",
    "\n",
    "def extract_neighborhood_stats(raster_array, raster_profile, point_geom, buffer_meters, stat_type='mean'):\n",
    "    \"\"\"\n",
    "    Extracts neighborhood statistics (mean, stddev) from a raster for a given point buffer.\n",
    "    This function crops the raster to the buffer's bounding box and then masks it with the circular buffer.\n",
    "    \"\"\"\n",
    "    buffer_geom = point_geom.buffer(buffer_meters)\n",
    "    \n",
    "    # Get the window (bounding box in pixel coordinates) for the buffer\n",
    "    window = rasterio.windows.from_bounds(*buffer_geom.bounds, transform=raster_profile['transform'])\n",
    "    \n",
    "    # Clamp window to raster dimensions to prevent out-of-bounds access\n",
    "    row_start, row_stop = int(window.row_off), int(window.row_off + window.height)\n",
    "    col_start, col_stop = int(window.col_off), int(window.col_off + window.width)\n",
    "    \n",
    "    row_start = max(0, row_start)\n",
    "    row_stop = min(raster_array.shape[0], row_stop)\n",
    "    col_start = max(0, col_start)\n",
    "    col_stop = min(raster_array.shape[1], col_stop)\n",
    "\n",
    "    if row_stop <= row_start or col_stop <= col_start:\n",
    "        return np.nan # No valid overlap\n",
    "    \n",
    "    # Extract the subset of the raster array (rectangular crop)\n",
    "    cropped_array = raster_array[row_start:row_stop, col_start:col_stop]\n",
    "    # Create a new transform for this cropped array's extent\n",
    "    cropped_transform = rasterio.windows.transform(window, raster_profile['transform'])\n",
    "\n",
    "    # Rasterize the buffer geometry onto a new array of the same shape as cropped_array\n",
    "    pixel_mask_for_buffer = rasterio.features.rasterize(\n",
    "        [buffer_geom],\n",
    "        out_shape=cropped_array.shape,\n",
    "        transform=cropped_transform,\n",
    "        fill=0,\n",
    "        all_touched=False, # Only pixels strictly inside\n",
    "        dtype=np.uint8\n",
    "    ).astype(bool) # Convert to boolean mask (True for inside, False for outside)\n",
    "\n",
    "    # Apply the pixel mask to the cropped array to get pixels within the circular buffer\n",
    "    pixels_within_buffer = cropped_array[pixel_mask_for_buffer]\n",
    "    \n",
    "    if pixels_within_buffer.size > 0:\n",
    "        if stat_type == 'mean':\n",
    "            return np.mean(pixels_within_buffer)\n",
    "        elif stat_type == 'std':\n",
    "            return np.std(pixels_within_buffer)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported stat_type. Use 'mean' or 'std'.\")\n",
    "    else:\n",
    "        return np.nan # No valid pixels in buffer\n",
    "\n",
    "def extract_raster_patch(raster_array, raster_profile, point_geom, patch_size):\n",
    "    \"\"\"\n",
    "    Extracts a square image patch around a point for CNN input.\n",
    "    Handles boundary conditions by padding the raster.\n",
    "    \"\"\"\n",
    "    # Convert point_geom to pixel coordinates\n",
    "    col, row = raster_profile['transform'](point_geom.x, point_geom.y, op=~raster_profile['transform'])\n",
    "    col, row = int(col), int(row)\n",
    "\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    # Create a padded array to handle points near edges gracefully\n",
    "    padded_array = np.pad(raster_array, half_patch, mode='reflect') \n",
    "    \n",
    "    # Adjust coordinates for the padded array\n",
    "    padded_row_start = row + half_patch - half_patch\n",
    "    padded_row_end = row + half_patch + half_patch\n",
    "    padded_col_start = col + half_patch - half_patch\n",
    "    padded_col_end = col + half_patch + half_patch\n",
    "\n",
    "    # Extract patch from padded array\n",
    "    patch = padded_array[padded_row_start:padded_row_end, padded_col_start:padded_col_end]\n",
    "    \n",
    "    # Ensure the patch is exactly patch_size x patch_size\n",
    "    if patch.shape != (patch_size, patch_size):\n",
    "        print(f\"Warning: Patch for point at ({point_geom.x}, {point_geom.y}) is not {patch_size}x{patch_size}. Returning NaN array.\")\n",
    "        return np.full((patch_size, patch_size), np.nan) # Return NaN array if extraction fails\n",
    "    \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd31ce4-7464-438b-89df-23786ed297df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stations</th>\n",
       "      <th>River</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>geometry</th>\n",
       "      <th>hydrological_dist_to_nearest_BF</th>\n",
       "      <th>num_upstream_BF</th>\n",
       "      <th>hydrological_dist_to_nearest_IND</th>\n",
       "      <th>num_upstream_IND</th>\n",
       "      <th>CrW</th>\n",
       "      <th>...</th>\n",
       "      <th>MW</th>\n",
       "      <th>SandW</th>\n",
       "      <th>SiltW</th>\n",
       "      <th>ClayW</th>\n",
       "      <th>FeW</th>\n",
       "      <th>variation_17_18</th>\n",
       "      <th>variation_18_19</th>\n",
       "      <th>variation_19_20</th>\n",
       "      <th>variation_20_21</th>\n",
       "      <th>variation_21_22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.910260</td>\n",
       "      <td>90.229845</td>\n",
       "      <td>POINT (10044340.399756001 2742476.70627368)</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>2</td>\n",
       "      <td>106.58</td>\n",
       "      <td>...</td>\n",
       "      <td>30.69</td>\n",
       "      <td>16</td>\n",
       "      <td>51</td>\n",
       "      <td>28</td>\n",
       "      <td>29400</td>\n",
       "      <td>14.396884</td>\n",
       "      <td>8.830319</td>\n",
       "      <td>11.964628</td>\n",
       "      <td>10.162627</td>\n",
       "      <td>10.226519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2</td>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.858227</td>\n",
       "      <td>90.240038</td>\n",
       "      <td>POINT (10045475.079325657 2736141.9436627394)</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>104.28</td>\n",
       "      <td>...</td>\n",
       "      <td>32.41</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>32100</td>\n",
       "      <td>12.920722</td>\n",
       "      <td>9.676226</td>\n",
       "      <td>13.489252</td>\n",
       "      <td>10.422661</td>\n",
       "      <td>11.249439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3</td>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.802571</td>\n",
       "      <td>90.245390</td>\n",
       "      <td>POINT (10046070.861240383 2729368.914036628)</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>6</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>4</td>\n",
       "      <td>89.77</td>\n",
       "      <td>...</td>\n",
       "      <td>30.14</td>\n",
       "      <td>57</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>27970</td>\n",
       "      <td>11.966286</td>\n",
       "      <td>10.483110</td>\n",
       "      <td>15.459976</td>\n",
       "      <td>10.687158</td>\n",
       "      <td>11.446218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S4</td>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.754298</td>\n",
       "      <td>90.246581</td>\n",
       "      <td>POINT (10046203.442753918 2723496.704977303)</td>\n",
       "      <td>0.073026</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027234</td>\n",
       "      <td>6</td>\n",
       "      <td>71.55</td>\n",
       "      <td>...</td>\n",
       "      <td>29.58</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>25780</td>\n",
       "      <td>21.385412</td>\n",
       "      <td>15.447605</td>\n",
       "      <td>20.766508</td>\n",
       "      <td>13.685776</td>\n",
       "      <td>13.207538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5</td>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.702157</td>\n",
       "      <td>90.277077</td>\n",
       "      <td>POINT (10049598.24194515 2717156.4153029486)</td>\n",
       "      <td>0.074113</td>\n",
       "      <td>15</td>\n",
       "      <td>0.122163</td>\n",
       "      <td>5</td>\n",
       "      <td>100.15</td>\n",
       "      <td>...</td>\n",
       "      <td>33.78</td>\n",
       "      <td>75</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>36480</td>\n",
       "      <td>18.108141</td>\n",
       "      <td>14.249969</td>\n",
       "      <td>17.320514</td>\n",
       "      <td>12.567463</td>\n",
       "      <td>11.587011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Stations        River        Lat       Long  \\\n",
       "0       S1  Dhaleshwari  23.910260  90.229845   \n",
       "1       S2  Dhaleshwari  23.858227  90.240038   \n",
       "2       S3  Dhaleshwari  23.802571  90.245390   \n",
       "3       S4  Dhaleshwari  23.754298  90.246581   \n",
       "4       S5  Dhaleshwari  23.702157  90.277077   \n",
       "\n",
       "                                        geometry  \\\n",
       "0    POINT (10044340.399756001 2742476.70627368)   \n",
       "1  POINT (10045475.079325657 2736141.9436627394)   \n",
       "2   POINT (10046070.861240383 2729368.914036628)   \n",
       "3   POINT (10046203.442753918 2723496.704977303)   \n",
       "4   POINT (10049598.24194515 2717156.4153029486)   \n",
       "\n",
       "   hydrological_dist_to_nearest_BF  num_upstream_BF  \\\n",
       "0                         0.000200                6   \n",
       "1                         0.000020                1   \n",
       "2                         0.000200                6   \n",
       "3                         0.073026                2   \n",
       "4                         0.074113               15   \n",
       "\n",
       "   hydrological_dist_to_nearest_IND  num_upstream_IND     CrW  ...     MW  \\\n",
       "0                          0.006790                 2  106.58  ...  30.69   \n",
       "1                          0.000000                 0  104.28  ...  32.41   \n",
       "2                          0.002100                 4   89.77  ...  30.14   \n",
       "3                          0.027234                 6   71.55  ...  29.58   \n",
       "4                          0.122163                 5  100.15  ...  33.78   \n",
       "\n",
       "   SandW  SiltW  ClayW    FeW  variation_17_18  variation_18_19  \\\n",
       "0     16     51     28  29400        14.396884         8.830319   \n",
       "1     16     62     27  32100        12.920722         9.676226   \n",
       "2     57     30     17  27970        11.966286        10.483110   \n",
       "3     71     18     13  25780        21.385412        15.447605   \n",
       "4     75     19      9  36480        18.108141        14.249969   \n",
       "\n",
       "   variation_19_20  variation_20_21  variation_21_22  \n",
       "0        11.964628        10.162627        10.226519  \n",
       "1        13.489252        10.422661        11.249439  \n",
       "2        15.459976        10.687158        11.446218  \n",
       "3        20.766508        13.685776        13.207538  \n",
       "4        17.320514        12.567463        11.587011  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainy_df = pd.read_csv(\"data/Hydro_LULC_Winter.csv\")\n",
    "rainy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b460178-d621-46a2-8712-f276bd14b63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Stations', 'River', 'Lat', 'Long', 'geometry',\n",
       "       'hydrological_dist_to_nearest_BF', 'num_upstream_BF',\n",
       "       'hydrological_dist_to_nearest_IND', 'num_upstream_IND', 'CrW', 'NiW',\n",
       "       'CuW', 'AsW', 'CdW', 'PbW', 'MW', 'SandW', 'SiltW', 'ClayW', 'FeW',\n",
       "       'variation_17_18', 'variation_18_19', 'variation_19_20',\n",
       "       'variation_20_21', 'variation_21_22'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "819a861c-fffc-4e02-b732-dc8bea5d23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_col = ['Stations', 'River', 'Lat', 'Long', 'CrW', 'NiW',\n",
    "       'CuW', 'AsW', 'CdW', 'PbW', 'FeW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b822b1-611c-47d8-83e0-7a0b4a0ac055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>River</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>CrW</th>\n",
       "      <th>NiW</th>\n",
       "      <th>CuW</th>\n",
       "      <th>AsW</th>\n",
       "      <th>CdW</th>\n",
       "      <th>PbW</th>\n",
       "      <th>FeW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stations</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S1</th>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.910260</td>\n",
       "      <td>90.229845</td>\n",
       "      <td>106.58</td>\n",
       "      <td>34.35</td>\n",
       "      <td>69.95</td>\n",
       "      <td>9.12</td>\n",
       "      <td>4.43</td>\n",
       "      <td>105.90</td>\n",
       "      <td>29400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.858227</td>\n",
       "      <td>90.240038</td>\n",
       "      <td>104.28</td>\n",
       "      <td>27.10</td>\n",
       "      <td>75.12</td>\n",
       "      <td>13.79</td>\n",
       "      <td>3.82</td>\n",
       "      <td>96.14</td>\n",
       "      <td>32100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S3</th>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.802571</td>\n",
       "      <td>90.245390</td>\n",
       "      <td>89.77</td>\n",
       "      <td>59.33</td>\n",
       "      <td>71.13</td>\n",
       "      <td>26.17</td>\n",
       "      <td>2.95</td>\n",
       "      <td>77.36</td>\n",
       "      <td>27970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S4</th>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.754298</td>\n",
       "      <td>90.246581</td>\n",
       "      <td>71.55</td>\n",
       "      <td>49.17</td>\n",
       "      <td>92.34</td>\n",
       "      <td>25.35</td>\n",
       "      <td>2.36</td>\n",
       "      <td>90.48</td>\n",
       "      <td>25780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S5</th>\n",
       "      <td>Dhaleshwari</td>\n",
       "      <td>23.702157</td>\n",
       "      <td>90.277077</td>\n",
       "      <td>100.15</td>\n",
       "      <td>50.68</td>\n",
       "      <td>100.22</td>\n",
       "      <td>28.37</td>\n",
       "      <td>2.10</td>\n",
       "      <td>79.10</td>\n",
       "      <td>36480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                River        Lat       Long     CrW    NiW     CuW    AsW  \\\n",
       "Stations                                                                    \n",
       "S1        Dhaleshwari  23.910260  90.229845  106.58  34.35   69.95   9.12   \n",
       "S2        Dhaleshwari  23.858227  90.240038  104.28  27.10   75.12  13.79   \n",
       "S3        Dhaleshwari  23.802571  90.245390   89.77  59.33   71.13  26.17   \n",
       "S4        Dhaleshwari  23.754298  90.246581   71.55  49.17   92.34  25.35   \n",
       "S5        Dhaleshwari  23.702157  90.277077  100.15  50.68  100.22  28.37   \n",
       "\n",
       "           CdW     PbW    FeW  \n",
       "Stations                       \n",
       "S1        4.43  105.90  29400  \n",
       "S2        3.82   96.14  32100  \n",
       "S3        2.95   77.36  27970  \n",
       "S4        2.36   90.48  25780  \n",
       "S5        2.10   79.10  36480  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heavy_metal_df = rainy_df[hm_col]\n",
    "heavy_metal_df.set_index('Stations', inplace=True)\n",
    "heavy_metal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8bd1ca-b04f-4f8b-bf6e-6c16274b6f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.2: Loading LULC Rasters and establishing Reference CRS/Transform ---\n",
      "Loaded LULC rasters for years: [2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Reference CRS (from LULC): EPSG:32646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 1.2: Loading LULC Rasters and establishing Reference CRS/Transform ---\")\n",
    "lulc_rasters = {}\n",
    "lulc_profiles = {}\n",
    "reference_crs = None\n",
    "reference_transform = None\n",
    "full_raster_height = None\n",
    "full_raster_width = None\n",
    "\n",
    "try:\n",
    "    # Use the first LULC file to get the reference CRS and transform\n",
    "    first_lulc_file = os.path.join(LULC_DIR, \"LULC2017.tif\")\n",
    "    if not os.path.exists(first_lulc_file):\n",
    "        raise FileNotFoundError(f\"Reference LULC file not found: {first_lulc_file}\")\n",
    "\n",
    "    with rasterio.open(first_lulc_file) as src:\n",
    "        reference_crs = src.crs\n",
    "        reference_transform = src.transform\n",
    "        full_raster_height, full_raster_width = src.shape\n",
    "\n",
    "    for year in LULC_YEARS:\n",
    "        filepath = os.path.join(LULC_DIR, f\"LULC{year}.tif\")\n",
    "        # Load and reproject if necessary using the helper function\n",
    "        lulc_rasters[year], lulc_profiles[year] = load_raster(filepath, reference_crs)\n",
    "        \n",
    "    if not lulc_rasters:\n",
    "        raise ValueError(\"No LULC rasters were loaded. Check LULC_DIR and file names.\")\n",
    "        \n",
    "    print(f\"Loaded LULC rasters for years: {list(lulc_rasters.keys())}\")\n",
    "    print(f\"Reference CRS (from LULC): {reference_crs}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LULC rasters or establishing reference: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e822c4d2-5756-48dd-8eb6-5a9da6dcfd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojected sampling points from PROJCS[\"WGS 84 / UTM zone 46N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",93],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32646\"]] to EPSG:32646.\n",
      "Loaded 17 sampling points.\n"
     ]
    }
   ],
   "source": [
    "# Step 1.3: Load Sampling Points (reprojected to reference_crs)\n",
    "try:\n",
    "    sampling_points_gdf = gpd.read_file(SAMPLING_POINTS_PATH)\n",
    "    if 'Stations' not in sampling_points_gdf.columns:\n",
    "        sampling_points_gdf['Stations'] = [f\"S{i+1}\" for i in range(len(sampling_points_gdf))]\n",
    "    sampling_points_gdf.set_index('Stations', inplace=True)\n",
    "\n",
    "    if sampling_points_gdf.crs != reference_crs:\n",
    "        sampling_points_gdf = sampling_points_gdf.to_crs(reference_crs)\n",
    "        print(f\"Reprojected sampling points from {sampling_points_gdf.crs} to {reference_crs}.\")\n",
    "    print(f\"Loaded {len(sampling_points_gdf)} sampling points.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or reprojecting sampling points: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce1d75c-88da-486c-b062-29ec541d44c2",
   "metadata": {},
   "source": [
    "## HYDRO & LULC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c6b6df-a8f9-46f8-90ae-f94d58f42692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Calculating Hydrological Properties and LULC Variations ---\n",
      "Loaded 270 brick field points and 195 industry points.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Calculate Hydrological and LULC Features ---\n",
    "print(\"\\n--- Step 2: Calculating Hydrological Properties and LULC Variations ---\")\n",
    "\n",
    "# Load Brick Field and Industry Points (reprojected to reference_crs)\n",
    "brick_field_points_gdf = None\n",
    "industry_points_gdf = None\n",
    "try:\n",
    "    brick_field_points_gdf = gpd.read_file(BRICK_FIELD_POINTS_PATH)\n",
    "    industry_points_gdf = gpd.read_file(INDUSTRY_POINTS_PATH)\n",
    "\n",
    "    if brick_field_points_gdf.crs != reference_crs:\n",
    "        brick_field_points_gdf = brick_field_points_gdf.to_crs(reference_crs)\n",
    "    if industry_points_gdf.crs != reference_crs:\n",
    "        industry_points_gdf = industry_points_gdf.to_crs(reference_crs)\n",
    "    print(f\"Loaded {len(brick_field_points_gdf)} brick field points and {len(industry_points_gdf)} industry points.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or reprojecting brick field/industry points: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eda6b6a-b227-41f3-a749-fd33e47865ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DEM raster with shape: (6266, 5764)\n"
     ]
    }
   ],
   "source": [
    "# Load DEM Raster\n",
    "dem_array = None\n",
    "dem_profile = None\n",
    "try:\n",
    "    dem_array, dem_profile = load_raster(DEM_PATH, reference_crs)\n",
    "    print(f\"Loaded DEM raster with shape: {dem_array.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DEM raster: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Generate Binary Change Maps (Year-to-Year)\n",
    "change_maps = {}\n",
    "for i in range(len(LULC_YEARS) - 1):\n",
    "    year1 = LULC_YEARS[i]\n",
    "    year2 = LULC_YEARS[i+1]\n",
    "    \n",
    "    lulc_t1 = lulc_rasters[year1]\n",
    "    lulc_t2 = lulc_rasters[year2]\n",
    "    \n",
    "    if lulc_t1.shape != lulc_t2.shape:\n",
    "        print(f\"Warning: LULC raster shapes mismatch for {year1} and {year2}. Skipping change map for this interval.\")\n",
    "        continue\n",
    "    change_map_array = (lulc_t1 != lulc_t2).astype(np.uint8)\n",
    "    change_maps[f'{year1}-{year2}'] = change_map_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25052e9c-1130-4d8c-843a-42c29a464fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LULC Variations calculated and saved to LULC_5km_Variations.csv\n",
      "Hydrological Properties calculated and saved to Hydrological_Properties.csv\n",
      "\n",
      "First 5 rows of Hydrological Properties:\n",
      "          dem_point_value  dem_mean_5km  dem_std_5km  dist_to_nearest_BF  \\\n",
      "Stations                                                                   \n",
      "S1                  -45.0    -44.545135     3.966224         2647.402248   \n",
      "S2                  -51.0    -45.338905     3.547604         3113.557901   \n",
      "S3                  -52.0    -46.650517     2.648768         1841.694727   \n",
      "S4                  -53.0    -47.438057     2.525883         2737.503779   \n",
      "S5                  -47.0    -48.027908     2.480211         1637.673006   \n",
      "\n",
      "          dist_to_nearest_IND  num_within_5km_BF  num_within_5km_IND  \n",
      "Stations                                                              \n",
      "S1                 522.326002                  3                  10  \n",
      "S2                3000.625941                  2                   2  \n",
      "S3                1298.585986                  9                   7  \n",
      "S4                 317.052883                  5                   4  \n",
      "S5                3307.441888                  1                   2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:46: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
      "/var/folders/nk/5ry1y2d128x8fgnl550m4c_h0000gp/T/ipykernel_7717/3804270280.py:54: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dem_point_value</th>\n",
       "      <th>dem_mean_5km</th>\n",
       "      <th>dem_std_5km</th>\n",
       "      <th>dist_to_nearest_BF</th>\n",
       "      <th>dist_to_nearest_IND</th>\n",
       "      <th>num_within_5km_BF</th>\n",
       "      <th>num_within_5km_IND</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stations</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S1</th>\n",
       "      <td>-45.0</td>\n",
       "      <td>-44.545135</td>\n",
       "      <td>3.966224</td>\n",
       "      <td>2647.402248</td>\n",
       "      <td>522.326002</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>-51.0</td>\n",
       "      <td>-45.338905</td>\n",
       "      <td>3.547604</td>\n",
       "      <td>3113.557901</td>\n",
       "      <td>3000.625941</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S3</th>\n",
       "      <td>-52.0</td>\n",
       "      <td>-46.650517</td>\n",
       "      <td>2.648768</td>\n",
       "      <td>1841.694727</td>\n",
       "      <td>1298.585986</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S4</th>\n",
       "      <td>-53.0</td>\n",
       "      <td>-47.438057</td>\n",
       "      <td>2.525883</td>\n",
       "      <td>2737.503779</td>\n",
       "      <td>317.052883</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S5</th>\n",
       "      <td>-47.0</td>\n",
       "      <td>-48.027908</td>\n",
       "      <td>2.480211</td>\n",
       "      <td>1637.673006</td>\n",
       "      <td>3307.441888</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S6</th>\n",
       "      <td>-51.0</td>\n",
       "      <td>-48.676369</td>\n",
       "      <td>2.290917</td>\n",
       "      <td>3880.167970</td>\n",
       "      <td>1519.752716</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7</th>\n",
       "      <td>-50.0</td>\n",
       "      <td>-48.972412</td>\n",
       "      <td>2.109034</td>\n",
       "      <td>311.431998</td>\n",
       "      <td>4735.791556</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S8</th>\n",
       "      <td>-50.0</td>\n",
       "      <td>-46.129311</td>\n",
       "      <td>4.607535</td>\n",
       "      <td>719.967325</td>\n",
       "      <td>936.147517</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S9</th>\n",
       "      <td>-51.0</td>\n",
       "      <td>-46.096786</td>\n",
       "      <td>5.321898</td>\n",
       "      <td>1772.540350</td>\n",
       "      <td>3219.036625</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S10</th>\n",
       "      <td>-45.0</td>\n",
       "      <td>-46.316338</td>\n",
       "      <td>5.068447</td>\n",
       "      <td>1448.820004</td>\n",
       "      <td>803.257359</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S11</th>\n",
       "      <td>-53.0</td>\n",
       "      <td>-46.813042</td>\n",
       "      <td>4.549114</td>\n",
       "      <td>4129.629166</td>\n",
       "      <td>3511.586937</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S12</th>\n",
       "      <td>-54.0</td>\n",
       "      <td>-48.730057</td>\n",
       "      <td>2.793475</td>\n",
       "      <td>3132.769571</td>\n",
       "      <td>392.638526</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S13</th>\n",
       "      <td>-47.0</td>\n",
       "      <td>-48.387615</td>\n",
       "      <td>2.485078</td>\n",
       "      <td>6445.100699</td>\n",
       "      <td>1876.298582</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S14</th>\n",
       "      <td>-51.0</td>\n",
       "      <td>-47.975891</td>\n",
       "      <td>2.938087</td>\n",
       "      <td>8040.810472</td>\n",
       "      <td>1571.760205</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S15</th>\n",
       "      <td>-52.0</td>\n",
       "      <td>-44.840939</td>\n",
       "      <td>4.045185</td>\n",
       "      <td>2152.770121</td>\n",
       "      <td>1485.083127</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S16</th>\n",
       "      <td>-54.0</td>\n",
       "      <td>-47.554409</td>\n",
       "      <td>2.586322</td>\n",
       "      <td>5308.323481</td>\n",
       "      <td>791.069150</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S17</th>\n",
       "      <td>-54.0</td>\n",
       "      <td>-47.308769</td>\n",
       "      <td>2.702843</td>\n",
       "      <td>3093.243391</td>\n",
       "      <td>296.994802</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dem_point_value  dem_mean_5km  dem_std_5km  dist_to_nearest_BF  \\\n",
       "Stations                                                                   \n",
       "S1                  -45.0    -44.545135     3.966224         2647.402248   \n",
       "S2                  -51.0    -45.338905     3.547604         3113.557901   \n",
       "S3                  -52.0    -46.650517     2.648768         1841.694727   \n",
       "S4                  -53.0    -47.438057     2.525883         2737.503779   \n",
       "S5                  -47.0    -48.027908     2.480211         1637.673006   \n",
       "S6                  -51.0    -48.676369     2.290917         3880.167970   \n",
       "S7                  -50.0    -48.972412     2.109034          311.431998   \n",
       "S8                  -50.0    -46.129311     4.607535          719.967325   \n",
       "S9                  -51.0    -46.096786     5.321898         1772.540350   \n",
       "S10                 -45.0    -46.316338     5.068447         1448.820004   \n",
       "S11                 -53.0    -46.813042     4.549114         4129.629166   \n",
       "S12                 -54.0    -48.730057     2.793475         3132.769571   \n",
       "S13                 -47.0    -48.387615     2.485078         6445.100699   \n",
       "S14                 -51.0    -47.975891     2.938087         8040.810472   \n",
       "S15                 -52.0    -44.840939     4.045185         2152.770121   \n",
       "S16                 -54.0    -47.554409     2.586322         5308.323481   \n",
       "S17                 -54.0    -47.308769     2.702843         3093.243391   \n",
       "\n",
       "          dist_to_nearest_IND  num_within_5km_BF  num_within_5km_IND  \n",
       "Stations                                                              \n",
       "S1                 522.326002                  3                  10  \n",
       "S2                3000.625941                  2                   2  \n",
       "S3                1298.585986                  9                   7  \n",
       "S4                 317.052883                  5                   4  \n",
       "S5                3307.441888                  1                   2  \n",
       "S6                1519.752716                  2                   4  \n",
       "S7                4735.791556                 28                   0  \n",
       "S8                 936.147517                  4                   4  \n",
       "S9                3219.036625                  6                   1  \n",
       "S10                803.257359                 27                   3  \n",
       "S11               3511.586937                  1                   1  \n",
       "S12                392.638526                  8                  17  \n",
       "S13               1876.298582                  0                   4  \n",
       "S14               1571.760205                  0                   2  \n",
       "S15               1485.083127                  5                   4  \n",
       "S16                791.069150                  0                  15  \n",
       "S17                296.994802                  7                  16  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lulc_variation_results = []\n",
    "hydro_properties_results = []\n",
    "UNIFORM_BUFFER_RADIUS_METERS = 4200\n",
    "OUTPUT_LULC_VARIATIONS_CSV = 'LULC_5km_Variations.csv'\n",
    "OUTPUT_HYDRO_PROPERTIES_CSV = 'Hydrological_Properties.csv'\n",
    "\n",
    "\n",
    "for station_id, point_row in sampling_points_gdf.iterrows():\n",
    "    point_geom = point_row.geometry\n",
    "    \n",
    "    point_lulc_results = {'Stations': station_id}\n",
    "    point_hydro_results = {'Stations': station_id}\n",
    "\n",
    "    # Calculate LULC Variations\n",
    "    for interval, change_map_array in change_maps.items():\n",
    "        try:\n",
    "            proportion_changed = extract_neighborhood_stats(\n",
    "                change_map_array, lulc_profiles[LULC_YEARS[0]], \n",
    "                point_geom, UNIFORM_BUFFER_RADIUS_METERS, stat_type='mean'\n",
    "            )\n",
    "            point_lulc_results[f'variation_{interval}'] = proportion_changed * 100\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LULC for point {station_id} for interval {interval}: {e}\")\n",
    "            point_lulc_results[f'variation_{interval}'] = np.nan\n",
    "    lulc_variation_results.append(point_lulc_results)\n",
    "\n",
    "    # Calculate Hydrological Properties\n",
    "    try:\n",
    "        # Correctly get DEM Point Value using the inverse transform\n",
    "        dem_transform_inv = ~dem_profile['transform']\n",
    "        col, row = dem_transform_inv * (point_geom.x, point_geom.y)\n",
    "\n",
    "        if 0 <= row < dem_array.shape[0] and 0 <= col < dem_array.shape[1]:\n",
    "            point_hydro_results['dem_point_value'] = dem_array[int(row), int(col)]\n",
    "        else:\n",
    "            point_hydro_results['dem_point_value'] = np.nan\n",
    "\n",
    "        # DEM Mean and StdDev in 5km radius\n",
    "        mean_dem = extract_neighborhood_stats(dem_array, dem_profile, point_geom, UNIFORM_BUFFER_RADIUS_METERS, 'mean')\n",
    "        std_dem = extract_neighborhood_stats(dem_array, dem_profile, point_geom, UNIFORM_BUFFER_RADIUS_METERS, 'std')\n",
    "        point_hydro_results['dem_mean_5km'] = mean_dem\n",
    "        point_hydro_results['dem_std_5km'] = std_dem\n",
    "\n",
    "        # Euclidean Distance to Nearest Brick Field\n",
    "        if not brick_field_points_gdf.empty:\n",
    "            nearest_bf_geom = nearest_points(point_geom, brick_field_points_gdf.unary_union)[1]\n",
    "            distance_bf = point_geom.distance(nearest_bf_geom)\n",
    "            point_hydro_results['dist_to_nearest_BF'] = distance_bf\n",
    "        else:\n",
    "            point_hydro_results['dist_to_nearest_BF'] = np.nan\n",
    "\n",
    "        # Euclidean Distance to Nearest Industry\n",
    "        if not industry_points_gdf.empty:\n",
    "            nearest_ind_geom = nearest_points(point_geom, industry_points_gdf.unary_union)[1]\n",
    "            distance_ind = point_geom.distance(nearest_ind_geom)\n",
    "            point_hydro_results['dist_to_nearest_IND'] = distance_ind\n",
    "        else:\n",
    "            point_hydro_results['dist_to_nearest_IND'] = np.nan\n",
    "\n",
    "        # Count sources within 5km circular influence radius\n",
    "        # This serves as a proxy for \"num_upstream\" without a true hydrological network.\n",
    "        bf_in_radius = brick_field_points_gdf[brick_field_points_gdf.within(point_geom.buffer(UNIFORM_BUFFER_RADIUS_METERS))]\n",
    "        point_hydro_results['num_within_5km_BF'] = len(bf_in_radius)\n",
    "        \n",
    "        ind_in_radius = industry_points_gdf[industry_points_gdf.within(point_geom.buffer(UNIFORM_BUFFER_RADIUS_METERS))]\n",
    "        point_hydro_results['num_within_5km_IND'] = len(ind_in_radius)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hydrological features for point {station_id}: {e}\")\n",
    "        for key in ['dem_point_value', 'dem_mean_5km', 'dem_std_5km', 'dist_to_nearest_BF', 'dist_to_nearest_IND', 'num_within_5km_BF', 'num_within_5km_IND']:\n",
    "            point_hydro_results[key] = np.nan\n",
    "    \n",
    "    hydro_properties_results.append(point_hydro_results)\n",
    "\n",
    "\n",
    "lulc_variations_df_calculated = pd.DataFrame(lulc_variation_results).set_index('Stations')\n",
    "lulc_variations_df_calculated.to_csv(OUTPUT_LULC_VARIATIONS_CSV)\n",
    "print(f\"LULC Variations calculated and saved to {OUTPUT_LULC_VARIATIONS_CSV}\")\n",
    "\n",
    "hydro_properties_df_calculated = pd.DataFrame(hydro_properties_results).set_index('Stations')\n",
    "hydro_properties_df_calculated.to_csv(OUTPUT_HYDRO_PROPERTIES_CSV)\n",
    "print(f\"Hydrological Properties calculated and saved to {OUTPUT_HYDRO_PROPERTIES_CSV}\")\n",
    "print(\"\\nFirst 5 rows of Hydrological Properties:\")\n",
    "print(hydro_properties_df_calculated.head())\n",
    "hydro_properties_df_calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04483a22-15d7-4dca-aeac-7ad5e985b796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Extracting Raster-Derived Environmental Indices within 4.2km radius ---\n",
      "Successfully loaded awei.tif\n",
      "Successfully loaded bui.tif\n",
      "Successfully loaded evi.tif\n",
      "Successfully loaded mndwi.tif\n",
      "Successfully loaded ndbi.tif\n",
      "Successfully loaded ndbsi.tif\n",
      "Successfully loaded ndsi.tif\n",
      "Successfully loaded ndvi.tif\n",
      "Successfully loaded ndwi.tif\n",
      "Successfully loaded savi.tif\n",
      "Successfully loaded ui.tif\n",
      "Raster-Derived Features calculated and saved to /Users/rakibhhridoy/Five_Rivers/gis/Raster_Derived_Features.csv\n",
      "\n",
      "First 5 rows of Raster-Derived Features:\n",
      "          awei_Mean_5km  awei_Std_5km  bui_Mean_5km  bui_Std_5km  \\\n",
      "Stations                                                           \n",
      "S1            -0.969579      0.125666     -0.391875     0.268516   \n",
      "S2            -0.975519      0.121869     -0.403841     0.271429   \n",
      "S3            -0.968839      0.134986     -0.340932     0.295861   \n",
      "S4            -0.966311      0.133928     -0.414045     0.287297   \n",
      "S5            -0.975424      0.114931     -0.385784     0.276382   \n",
      "\n",
      "          evi_Mean_5km  evi_Std_5km  mndwi_Mean_5km  mndwi_Std_5km  \\\n",
      "Stations                                                             \n",
      "S1            0.369660     0.196753       -0.258457       0.084574   \n",
      "S2            0.382916     0.195662       -0.267288       0.083496   \n",
      "S3            0.350675     0.213069       -0.267594       0.087344   \n",
      "S4            0.405030     0.208361       -0.277331       0.084295   \n",
      "S5            0.386913     0.206132       -0.277028       0.074940   \n",
      "\n",
      "          ndbi_Mean_5km  ndbi_Std_5km  ...  ndsi_Mean_5km  ndsi_Std_5km  \\\n",
      "Stations                               ...                                \n",
      "S1            -0.065430      0.110392  ...      -0.258457      0.084574   \n",
      "S2            -0.067960      0.111399  ...      -0.267288      0.083496   \n",
      "S3            -0.038436      0.116189  ...      -0.267594      0.087344   \n",
      "S4            -0.066000      0.109603  ...      -0.277331      0.084295   \n",
      "S5            -0.052412      0.102698  ...      -0.277028      0.074940   \n",
      "\n",
      "          ndvi_Mean_5km  ndvi_Std_5km  ndwi_Mean_5km  ndwi_Std_5km  \\\n",
      "Stations                                                             \n",
      "S1             0.326525      0.166733      -0.314660      0.140318   \n",
      "S2             0.335921      0.168417      -0.325435      0.140863   \n",
      "S3             0.302695      0.186979      -0.297655      0.161455   \n",
      "S4             0.348139      0.183284      -0.331705      0.160568   \n",
      "S5             0.333334      0.177635      -0.319583      0.149776   \n",
      "\n",
      "          savi_Mean_5km  savi_Std_5km  ui_Mean_5km  ui_Std_5km  \n",
      "Stations                                                        \n",
      "S1             0.268071      0.138524    -0.196237    0.144225  \n",
      "S2             0.278085      0.138280    -0.197253    0.149461  \n",
      "S3             0.254292      0.152978    -0.148642    0.159921  \n",
      "S4             0.293939      0.149119    -0.196545    0.146529  \n",
      "S5             0.279229      0.146328    -0.181397    0.139160  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Extract Raster-Derived Environmental Indices (5km radius) ---\n",
    "print(f\"\\n--- Step 4: Extracting Raster-Derived Environmental Indices within {UNIFORM_BUFFER_RADIUS_METERS/1000}km radius ---\")\n",
    "\n",
    "raster_features_results = []\n",
    "\n",
    "# Load all specified raster indices\n",
    "loaded_indices = {}\n",
    "for index_file in RASTER_INDICES_TO_EXTRACT:\n",
    "    try:\n",
    "        index_path = os.path.join(CAL_INDICES_DIR, index_file)\n",
    "        \n",
    "        # Use rasterio's built-in reprojection\n",
    "        with rasterio.open(index_path) as src:\n",
    "            # Reproject to target CRS if needed\n",
    "            if src.crs != reference_crs:\n",
    "                transform, width, height = rasterio.warp.calculate_default_transform(\n",
    "                    src.crs, reference_crs, src.width, src.height, *src.bounds\n",
    "                )\n",
    "                array = np.empty((height, width))\n",
    "                rasterio.warp.reproject(\n",
    "                    src.read(1),\n",
    "                    array,\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=reference_crs,\n",
    "                    resampling=rasterio.warp.Resampling.bilinear\n",
    "                )\n",
    "                profile = src.profile.copy()\n",
    "                profile.update({\n",
    "                    'crs': reference_crs,\n",
    "                    'transform': transform,\n",
    "                    'width': width,\n",
    "                    'height': height\n",
    "                })\n",
    "            else:\n",
    "                array = src.read(1)\n",
    "                profile = src.profile\n",
    "            \n",
    "        loaded_indices[index_file] = {'array': array, 'profile': profile}\n",
    "        print(f\"Successfully loaded {index_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {index_file}: {str(e)}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "# Make sure sampling_points_gdf has a 'Stations' column\n",
    "if 'Stations' not in sampling_points_gdf.columns:\n",
    "    sampling_points_gdf['Stations'] = [f'S{i+1}' for i in range(len(sampling_points_gdf))]\n",
    "\n",
    "for station_id, point_row in sampling_points_gdf.iterrows():\n",
    "    point_geom = point_row.geometry\n",
    "    station_name = point_row['Stations']  # Get station name from the row\n",
    "    \n",
    "    point_results = {'Stations': station_name}  # Use the station name from the DataFrame\n",
    "\n",
    "    for index_file, data in loaded_indices.items():\n",
    "        index_name = os.path.splitext(index_file)[0]  # Get base name without extension\n",
    "        \n",
    "        try:\n",
    "            # Extract mean and std values\n",
    "            mean_val = extract_neighborhood_stats(\n",
    "                data['array'], \n",
    "                data['profile'], \n",
    "                point_geom, \n",
    "                UNIFORM_BUFFER_RADIUS_METERS, \n",
    "                'mean'\n",
    "            )\n",
    "            std_val = extract_neighborhood_stats(\n",
    "                data['array'], \n",
    "                data['profile'], \n",
    "                point_geom, \n",
    "                UNIFORM_BUFFER_RADIUS_METERS, \n",
    "                'std'\n",
    "            )\n",
    "            \n",
    "            point_results[f'{index_name}_Mean_5km'] = mean_val\n",
    "            point_results[f'{index_name}_Std_5km'] = std_val\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {index_name} features for station {station_name}: {str(e)}\")\n",
    "            point_results[f'{index_name}_Mean_5km'] = np.nan\n",
    "            point_results[f'{index_name}_Std_5km'] = np.nan\n",
    "            \n",
    "    raster_features_results.append(point_results)\n",
    "\n",
    "# Create DataFrame and set 'Stations' as index\n",
    "raster_features_df = pd.DataFrame(raster_features_results)\n",
    "if not raster_features_df.empty:\n",
    "    raster_features_df.set_index('Stations', inplace=True)\n",
    "    raster_features_df.to_csv(OUTPUT_RASTER_FEATURES_CSV)\n",
    "    print(f\"Raster-Derived Features calculated and saved to {OUTPUT_RASTER_FEATURES_CSV}\")\n",
    "    print(\"\\nFirst 5 rows of Raster-Derived Features:\")\n",
    "    print(raster_features_df.head())\n",
    "else:\n",
    "    print(\"Warning: No raster features were extracted. Output file not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d37e4f3-b909-4ce4-9fe0-a32e6e1859c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awei_Mean_5km</th>\n",
       "      <th>awei_Std_5km</th>\n",
       "      <th>bui_Mean_5km</th>\n",
       "      <th>bui_Std_5km</th>\n",
       "      <th>evi_Mean_5km</th>\n",
       "      <th>evi_Std_5km</th>\n",
       "      <th>mndwi_Mean_5km</th>\n",
       "      <th>mndwi_Std_5km</th>\n",
       "      <th>ndbi_Mean_5km</th>\n",
       "      <th>ndbi_Std_5km</th>\n",
       "      <th>...</th>\n",
       "      <th>ndsi_Mean_5km</th>\n",
       "      <th>ndsi_Std_5km</th>\n",
       "      <th>ndvi_Mean_5km</th>\n",
       "      <th>ndvi_Std_5km</th>\n",
       "      <th>ndwi_Mean_5km</th>\n",
       "      <th>ndwi_Std_5km</th>\n",
       "      <th>savi_Mean_5km</th>\n",
       "      <th>savi_Std_5km</th>\n",
       "      <th>ui_Mean_5km</th>\n",
       "      <th>ui_Std_5km</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stations</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S1</th>\n",
       "      <td>-0.969579</td>\n",
       "      <td>0.125666</td>\n",
       "      <td>-0.391875</td>\n",
       "      <td>0.268516</td>\n",
       "      <td>0.369660</td>\n",
       "      <td>0.196753</td>\n",
       "      <td>-0.258457</td>\n",
       "      <td>0.084574</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>0.110392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258457</td>\n",
       "      <td>0.084574</td>\n",
       "      <td>0.326525</td>\n",
       "      <td>0.166733</td>\n",
       "      <td>-0.314660</td>\n",
       "      <td>0.140318</td>\n",
       "      <td>0.268071</td>\n",
       "      <td>0.138524</td>\n",
       "      <td>-0.196237</td>\n",
       "      <td>0.144225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>-0.975519</td>\n",
       "      <td>0.121869</td>\n",
       "      <td>-0.403841</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.382916</td>\n",
       "      <td>0.195662</td>\n",
       "      <td>-0.267288</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>-0.067960</td>\n",
       "      <td>0.111399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267288</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.335921</td>\n",
       "      <td>0.168417</td>\n",
       "      <td>-0.325435</td>\n",
       "      <td>0.140863</td>\n",
       "      <td>0.278085</td>\n",
       "      <td>0.138280</td>\n",
       "      <td>-0.197253</td>\n",
       "      <td>0.149461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S3</th>\n",
       "      <td>-0.968839</td>\n",
       "      <td>0.134986</td>\n",
       "      <td>-0.340932</td>\n",
       "      <td>0.295861</td>\n",
       "      <td>0.350675</td>\n",
       "      <td>0.213069</td>\n",
       "      <td>-0.267594</td>\n",
       "      <td>0.087344</td>\n",
       "      <td>-0.038436</td>\n",
       "      <td>0.116189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267594</td>\n",
       "      <td>0.087344</td>\n",
       "      <td>0.302695</td>\n",
       "      <td>0.186979</td>\n",
       "      <td>-0.297655</td>\n",
       "      <td>0.161455</td>\n",
       "      <td>0.254292</td>\n",
       "      <td>0.152978</td>\n",
       "      <td>-0.148642</td>\n",
       "      <td>0.159921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S4</th>\n",
       "      <td>-0.966311</td>\n",
       "      <td>0.133928</td>\n",
       "      <td>-0.414045</td>\n",
       "      <td>0.287297</td>\n",
       "      <td>0.405030</td>\n",
       "      <td>0.208361</td>\n",
       "      <td>-0.277331</td>\n",
       "      <td>0.084295</td>\n",
       "      <td>-0.066000</td>\n",
       "      <td>0.109603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277331</td>\n",
       "      <td>0.084295</td>\n",
       "      <td>0.348139</td>\n",
       "      <td>0.183284</td>\n",
       "      <td>-0.331705</td>\n",
       "      <td>0.160568</td>\n",
       "      <td>0.293939</td>\n",
       "      <td>0.149119</td>\n",
       "      <td>-0.196545</td>\n",
       "      <td>0.146529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S5</th>\n",
       "      <td>-0.975424</td>\n",
       "      <td>0.114931</td>\n",
       "      <td>-0.385784</td>\n",
       "      <td>0.276382</td>\n",
       "      <td>0.386913</td>\n",
       "      <td>0.206132</td>\n",
       "      <td>-0.277028</td>\n",
       "      <td>0.074940</td>\n",
       "      <td>-0.052412</td>\n",
       "      <td>0.102698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277028</td>\n",
       "      <td>0.074940</td>\n",
       "      <td>0.333334</td>\n",
       "      <td>0.177635</td>\n",
       "      <td>-0.319583</td>\n",
       "      <td>0.149776</td>\n",
       "      <td>0.279229</td>\n",
       "      <td>0.146328</td>\n",
       "      <td>-0.181397</td>\n",
       "      <td>0.139160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S6</th>\n",
       "      <td>-0.970387</td>\n",
       "      <td>0.117693</td>\n",
       "      <td>-0.419937</td>\n",
       "      <td>0.275770</td>\n",
       "      <td>0.402775</td>\n",
       "      <td>0.207737</td>\n",
       "      <td>-0.266767</td>\n",
       "      <td>0.074856</td>\n",
       "      <td>-0.071562</td>\n",
       "      <td>0.109601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266767</td>\n",
       "      <td>0.074856</td>\n",
       "      <td>0.348554</td>\n",
       "      <td>0.172189</td>\n",
       "      <td>-0.327271</td>\n",
       "      <td>0.145244</td>\n",
       "      <td>0.289568</td>\n",
       "      <td>0.145474</td>\n",
       "      <td>-0.206157</td>\n",
       "      <td>0.145823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7</th>\n",
       "      <td>-0.962058</td>\n",
       "      <td>0.136722</td>\n",
       "      <td>-0.368875</td>\n",
       "      <td>0.276895</td>\n",
       "      <td>0.372610</td>\n",
       "      <td>0.204317</td>\n",
       "      <td>-0.267226</td>\n",
       "      <td>0.083069</td>\n",
       "      <td>-0.047519</td>\n",
       "      <td>0.109168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267226</td>\n",
       "      <td>0.083069</td>\n",
       "      <td>0.321361</td>\n",
       "      <td>0.174628</td>\n",
       "      <td>-0.306425</td>\n",
       "      <td>0.148536</td>\n",
       "      <td>0.267837</td>\n",
       "      <td>0.144281</td>\n",
       "      <td>-0.166605</td>\n",
       "      <td>0.147925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S8</th>\n",
       "      <td>-0.911295</td>\n",
       "      <td>0.215607</td>\n",
       "      <td>-0.273451</td>\n",
       "      <td>0.281583</td>\n",
       "      <td>0.288726</td>\n",
       "      <td>0.219451</td>\n",
       "      <td>-0.225360</td>\n",
       "      <td>0.127602</td>\n",
       "      <td>-0.024565</td>\n",
       "      <td>0.117395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225360</td>\n",
       "      <td>0.127602</td>\n",
       "      <td>0.249471</td>\n",
       "      <td>0.184091</td>\n",
       "      <td>-0.245368</td>\n",
       "      <td>0.164379</td>\n",
       "      <td>0.205817</td>\n",
       "      <td>0.152410</td>\n",
       "      <td>-0.123441</td>\n",
       "      <td>0.158337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S9</th>\n",
       "      <td>-0.908223</td>\n",
       "      <td>0.213117</td>\n",
       "      <td>-0.268975</td>\n",
       "      <td>0.298324</td>\n",
       "      <td>0.278238</td>\n",
       "      <td>0.225503</td>\n",
       "      <td>-0.212291</td>\n",
       "      <td>0.120741</td>\n",
       "      <td>-0.029250</td>\n",
       "      <td>0.125940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212291</td>\n",
       "      <td>0.120741</td>\n",
       "      <td>0.240201</td>\n",
       "      <td>0.188328</td>\n",
       "      <td>-0.236469</td>\n",
       "      <td>0.166020</td>\n",
       "      <td>0.198267</td>\n",
       "      <td>0.157254</td>\n",
       "      <td>-0.128814</td>\n",
       "      <td>0.170339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S10</th>\n",
       "      <td>-0.925424</td>\n",
       "      <td>0.193496</td>\n",
       "      <td>-0.185043</td>\n",
       "      <td>0.296732</td>\n",
       "      <td>0.222989</td>\n",
       "      <td>0.217097</td>\n",
       "      <td>-0.211518</td>\n",
       "      <td>0.107006</td>\n",
       "      <td>0.006872</td>\n",
       "      <td>0.124079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211518</td>\n",
       "      <td>0.107006</td>\n",
       "      <td>0.192188</td>\n",
       "      <td>0.185083</td>\n",
       "      <td>-0.201415</td>\n",
       "      <td>0.161885</td>\n",
       "      <td>0.158959</td>\n",
       "      <td>0.152160</td>\n",
       "      <td>-0.070563</td>\n",
       "      <td>0.169799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S11</th>\n",
       "      <td>-0.967044</td>\n",
       "      <td>0.144577</td>\n",
       "      <td>-0.241094</td>\n",
       "      <td>0.279690</td>\n",
       "      <td>0.281772</td>\n",
       "      <td>0.214059</td>\n",
       "      <td>-0.249940</td>\n",
       "      <td>0.092854</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.104567</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249940</td>\n",
       "      <td>0.092854</td>\n",
       "      <td>0.241751</td>\n",
       "      <td>0.182776</td>\n",
       "      <td>-0.244814</td>\n",
       "      <td>0.156136</td>\n",
       "      <td>0.199891</td>\n",
       "      <td>0.150823</td>\n",
       "      <td>-0.086674</td>\n",
       "      <td>0.153465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S12</th>\n",
       "      <td>-0.939712</td>\n",
       "      <td>0.179793</td>\n",
       "      <td>-0.195548</td>\n",
       "      <td>0.260477</td>\n",
       "      <td>0.239192</td>\n",
       "      <td>0.197486</td>\n",
       "      <td>-0.221708</td>\n",
       "      <td>0.100958</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>0.102877</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221708</td>\n",
       "      <td>0.100958</td>\n",
       "      <td>0.205425</td>\n",
       "      <td>0.169121</td>\n",
       "      <td>-0.209438</td>\n",
       "      <td>0.151336</td>\n",
       "      <td>0.169243</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>-0.064381</td>\n",
       "      <td>0.142201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S13</th>\n",
       "      <td>-0.947536</td>\n",
       "      <td>0.156188</td>\n",
       "      <td>-0.299042</td>\n",
       "      <td>0.262146</td>\n",
       "      <td>0.306011</td>\n",
       "      <td>0.192063</td>\n",
       "      <td>-0.228607</td>\n",
       "      <td>0.089011</td>\n",
       "      <td>-0.031589</td>\n",
       "      <td>0.106372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228607</td>\n",
       "      <td>0.089011</td>\n",
       "      <td>0.267484</td>\n",
       "      <td>0.164355</td>\n",
       "      <td>-0.255010</td>\n",
       "      <td>0.145022</td>\n",
       "      <td>0.217400</td>\n",
       "      <td>0.134372</td>\n",
       "      <td>-0.137150</td>\n",
       "      <td>0.147699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S14</th>\n",
       "      <td>-0.972457</td>\n",
       "      <td>0.106528</td>\n",
       "      <td>-0.285881</td>\n",
       "      <td>0.244279</td>\n",
       "      <td>0.303734</td>\n",
       "      <td>0.175398</td>\n",
       "      <td>-0.253183</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>-0.014494</td>\n",
       "      <td>0.101221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253183</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>0.271421</td>\n",
       "      <td>0.151088</td>\n",
       "      <td>-0.263737</td>\n",
       "      <td>0.127796</td>\n",
       "      <td>0.219514</td>\n",
       "      <td>0.124160</td>\n",
       "      <td>-0.120276</td>\n",
       "      <td>0.138453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S15</th>\n",
       "      <td>-0.952942</td>\n",
       "      <td>0.151389</td>\n",
       "      <td>-0.349193</td>\n",
       "      <td>0.258896</td>\n",
       "      <td>0.338682</td>\n",
       "      <td>0.184207</td>\n",
       "      <td>-0.248923</td>\n",
       "      <td>0.089278</td>\n",
       "      <td>-0.044555</td>\n",
       "      <td>0.104345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248923</td>\n",
       "      <td>0.089278</td>\n",
       "      <td>0.304591</td>\n",
       "      <td>0.163721</td>\n",
       "      <td>-0.286872</td>\n",
       "      <td>0.144649</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.131027</td>\n",
       "      <td>-0.166204</td>\n",
       "      <td>0.141667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S16</th>\n",
       "      <td>-0.949471</td>\n",
       "      <td>0.155413</td>\n",
       "      <td>-0.228862</td>\n",
       "      <td>0.249034</td>\n",
       "      <td>0.257974</td>\n",
       "      <td>0.185465</td>\n",
       "      <td>-0.209759</td>\n",
       "      <td>0.081398</td>\n",
       "      <td>-0.010008</td>\n",
       "      <td>0.103525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209759</td>\n",
       "      <td>0.081398</td>\n",
       "      <td>0.218860</td>\n",
       "      <td>0.154316</td>\n",
       "      <td>-0.216536</td>\n",
       "      <td>0.132863</td>\n",
       "      <td>0.177135</td>\n",
       "      <td>0.126952</td>\n",
       "      <td>-0.089701</td>\n",
       "      <td>0.147261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S17</th>\n",
       "      <td>-0.960438</td>\n",
       "      <td>0.138603</td>\n",
       "      <td>-0.273948</td>\n",
       "      <td>0.277066</td>\n",
       "      <td>0.299946</td>\n",
       "      <td>0.206826</td>\n",
       "      <td>-0.230555</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>-0.020812</td>\n",
       "      <td>0.113619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230555</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>0.253145</td>\n",
       "      <td>0.170179</td>\n",
       "      <td>-0.246078</td>\n",
       "      <td>0.143351</td>\n",
       "      <td>0.206801</td>\n",
       "      <td>0.142082</td>\n",
       "      <td>-0.111420</td>\n",
       "      <td>0.163782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          awei_Mean_5km  awei_Std_5km  bui_Mean_5km  bui_Std_5km  \\\n",
       "Stations                                                           \n",
       "S1            -0.969579      0.125666     -0.391875     0.268516   \n",
       "S2            -0.975519      0.121869     -0.403841     0.271429   \n",
       "S3            -0.968839      0.134986     -0.340932     0.295861   \n",
       "S4            -0.966311      0.133928     -0.414045     0.287297   \n",
       "S5            -0.975424      0.114931     -0.385784     0.276382   \n",
       "S6            -0.970387      0.117693     -0.419937     0.275770   \n",
       "S7            -0.962058      0.136722     -0.368875     0.276895   \n",
       "S8            -0.911295      0.215607     -0.273451     0.281583   \n",
       "S9            -0.908223      0.213117     -0.268975     0.298324   \n",
       "S10           -0.925424      0.193496     -0.185043     0.296732   \n",
       "S11           -0.967044      0.144577     -0.241094     0.279690   \n",
       "S12           -0.939712      0.179793     -0.195548     0.260477   \n",
       "S13           -0.947536      0.156188     -0.299042     0.262146   \n",
       "S14           -0.972457      0.106528     -0.285881     0.244279   \n",
       "S15           -0.952942      0.151389     -0.349193     0.258896   \n",
       "S16           -0.949471      0.155413     -0.228862     0.249034   \n",
       "S17           -0.960438      0.138603     -0.273948     0.277066   \n",
       "\n",
       "          evi_Mean_5km  evi_Std_5km  mndwi_Mean_5km  mndwi_Std_5km  \\\n",
       "Stations                                                             \n",
       "S1            0.369660     0.196753       -0.258457       0.084574   \n",
       "S2            0.382916     0.195662       -0.267288       0.083496   \n",
       "S3            0.350675     0.213069       -0.267594       0.087344   \n",
       "S4            0.405030     0.208361       -0.277331       0.084295   \n",
       "S5            0.386913     0.206132       -0.277028       0.074940   \n",
       "S6            0.402775     0.207737       -0.266767       0.074856   \n",
       "S7            0.372610     0.204317       -0.267226       0.083069   \n",
       "S8            0.288726     0.219451       -0.225360       0.127602   \n",
       "S9            0.278238     0.225503       -0.212291       0.120741   \n",
       "S10           0.222989     0.217097       -0.211518       0.107006   \n",
       "S11           0.281772     0.214059       -0.249940       0.092854   \n",
       "S12           0.239192     0.197486       -0.221708       0.100958   \n",
       "S13           0.306011     0.192063       -0.228607       0.089011   \n",
       "S14           0.303734     0.175398       -0.253183       0.074251   \n",
       "S15           0.338682     0.184207       -0.248923       0.089278   \n",
       "S16           0.257974     0.185465       -0.209759       0.081398   \n",
       "S17           0.299946     0.206826       -0.230555       0.076586   \n",
       "\n",
       "          ndbi_Mean_5km  ndbi_Std_5km  ...  ndsi_Mean_5km  ndsi_Std_5km  \\\n",
       "Stations                               ...                                \n",
       "S1            -0.065430      0.110392  ...      -0.258457      0.084574   \n",
       "S2            -0.067960      0.111399  ...      -0.267288      0.083496   \n",
       "S3            -0.038436      0.116189  ...      -0.267594      0.087344   \n",
       "S4            -0.066000      0.109603  ...      -0.277331      0.084295   \n",
       "S5            -0.052412      0.102698  ...      -0.277028      0.074940   \n",
       "S6            -0.071562      0.109601  ...      -0.266767      0.074856   \n",
       "S7            -0.047519      0.109168  ...      -0.267226      0.083069   \n",
       "S8            -0.024565      0.117395  ...      -0.225360      0.127602   \n",
       "S9            -0.029250      0.125940  ...      -0.212291      0.120741   \n",
       "S10            0.006872      0.124079  ...      -0.211518      0.107006   \n",
       "S11            0.000638      0.104567  ...      -0.249940      0.092854   \n",
       "S12            0.009686      0.102877  ...      -0.221708      0.100958   \n",
       "S13           -0.031589      0.106372  ...      -0.228607      0.089011   \n",
       "S14           -0.014494      0.101221  ...      -0.253183      0.074251   \n",
       "S15           -0.044555      0.104345  ...      -0.248923      0.089278   \n",
       "S16           -0.010008      0.103525  ...      -0.209759      0.081398   \n",
       "S17           -0.020812      0.113619  ...      -0.230555      0.076586   \n",
       "\n",
       "          ndvi_Mean_5km  ndvi_Std_5km  ndwi_Mean_5km  ndwi_Std_5km  \\\n",
       "Stations                                                             \n",
       "S1             0.326525      0.166733      -0.314660      0.140318   \n",
       "S2             0.335921      0.168417      -0.325435      0.140863   \n",
       "S3             0.302695      0.186979      -0.297655      0.161455   \n",
       "S4             0.348139      0.183284      -0.331705      0.160568   \n",
       "S5             0.333334      0.177635      -0.319583      0.149776   \n",
       "S6             0.348554      0.172189      -0.327271      0.145244   \n",
       "S7             0.321361      0.174628      -0.306425      0.148536   \n",
       "S8             0.249471      0.184091      -0.245368      0.164379   \n",
       "S9             0.240201      0.188328      -0.236469      0.166020   \n",
       "S10            0.192188      0.185083      -0.201415      0.161885   \n",
       "S11            0.241751      0.182776      -0.244814      0.156136   \n",
       "S12            0.205425      0.169121      -0.209438      0.151336   \n",
       "S13            0.267484      0.164355      -0.255010      0.145022   \n",
       "S14            0.271421      0.151088      -0.263737      0.127796   \n",
       "S15            0.304591      0.163721      -0.286872      0.144649   \n",
       "S16            0.218860      0.154316      -0.216536      0.132863   \n",
       "S17            0.253145      0.170179      -0.246078      0.143351   \n",
       "\n",
       "          savi_Mean_5km  savi_Std_5km  ui_Mean_5km  ui_Std_5km  \n",
       "Stations                                                        \n",
       "S1             0.268071      0.138524    -0.196237    0.144225  \n",
       "S2             0.278085      0.138280    -0.197253    0.149461  \n",
       "S3             0.254292      0.152978    -0.148642    0.159921  \n",
       "S4             0.293939      0.149119    -0.196545    0.146529  \n",
       "S5             0.279229      0.146328    -0.181397    0.139160  \n",
       "S6             0.289568      0.145474    -0.206157    0.145823  \n",
       "S7             0.267837      0.144281    -0.166605    0.147925  \n",
       "S8             0.205817      0.152410    -0.123441    0.158337  \n",
       "S9             0.198267      0.157254    -0.128814    0.170339  \n",
       "S10            0.158959      0.152160    -0.070563    0.169799  \n",
       "S11            0.199891      0.150823    -0.086674    0.153465  \n",
       "S12            0.169243      0.138629    -0.064381    0.142201  \n",
       "S13            0.217400      0.134372    -0.137150    0.147699  \n",
       "S14            0.219514      0.124160    -0.120276    0.138453  \n",
       "S15            0.245763      0.131027    -0.166204    0.141667  \n",
       "S16            0.177135      0.126952    -0.089701    0.147261  \n",
       "S17            0.206801      0.142082    -0.111420    0.163782  \n",
       "\n",
       "[17 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raster_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f51679a-f3bf-4885-89c7-18c84c541832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded awei.tif\n",
      "Successfully loaded bui.tif\n",
      "Successfully loaded evi.tif\n",
      "Successfully loaded mndwi.tif\n",
      "Successfully loaded ndbi.tif\n",
      "Successfully loaded ndbsi.tif\n",
      "Successfully loaded ndsi.tif\n",
      "Successfully loaded ndvi.tif\n",
      "Successfully loaded ndwi.tif\n",
      "Successfully loaded savi.tif\n",
      "Successfully loaded ui.tif\n",
      "Raster-Derived Features calculated and saved to /Users/rakibhhridoy/Five_Rivers/gis/Raster_Derived_Features.csv\n",
      "\n",
      "First 5 rows of Raster-Derived Features:\n",
      "          awei_Mean_5km  awei_Std_5km  bui_Mean_5km  bui_Std_5km  \\\n",
      "Stations                                                           \n",
      "S1            -0.969734      0.125220     -0.392400     0.268581   \n",
      "S2            -0.975545      0.121653     -0.404656     0.270563   \n",
      "S3            -0.969130      0.133851     -0.344786     0.295459   \n",
      "S4            -0.966623      0.133369     -0.413579     0.287817   \n",
      "S5            -0.975206      0.115340     -0.385491     0.277192   \n",
      "\n",
      "          evi_Mean_5km  evi_Std_5km  mndwi_Mean_5km  mndwi_Std_5km  \\\n",
      "Stations                                                             \n",
      "S1            0.370143     0.196729       -0.258778       0.084377   \n",
      "S2            0.383590     0.195119       -0.267662       0.083385   \n",
      "S3            0.353004     0.212681       -0.268019       0.086924   \n",
      "S4            0.404575     0.208671       -0.277133       0.084082   \n",
      "S5            0.386739     0.206494       -0.276880       0.075112   \n",
      "\n",
      "          ndbi_Mean_5km  ndbi_Std_5km  ...  ndsi_Mean_5km  ndsi_Std_5km  \\\n",
      "Stations                               ...                                \n",
      "S1            -0.065579      0.110679  ...      -0.258778      0.084377   \n",
      "S2            -0.068138      0.111010  ...      -0.267662      0.083385   \n",
      "S3            -0.040037      0.116289  ...      -0.268019      0.086924   \n",
      "S4            -0.065932      0.110034  ...      -0.277133      0.084082   \n",
      "S5            -0.052333      0.103177  ...      -0.276880      0.075112   \n",
      "\n",
      "          ndvi_Mean_5km  ndvi_Std_5km  ndwi_Mean_5km  ndwi_Std_5km  \\\n",
      "Stations                                                             \n",
      "S1             0.326918      0.166530      -0.315127      0.139971   \n",
      "S2             0.336557      0.167914      -0.325971      0.140518   \n",
      "S3             0.304957      0.186484      -0.299539      0.160875   \n",
      "S4             0.347748      0.183410      -0.331451      0.160435   \n",
      "S5             0.333129      0.178004      -0.319351      0.150169   \n",
      "\n",
      "          savi_Mean_5km  savi_Std_5km  ui_Mean_5km  ui_Std_5km  \n",
      "Stations                                                        \n",
      "S1             0.268463      0.138414    -0.196683    0.144285  \n",
      "S2             0.278609      0.137878    -0.197708    0.148793  \n",
      "S3             0.256057      0.152663    -0.151110    0.159738  \n",
      "S4             0.293582      0.149312    -0.196330    0.147066  \n",
      "S5             0.279081      0.146610    -0.181140    0.139715  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awei_Mean_5km</th>\n",
       "      <th>awei_Std_5km</th>\n",
       "      <th>bui_Mean_5km</th>\n",
       "      <th>bui_Std_5km</th>\n",
       "      <th>evi_Mean_5km</th>\n",
       "      <th>evi_Std_5km</th>\n",
       "      <th>mndwi_Mean_5km</th>\n",
       "      <th>mndwi_Std_5km</th>\n",
       "      <th>ndbi_Mean_5km</th>\n",
       "      <th>ndbi_Std_5km</th>\n",
       "      <th>...</th>\n",
       "      <th>ndsi_Mean_5km</th>\n",
       "      <th>ndsi_Std_5km</th>\n",
       "      <th>ndvi_Mean_5km</th>\n",
       "      <th>ndvi_Std_5km</th>\n",
       "      <th>ndwi_Mean_5km</th>\n",
       "      <th>ndwi_Std_5km</th>\n",
       "      <th>savi_Mean_5km</th>\n",
       "      <th>savi_Std_5km</th>\n",
       "      <th>ui_Mean_5km</th>\n",
       "      <th>ui_Std_5km</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stations</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S1</th>\n",
       "      <td>-0.969734</td>\n",
       "      <td>0.125220</td>\n",
       "      <td>-0.392400</td>\n",
       "      <td>0.268581</td>\n",
       "      <td>0.370143</td>\n",
       "      <td>0.196729</td>\n",
       "      <td>-0.258778</td>\n",
       "      <td>0.084377</td>\n",
       "      <td>-0.065579</td>\n",
       "      <td>0.110679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258778</td>\n",
       "      <td>0.084377</td>\n",
       "      <td>0.326918</td>\n",
       "      <td>0.166530</td>\n",
       "      <td>-0.315127</td>\n",
       "      <td>0.139971</td>\n",
       "      <td>0.268463</td>\n",
       "      <td>0.138414</td>\n",
       "      <td>-0.196683</td>\n",
       "      <td>0.144285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>-0.975545</td>\n",
       "      <td>0.121653</td>\n",
       "      <td>-0.404656</td>\n",
       "      <td>0.270563</td>\n",
       "      <td>0.383590</td>\n",
       "      <td>0.195119</td>\n",
       "      <td>-0.267662</td>\n",
       "      <td>0.083385</td>\n",
       "      <td>-0.068138</td>\n",
       "      <td>0.111010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267662</td>\n",
       "      <td>0.083385</td>\n",
       "      <td>0.336557</td>\n",
       "      <td>0.167914</td>\n",
       "      <td>-0.325971</td>\n",
       "      <td>0.140518</td>\n",
       "      <td>0.278609</td>\n",
       "      <td>0.137878</td>\n",
       "      <td>-0.197708</td>\n",
       "      <td>0.148793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S3</th>\n",
       "      <td>-0.969130</td>\n",
       "      <td>0.133851</td>\n",
       "      <td>-0.344786</td>\n",
       "      <td>0.295459</td>\n",
       "      <td>0.353004</td>\n",
       "      <td>0.212681</td>\n",
       "      <td>-0.268019</td>\n",
       "      <td>0.086924</td>\n",
       "      <td>-0.040037</td>\n",
       "      <td>0.116289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268019</td>\n",
       "      <td>0.086924</td>\n",
       "      <td>0.304957</td>\n",
       "      <td>0.186484</td>\n",
       "      <td>-0.299539</td>\n",
       "      <td>0.160875</td>\n",
       "      <td>0.256057</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>-0.151110</td>\n",
       "      <td>0.159738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S4</th>\n",
       "      <td>-0.966623</td>\n",
       "      <td>0.133369</td>\n",
       "      <td>-0.413579</td>\n",
       "      <td>0.287817</td>\n",
       "      <td>0.404575</td>\n",
       "      <td>0.208671</td>\n",
       "      <td>-0.277133</td>\n",
       "      <td>0.084082</td>\n",
       "      <td>-0.065932</td>\n",
       "      <td>0.110034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277133</td>\n",
       "      <td>0.084082</td>\n",
       "      <td>0.347748</td>\n",
       "      <td>0.183410</td>\n",
       "      <td>-0.331451</td>\n",
       "      <td>0.160435</td>\n",
       "      <td>0.293582</td>\n",
       "      <td>0.149312</td>\n",
       "      <td>-0.196330</td>\n",
       "      <td>0.147066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S5</th>\n",
       "      <td>-0.975206</td>\n",
       "      <td>0.115340</td>\n",
       "      <td>-0.385491</td>\n",
       "      <td>0.277192</td>\n",
       "      <td>0.386739</td>\n",
       "      <td>0.206494</td>\n",
       "      <td>-0.276880</td>\n",
       "      <td>0.075112</td>\n",
       "      <td>-0.052333</td>\n",
       "      <td>0.103177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276880</td>\n",
       "      <td>0.075112</td>\n",
       "      <td>0.333129</td>\n",
       "      <td>0.178004</td>\n",
       "      <td>-0.319351</td>\n",
       "      <td>0.150169</td>\n",
       "      <td>0.279081</td>\n",
       "      <td>0.146610</td>\n",
       "      <td>-0.181140</td>\n",
       "      <td>0.139715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S6</th>\n",
       "      <td>-0.970859</td>\n",
       "      <td>0.116927</td>\n",
       "      <td>-0.420915</td>\n",
       "      <td>0.275077</td>\n",
       "      <td>0.403468</td>\n",
       "      <td>0.207132</td>\n",
       "      <td>-0.267240</td>\n",
       "      <td>0.074591</td>\n",
       "      <td>-0.071800</td>\n",
       "      <td>0.109332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267240</td>\n",
       "      <td>0.074591</td>\n",
       "      <td>0.349287</td>\n",
       "      <td>0.171751</td>\n",
       "      <td>-0.327956</td>\n",
       "      <td>0.144789</td>\n",
       "      <td>0.290144</td>\n",
       "      <td>0.145079</td>\n",
       "      <td>-0.206604</td>\n",
       "      <td>0.145446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7</th>\n",
       "      <td>-0.962687</td>\n",
       "      <td>0.135294</td>\n",
       "      <td>-0.369700</td>\n",
       "      <td>0.276092</td>\n",
       "      <td>0.373285</td>\n",
       "      <td>0.203899</td>\n",
       "      <td>-0.267627</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>-0.047726</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267627</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>0.321981</td>\n",
       "      <td>0.174021</td>\n",
       "      <td>-0.307016</td>\n",
       "      <td>0.147871</td>\n",
       "      <td>0.268282</td>\n",
       "      <td>0.143934</td>\n",
       "      <td>-0.167008</td>\n",
       "      <td>0.147687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S8</th>\n",
       "      <td>-0.912257</td>\n",
       "      <td>0.214371</td>\n",
       "      <td>-0.273639</td>\n",
       "      <td>0.281524</td>\n",
       "      <td>0.289075</td>\n",
       "      <td>0.219483</td>\n",
       "      <td>-0.225617</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>-0.024594</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225617</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>0.249634</td>\n",
       "      <td>0.183916</td>\n",
       "      <td>-0.245642</td>\n",
       "      <td>0.164066</td>\n",
       "      <td>0.205987</td>\n",
       "      <td>0.152365</td>\n",
       "      <td>-0.123514</td>\n",
       "      <td>0.158304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S9</th>\n",
       "      <td>-0.911569</td>\n",
       "      <td>0.209866</td>\n",
       "      <td>-0.268629</td>\n",
       "      <td>0.297243</td>\n",
       "      <td>0.279090</td>\n",
       "      <td>0.224655</td>\n",
       "      <td>-0.214845</td>\n",
       "      <td>0.119808</td>\n",
       "      <td>-0.028060</td>\n",
       "      <td>0.125581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214845</td>\n",
       "      <td>0.119808</td>\n",
       "      <td>0.241047</td>\n",
       "      <td>0.187538</td>\n",
       "      <td>-0.237883</td>\n",
       "      <td>0.165049</td>\n",
       "      <td>0.198980</td>\n",
       "      <td>0.156682</td>\n",
       "      <td>-0.127595</td>\n",
       "      <td>0.169854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S10</th>\n",
       "      <td>-0.926832</td>\n",
       "      <td>0.191991</td>\n",
       "      <td>-0.186091</td>\n",
       "      <td>0.295221</td>\n",
       "      <td>0.224110</td>\n",
       "      <td>0.216209</td>\n",
       "      <td>-0.212660</td>\n",
       "      <td>0.106769</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>0.123143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212660</td>\n",
       "      <td>0.106769</td>\n",
       "      <td>0.193279</td>\n",
       "      <td>0.184460</td>\n",
       "      <td>-0.202531</td>\n",
       "      <td>0.161284</td>\n",
       "      <td>0.159810</td>\n",
       "      <td>0.151603</td>\n",
       "      <td>-0.070946</td>\n",
       "      <td>0.168767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S11</th>\n",
       "      <td>-0.967904</td>\n",
       "      <td>0.142618</td>\n",
       "      <td>-0.242683</td>\n",
       "      <td>0.279836</td>\n",
       "      <td>0.282695</td>\n",
       "      <td>0.214126</td>\n",
       "      <td>-0.250114</td>\n",
       "      <td>0.092108</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>0.104781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250114</td>\n",
       "      <td>0.092108</td>\n",
       "      <td>0.242637</td>\n",
       "      <td>0.182637</td>\n",
       "      <td>-0.245637</td>\n",
       "      <td>0.155674</td>\n",
       "      <td>0.200581</td>\n",
       "      <td>0.150852</td>\n",
       "      <td>-0.087870</td>\n",
       "      <td>0.153718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S12</th>\n",
       "      <td>-0.939381</td>\n",
       "      <td>0.180241</td>\n",
       "      <td>-0.193897</td>\n",
       "      <td>0.260218</td>\n",
       "      <td>0.238057</td>\n",
       "      <td>0.197188</td>\n",
       "      <td>-0.221493</td>\n",
       "      <td>0.100933</td>\n",
       "      <td>0.010339</td>\n",
       "      <td>0.102811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221493</td>\n",
       "      <td>0.100933</td>\n",
       "      <td>0.204421</td>\n",
       "      <td>0.168909</td>\n",
       "      <td>-0.208612</td>\n",
       "      <td>0.151135</td>\n",
       "      <td>0.168406</td>\n",
       "      <td>0.138426</td>\n",
       "      <td>-0.063331</td>\n",
       "      <td>0.142096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S13</th>\n",
       "      <td>-0.946959</td>\n",
       "      <td>0.157435</td>\n",
       "      <td>-0.297070</td>\n",
       "      <td>0.262549</td>\n",
       "      <td>0.304693</td>\n",
       "      <td>0.192439</td>\n",
       "      <td>-0.228213</td>\n",
       "      <td>0.089517</td>\n",
       "      <td>-0.030887</td>\n",
       "      <td>0.106504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228213</td>\n",
       "      <td>0.089517</td>\n",
       "      <td>0.266211</td>\n",
       "      <td>0.164723</td>\n",
       "      <td>-0.253965</td>\n",
       "      <td>0.145395</td>\n",
       "      <td>0.216405</td>\n",
       "      <td>0.134624</td>\n",
       "      <td>-0.135850</td>\n",
       "      <td>0.147933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S14</th>\n",
       "      <td>-0.972574</td>\n",
       "      <td>0.106274</td>\n",
       "      <td>-0.287535</td>\n",
       "      <td>0.244014</td>\n",
       "      <td>0.304888</td>\n",
       "      <td>0.175301</td>\n",
       "      <td>-0.253459</td>\n",
       "      <td>0.074301</td>\n",
       "      <td>-0.015164</td>\n",
       "      <td>0.101140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253459</td>\n",
       "      <td>0.074301</td>\n",
       "      <td>0.272403</td>\n",
       "      <td>0.150928</td>\n",
       "      <td>-0.264641</td>\n",
       "      <td>0.127650</td>\n",
       "      <td>0.220338</td>\n",
       "      <td>0.124060</td>\n",
       "      <td>-0.121335</td>\n",
       "      <td>0.138313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S15</th>\n",
       "      <td>-0.953562</td>\n",
       "      <td>0.150583</td>\n",
       "      <td>-0.348358</td>\n",
       "      <td>0.258878</td>\n",
       "      <td>0.338384</td>\n",
       "      <td>0.184151</td>\n",
       "      <td>-0.249199</td>\n",
       "      <td>0.088803</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>0.104471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249199</td>\n",
       "      <td>0.088803</td>\n",
       "      <td>0.304223</td>\n",
       "      <td>0.163522</td>\n",
       "      <td>-0.286714</td>\n",
       "      <td>0.144222</td>\n",
       "      <td>0.245536</td>\n",
       "      <td>0.130982</td>\n",
       "      <td>-0.165660</td>\n",
       "      <td>0.141873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S16</th>\n",
       "      <td>-0.948961</td>\n",
       "      <td>0.156283</td>\n",
       "      <td>-0.230643</td>\n",
       "      <td>0.250375</td>\n",
       "      <td>0.259134</td>\n",
       "      <td>0.186438</td>\n",
       "      <td>-0.209434</td>\n",
       "      <td>0.081880</td>\n",
       "      <td>-0.010911</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209434</td>\n",
       "      <td>0.081880</td>\n",
       "      <td>0.219738</td>\n",
       "      <td>0.155096</td>\n",
       "      <td>-0.217052</td>\n",
       "      <td>0.133469</td>\n",
       "      <td>0.177903</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>-0.091081</td>\n",
       "      <td>0.148213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S17</th>\n",
       "      <td>-0.959388</td>\n",
       "      <td>0.140262</td>\n",
       "      <td>-0.274293</td>\n",
       "      <td>0.277351</td>\n",
       "      <td>0.299989</td>\n",
       "      <td>0.207142</td>\n",
       "      <td>-0.230018</td>\n",
       "      <td>0.077240</td>\n",
       "      <td>-0.021189</td>\n",
       "      <td>0.113644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230018</td>\n",
       "      <td>0.077240</td>\n",
       "      <td>0.253114</td>\n",
       "      <td>0.170487</td>\n",
       "      <td>-0.245913</td>\n",
       "      <td>0.143858</td>\n",
       "      <td>0.206823</td>\n",
       "      <td>0.142244</td>\n",
       "      <td>-0.111999</td>\n",
       "      <td>0.163657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          awei_Mean_5km  awei_Std_5km  bui_Mean_5km  bui_Std_5km  \\\n",
       "Stations                                                           \n",
       "S1            -0.969734      0.125220     -0.392400     0.268581   \n",
       "S2            -0.975545      0.121653     -0.404656     0.270563   \n",
       "S3            -0.969130      0.133851     -0.344786     0.295459   \n",
       "S4            -0.966623      0.133369     -0.413579     0.287817   \n",
       "S5            -0.975206      0.115340     -0.385491     0.277192   \n",
       "S6            -0.970859      0.116927     -0.420915     0.275077   \n",
       "S7            -0.962687      0.135294     -0.369700     0.276092   \n",
       "S8            -0.912257      0.214371     -0.273639     0.281524   \n",
       "S9            -0.911569      0.209866     -0.268629     0.297243   \n",
       "S10           -0.926832      0.191991     -0.186091     0.295221   \n",
       "S11           -0.967904      0.142618     -0.242683     0.279836   \n",
       "S12           -0.939381      0.180241     -0.193897     0.260218   \n",
       "S13           -0.946959      0.157435     -0.297070     0.262549   \n",
       "S14           -0.972574      0.106274     -0.287535     0.244014   \n",
       "S15           -0.953562      0.150583     -0.348358     0.258878   \n",
       "S16           -0.948961      0.156283     -0.230643     0.250375   \n",
       "S17           -0.959388      0.140262     -0.274293     0.277351   \n",
       "\n",
       "          evi_Mean_5km  evi_Std_5km  mndwi_Mean_5km  mndwi_Std_5km  \\\n",
       "Stations                                                             \n",
       "S1            0.370143     0.196729       -0.258778       0.084377   \n",
       "S2            0.383590     0.195119       -0.267662       0.083385   \n",
       "S3            0.353004     0.212681       -0.268019       0.086924   \n",
       "S4            0.404575     0.208671       -0.277133       0.084082   \n",
       "S5            0.386739     0.206494       -0.276880       0.075112   \n",
       "S6            0.403468     0.207132       -0.267240       0.074591   \n",
       "S7            0.373285     0.203899       -0.267627       0.082630   \n",
       "S8            0.289075     0.219483       -0.225617       0.127090   \n",
       "S9            0.279090     0.224655       -0.214845       0.119808   \n",
       "S10           0.224110     0.216209       -0.212660       0.106769   \n",
       "S11           0.282695     0.214126       -0.250114       0.092108   \n",
       "S12           0.238057     0.197188       -0.221493       0.100933   \n",
       "S13           0.304693     0.192439       -0.228213       0.089517   \n",
       "S14           0.304888     0.175301       -0.253459       0.074301   \n",
       "S15           0.338384     0.184151       -0.249199       0.088803   \n",
       "S16           0.259134     0.186438       -0.209434       0.081880   \n",
       "S17           0.299989     0.207142       -0.230018       0.077240   \n",
       "\n",
       "          ndbi_Mean_5km  ndbi_Std_5km  ...  ndsi_Mean_5km  ndsi_Std_5km  \\\n",
       "Stations                               ...                                \n",
       "S1            -0.065579      0.110679  ...      -0.258778      0.084377   \n",
       "S2            -0.068138      0.111010  ...      -0.267662      0.083385   \n",
       "S3            -0.040037      0.116289  ...      -0.268019      0.086924   \n",
       "S4            -0.065932      0.110034  ...      -0.277133      0.084082   \n",
       "S5            -0.052333      0.103177  ...      -0.276880      0.075112   \n",
       "S6            -0.071800      0.109332  ...      -0.267240      0.074591   \n",
       "S7            -0.047726      0.108989  ...      -0.267627      0.082630   \n",
       "S8            -0.024594      0.117400  ...      -0.225617      0.127090   \n",
       "S9            -0.028060      0.125581  ...      -0.214845      0.119808   \n",
       "S10            0.006925      0.123143  ...      -0.212660      0.106769   \n",
       "S11           -0.000066      0.104781  ...      -0.250114      0.092108   \n",
       "S12            0.010339      0.102811  ...      -0.221493      0.100933   \n",
       "S13           -0.030887      0.106504  ...      -0.228213      0.089517   \n",
       "S14           -0.015164      0.101140  ...      -0.253459      0.074301   \n",
       "S15           -0.044091      0.104471  ...      -0.249199      0.088803   \n",
       "S16           -0.010911      0.104153  ...      -0.209434      0.081880   \n",
       "S17           -0.021189      0.113644  ...      -0.230018      0.077240   \n",
       "\n",
       "          ndvi_Mean_5km  ndvi_Std_5km  ndwi_Mean_5km  ndwi_Std_5km  \\\n",
       "Stations                                                             \n",
       "S1             0.326918      0.166530      -0.315127      0.139971   \n",
       "S2             0.336557      0.167914      -0.325971      0.140518   \n",
       "S3             0.304957      0.186484      -0.299539      0.160875   \n",
       "S4             0.347748      0.183410      -0.331451      0.160435   \n",
       "S5             0.333129      0.178004      -0.319351      0.150169   \n",
       "S6             0.349287      0.171751      -0.327956      0.144789   \n",
       "S7             0.321981      0.174021      -0.307016      0.147871   \n",
       "S8             0.249634      0.183916      -0.245642      0.164066   \n",
       "S9             0.241047      0.187538      -0.237883      0.165049   \n",
       "S10            0.193279      0.184460      -0.202531      0.161284   \n",
       "S11            0.242637      0.182637      -0.245637      0.155674   \n",
       "S12            0.204421      0.168909      -0.208612      0.151135   \n",
       "S13            0.266211      0.164723      -0.253965      0.145395   \n",
       "S14            0.272403      0.150928      -0.264641      0.127650   \n",
       "S15            0.304223      0.163522      -0.286714      0.144222   \n",
       "S16            0.219738      0.155096      -0.217052      0.133469   \n",
       "S17            0.253114      0.170487      -0.245913      0.143858   \n",
       "\n",
       "          savi_Mean_5km  savi_Std_5km  ui_Mean_5km  ui_Std_5km  \n",
       "Stations                                                        \n",
       "S1             0.268463      0.138414    -0.196683    0.144285  \n",
       "S2             0.278609      0.137878    -0.197708    0.148793  \n",
       "S3             0.256057      0.152663    -0.151110    0.159738  \n",
       "S4             0.293582      0.149312    -0.196330    0.147066  \n",
       "S5             0.279081      0.146610    -0.181140    0.139715  \n",
       "S6             0.290144      0.145079    -0.206604    0.145446  \n",
       "S7             0.268282      0.143934    -0.167008    0.147687  \n",
       "S8             0.205987      0.152365    -0.123514    0.158304  \n",
       "S9             0.198980      0.156682    -0.127595    0.169854  \n",
       "S10            0.159810      0.151603    -0.070946    0.168767  \n",
       "S11            0.200581      0.150852    -0.087870    0.153718  \n",
       "S12            0.168406      0.138426    -0.063331    0.142096  \n",
       "S13            0.216405      0.134624    -0.135850    0.147933  \n",
       "S14            0.220338      0.124060    -0.121335    0.138313  \n",
       "S15            0.245536      0.130982    -0.165660    0.141873  \n",
       "S16            0.177903      0.127599    -0.091081    0.148213  \n",
       "S17            0.206823      0.142244    -0.111999    0.163657  \n",
       "\n",
       "[17 rows x 22 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 4: Extract Raster-Derived Environmental Indices (5km radius) ---\n",
    "UNIFORM_BUFFER_RADIUS_METERS = 5000\n",
    "\n",
    "raster_features_results = []\n",
    "\n",
    "# Load all specified raster indices\n",
    "loaded_indices = {}\n",
    "for index_file in RASTER_INDICES_TO_EXTRACT:\n",
    "    try:\n",
    "        index_path = os.path.join(CAL_INDICES_DIR, index_file)\n",
    "        \n",
    "        # Use rasterio's built-in reprojection\n",
    "        with rasterio.open(index_path) as src:\n",
    "            # Reproject to target CRS if needed\n",
    "            if src.crs != reference_crs:\n",
    "                transform, width, height = rasterio.warp.calculate_default_transform(\n",
    "                    src.crs, reference_crs, src.width, src.height, *src.bounds\n",
    "                )\n",
    "                array = np.empty((height, width))\n",
    "                rasterio.warp.reproject(\n",
    "                    src.read(1),\n",
    "                    array,\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=reference_crs,\n",
    "                    resampling=rasterio.warp.Resampling.bilinear\n",
    "                )\n",
    "                profile = src.profile.copy()\n",
    "                profile.update({\n",
    "                    'crs': reference_crs,\n",
    "                    'transform': transform,\n",
    "                    'width': width,\n",
    "                    'height': height\n",
    "                })\n",
    "            else:\n",
    "                array = src.read(1)\n",
    "                profile = src.profile\n",
    "            \n",
    "        loaded_indices[index_file] = {'array': array, 'profile': profile}\n",
    "        print(f\"Successfully loaded {index_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {index_file}: {str(e)}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "# Make sure sampling_points_gdf has a 'Stations' column\n",
    "if 'Stations' not in sampling_points_gdf.columns:\n",
    "    sampling_points_gdf['Stations'] = [f'S{i+1}' for i in range(len(sampling_points_gdf))]\n",
    "\n",
    "for station_id, point_row in sampling_points_gdf.iterrows():\n",
    "    point_geom = point_row.geometry\n",
    "    station_name = point_row['Stations']  # Get station name from the row\n",
    "    \n",
    "    point_results = {'Stations': station_name}  # Use the station name from the DataFrame\n",
    "\n",
    "    for index_file, data in loaded_indices.items():\n",
    "        index_name = os.path.splitext(index_file)[0]  # Get base name without extension\n",
    "        \n",
    "        try:\n",
    "            # Extract mean and std values\n",
    "            mean_val = extract_neighborhood_stats(\n",
    "                data['array'], \n",
    "                data['profile'], \n",
    "                point_geom, \n",
    "                UNIFORM_BUFFER_RADIUS_METERS, \n",
    "                'mean'\n",
    "            )\n",
    "            std_val = extract_neighborhood_stats(\n",
    "                data['array'], \n",
    "                data['profile'], \n",
    "                point_geom, \n",
    "                UNIFORM_BUFFER_RADIUS_METERS, \n",
    "                'std'\n",
    "            )\n",
    "            \n",
    "            point_results[f'{index_name}_Mean_5km'] = mean_val\n",
    "            point_results[f'{index_name}_Std_5km'] = std_val\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {index_name} features for station {station_name}: {str(e)}\")\n",
    "            point_results[f'{index_name}_Mean_5km'] = np.nan\n",
    "            point_results[f'{index_name}_Std_5km'] = np.nan\n",
    "            \n",
    "    raster_features_results.append(point_results)\n",
    "\n",
    "# Create DataFrame and set 'Stations' as index\n",
    "raster_features_df = pd.DataFrame(raster_features_results)\n",
    "if not raster_features_df.empty:\n",
    "    raster_features_df.set_index('Stations', inplace=True)\n",
    "    raster_features_df.to_csv(OUTPUT_RASTER_FEATURES_CSV)\n",
    "    print(f\"Raster-Derived Features calculated and saved to {OUTPUT_RASTER_FEATURES_CSV}\")\n",
    "    print(\"\\nFirst 5 rows of Raster-Derived Features:\")\n",
    "    print(raster_features_df.head())\n",
    "else:\n",
    "    print(\"Warning: No raster features were extracted. Output file not created.\")\n",
    "raster_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d995fb-1c3e-47c8-ae34-d8564408566a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e26ff71-7877-448b-8197-3c47657fed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Consolidating All Features and Creating Multi-Modal Inputs ---\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Consolidate All Features, Scale Data, and Create Multi-Modal Inputs ---\n",
    "print(\"\\n--- Step 4: Consolidating All Features and Creating Multi-Modal Inputs ---\")\n",
    "\n",
    "# Step 4.1: Consolidate all dataframes\n",
    "combined_features_df = heavy_metal_df.copy()\n",
    "combined_features_df = combined_features_df.merge(lulc_variations_df_calculated, left_index=True, right_index=True, how='left')\n",
    "combined_features_df = combined_features_df.merge(hydro_properties_df_calculated, left_index=True, right_index=True, how='left')\n",
    "combined_features_df = combined_features_df.merge(raster_features_df, left_index=True, right_index=True, how='left')\n",
    "combined_features_df.to_csv(OUTPUT_COMBINED_FEATURES_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982937e-cc28-493c-85c7-dbf58cd00025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Combined features shape: {combined_features_df.shape}\")\n",
    "print(\"\\nFirst 5 rows of Combined Features:\")\n",
    "print(combined_features_df)\n",
    "\n",
    "# Step 4.2: Prepare Inputs for each model type\n",
    "# Split data into features (X) and target (y)\n",
    "X_df = combined_features_df.drop(columns=['AsR']) # Example: Dropping heavy metal columns\n",
    "y_df = combined_features_df['AsR']\n",
    "print(f\"Features (X) shape: {X_df.shape}, Targets (y) shape: {y_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1a956c9-4e24-45c5-8da2-3a3a55c6e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Input (Scaled Tabular Features) shape: (17, 36)\n"
     ]
    }
   ],
   "source": [
    "# MLP Input: Scaled Tabular Data\n",
    "# Drop non-numerical columns for scaling\n",
    "X_tabular_numeric = X_df.select_dtypes(include=np.number)\n",
    "scaler = StandardScaler()\n",
    "X_mlp_input = scaler.fit_transform(X_tabular_numeric)\n",
    "print(f\"MLP Input (Scaled Tabular Features) shape: {X_mlp_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e081ac9b-776a-4568-8aeb-33d9a05c3271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Input (Stacked Raster Patches) shape: (17, 33, 33, 18)\n"
     ]
    }
   ],
   "source": [
    "def extract_raster_patch(raster_array, raster_profile, point_geom, patch_size):\n",
    "    \"\"\"\n",
    "    Extracts a square image patch around a point for CNN input.\n",
    "    Handles boundary conditions by padding the raster.\n",
    "    \"\"\"\n",
    "    # Convert point_geom to pixel coordinates\n",
    "    transform = raster_profile['transform']\n",
    "    col, row = ~transform * (point_geom.x, point_geom.y)  # Correct coordinate transformation\n",
    "    col, row = int(round(col)), int(round(row))\n",
    "    \n",
    "    half_patch = patch_size // 2\n",
    "    \n",
    "    # Get array dimensions\n",
    "    height, width = raster_array.shape\n",
    "    \n",
    "    # Calculate bounds with padding if needed\n",
    "    row_start = max(0, row - half_patch)\n",
    "    row_end = min(height, row + half_patch + 1)\n",
    "    col_start = max(0, col - half_patch)\n",
    "    col_end = min(width, col + half_patch + 1)\n",
    "    \n",
    "    # Extract the patch\n",
    "    patch = raster_array[row_start:row_end, col_start:col_end]\n",
    "    \n",
    "    # Pad if necessary (when near edges)\n",
    "    pad_width = (\n",
    "        (max(0, half_patch - row), max(0, row + half_patch + 1 - height)),\n",
    "        (max(0, half_patch - col), max(0, col + half_patch + 1 - width))\n",
    "    )\n",
    "    \n",
    "    if any(p > 0 for p in sum(pad_width, ())):\n",
    "        patch = np.pad(patch, pad_width, mode='constant', constant_values=0)\n",
    "    \n",
    "    return patch\n",
    "\n",
    "# CNN Input: Stacked Raster Patches\n",
    "X_cnn_input = {}\n",
    "for station_id, point_row in sampling_points_gdf.iterrows():\n",
    "    point_geom = point_row.geometry\n",
    "    patches = []\n",
    "    \n",
    "    # Create a patch for each raster (DEM, LULC2022, all indices)\n",
    "    for year in LULC_YEARS:\n",
    "        try:\n",
    "            lulc_array = lulc_rasters[year]\n",
    "            lulc_profile = lulc_profiles[year]\n",
    "            patch = extract_raster_patch(lulc_array, lulc_profile, point_geom, CNN_PATCH_SIZE)\n",
    "            patches.append(patch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting LULC {year} patch for station {station_id}: {str(e)}\")\n",
    "            patches.append(np.zeros((CNN_PATCH_SIZE, CNN_PATCH_SIZE)))\n",
    "    \n",
    "    try:\n",
    "        dem_patch = extract_raster_patch(dem_array, dem_profile, point_geom, CNN_PATCH_SIZE)\n",
    "        patches.append(dem_patch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting DEM patch for station {station_id}: {str(e)}\")\n",
    "        patches.append(np.zeros((CNN_PATCH_SIZE, CNN_PATCH_SIZE)))\n",
    "    \n",
    "    for index_file, data in loaded_indices.items():\n",
    "        try:\n",
    "            index_patch = extract_raster_patch(data['array'], data['profile'], point_geom, CNN_PATCH_SIZE)\n",
    "            patches.append(index_patch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {index_file} patch for station {station_id}: {str(e)}\")\n",
    "            patches.append(np.zeros((CNN_PATCH_SIZE, CNN_PATCH_SIZE)))\n",
    "    \n",
    "    # Stack the patches to create a multi-channel image input\n",
    "    try:\n",
    "        stacked_patches = np.stack(patches, axis=-1)\n",
    "        # Handle NaN values if any patches failed to extract correctly\n",
    "        stacked_patches = np.nan_to_num(stacked_patches, nan=0.0)\n",
    "        X_cnn_input[station_id] = stacked_patches\n",
    "    except Exception as e:\n",
    "        print(f\"Error stacking patches for station {station_id}: {str(e)}\")\n",
    "        X_cnn_input[station_id] = np.zeros((CNN_PATCH_SIZE, CNN_PATCH_SIZE, len(patches)))\n",
    "\n",
    "# Convert to numpy array\n",
    "X_cnn_input_list = np.array(list(X_cnn_input.values()))\n",
    "print(f\"CNN Input (Stacked Raster Patches) shape: {X_cnn_input_list.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "262ae2f7-13f9-40a9-946d-7f77ee996dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN Graph created with 17 nodes and 2 edges.\n"
     ]
    }
   ],
   "source": [
    "# GNN Input: Graph Structure and Node Features\n",
    "# Create a proximity graph based on GNN_EDGE_DISTANCE_THRESHOLD_METERS\n",
    "G = nx.Graph()\n",
    "station_points = {station: point.coords[0] for station, point in sampling_points_gdf['geometry'].items()}\n",
    "\n",
    "# Add nodes with their features\n",
    "for station_id, features in X_tabular_numeric.iterrows():\n",
    "    G.add_node(station_id, features=features.to_dict())\n",
    "\n",
    "# Add edges based on proximity\n",
    "station_ids = list(station_points.keys())\n",
    "distances = cdist(list(station_points.values()), list(station_points.values()))\n",
    "for i in range(len(station_ids)):\n",
    "    for j in range(i + 1, len(station_ids)):\n",
    "        if distances[i, j] <= GNN_EDGE_DISTANCE_THRESHOLD_METERS:\n",
    "            G.add_edge(station_ids[i], station_ids[j], weight=distances[i, j])\n",
    "\n",
    "print(f\"GNN Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371fa49-09dc-437c-87c0-12147f1fec8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3262696d-9a21-486b-b3a9-cfbcb588d80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Stations\n",
       " S1      7.96\n",
       " S2      9.88\n",
       " S3     15.48\n",
       " S4     18.77\n",
       " S5     20.96\n",
       " S6     10.42\n",
       " S7     19.88\n",
       " S8     11.36\n",
       " S9     10.78\n",
       " S10    18.90\n",
       " S11     9.88\n",
       " S12     7.98\n",
       " S13     6.67\n",
       " S14     6.90\n",
       " S15    15.71\n",
       " S16    13.92\n",
       " S17    15.84\n",
       " Name: AsR, dtype: float64,\n",
       "                  River        Lat       Long    CrR    NiR     CuR    CdR  \\\n",
       " Stations                                                                    \n",
       " S1         Dhaleshwari  23.910260  90.229845  92.69  19.18   40.34   2.66   \n",
       " S2         Dhaleshwari  23.858227  90.240038  88.40  17.21   41.56   2.97   \n",
       " S3         Dhaleshwari  23.802571  90.245390  66.92  37.52   49.47   2.10   \n",
       " S4         Dhaleshwari  23.754298  90.246581  55.56  26.08   69.77   1.79   \n",
       " S5         Dhaleshwari  23.702157  90.277077  64.50  30.62   73.19   1.45   \n",
       " S6         Dhaleshwari  23.657826  90.317763  60.35  11.72   98.28   0.82   \n",
       " S7         Dhaleshwari  23.628000  90.388647  69.37  14.95   96.72   0.97   \n",
       " S8               Turag  23.877666  90.350573  15.70  40.76   73.87   2.17   \n",
       " S9               Turag  23.826986  90.342201  12.74  39.43   87.20   3.60   \n",
       " S10              Turag  23.792572  90.339601  11.13  47.01   80.78   2.79   \n",
       " S11          Buriganga  23.716059  90.359274  74.47  49.94  141.40   5.56   \n",
       " S12          Buriganga  23.664709  90.451701  86.82  55.88  147.90   7.94   \n",
       " S13               Balu  23.764005  90.482538  10.37  29.73   56.47   1.96   \n",
       " S14               Balu  23.808202  90.484506   9.44  21.94   53.71   1.15   \n",
       " S15       Shitalakshya  23.832710  90.548478  51.37  16.78   71.89   8.70   \n",
       " S16       Shitalakshya  23.715818  90.504423  40.75  46.14   85.56  10.16   \n",
       " S17       Shitalakshya  23.628420  90.514128  44.82  50.63   91.33   9.42   \n",
       " \n",
       "              PbR    FeR  variation_2017-2018  ...  ndsi_Mean_5km  \\\n",
       " Stations                                      ...                  \n",
       " S1         50.73  26700            14.396884  ...      -0.258778   \n",
       " S2         38.90  34970            12.920722  ...      -0.267662   \n",
       " S3         32.79  23970            11.966286  ...      -0.268019   \n",
       " S4         43.40  23990            21.385412  ...      -0.277133   \n",
       " S5         53.55  35130            18.108141  ...      -0.276880   \n",
       " S6         37.88  38700            14.453125  ...      -0.267240   \n",
       " S7         33.47  43480            16.136396  ...      -0.267627   \n",
       " S8         98.10  45600            23.558909  ...      -0.225617   \n",
       " S9        170.40  47900            25.581283  ...      -0.214845   \n",
       " S10       108.60  50395            18.297140  ...      -0.212660   \n",
       " S11       155.40  42650             6.657204  ...      -0.250114   \n",
       " S12       177.90  46870             9.600091  ...      -0.221493   \n",
       " S13        35.98  32700            22.545809  ...      -0.228213   \n",
       " S14        31.48  26320            30.934235  ...      -0.253459   \n",
       " S15        76.52  30790            28.319101  ...      -0.249199   \n",
       " S16        98.47  36500            12.688490  ...      -0.209434   \n",
       " S17        95.44  25110             9.336868  ...      -0.230018   \n",
       " \n",
       "           ndsi_Std_5km  ndvi_Mean_5km  ndvi_Std_5km  ndwi_Mean_5km  \\\n",
       " Stations                                                             \n",
       " S1            0.084377       0.326918      0.166530      -0.315127   \n",
       " S2            0.083385       0.336557      0.167914      -0.325971   \n",
       " S3            0.086924       0.304957      0.186484      -0.299539   \n",
       " S4            0.084082       0.347748      0.183410      -0.331451   \n",
       " S5            0.075112       0.333129      0.178004      -0.319351   \n",
       " S6            0.074591       0.349287      0.171751      -0.327956   \n",
       " S7            0.082630       0.321981      0.174021      -0.307016   \n",
       " S8            0.127090       0.249634      0.183916      -0.245642   \n",
       " S9            0.119808       0.241047      0.187538      -0.237883   \n",
       " S10           0.106769       0.193279      0.184460      -0.202531   \n",
       " S11           0.092108       0.242637      0.182637      -0.245637   \n",
       " S12           0.100933       0.204421      0.168909      -0.208612   \n",
       " S13           0.089517       0.266211      0.164723      -0.253965   \n",
       " S14           0.074301       0.272403      0.150928      -0.264641   \n",
       " S15           0.088803       0.304223      0.163522      -0.286714   \n",
       " S16           0.081880       0.219738      0.155096      -0.217052   \n",
       " S17           0.077240       0.253114      0.170487      -0.245913   \n",
       " \n",
       "           ndwi_Std_5km  savi_Mean_5km  savi_Std_5km  ui_Mean_5km  ui_Std_5km  \n",
       " Stations                                                                      \n",
       " S1            0.139971       0.268463      0.138414    -0.196683    0.144285  \n",
       " S2            0.140518       0.278609      0.137878    -0.197708    0.148793  \n",
       " S3            0.160875       0.256057      0.152663    -0.151110    0.159738  \n",
       " S4            0.160435       0.293582      0.149312    -0.196330    0.147066  \n",
       " S5            0.150169       0.279081      0.146610    -0.181140    0.139715  \n",
       " S6            0.144789       0.290144      0.145079    -0.206604    0.145446  \n",
       " S7            0.147871       0.268282      0.143934    -0.167008    0.147687  \n",
       " S8            0.164066       0.205987      0.152365    -0.123514    0.158304  \n",
       " S9            0.165049       0.198980      0.156682    -0.127595    0.169854  \n",
       " S10           0.161284       0.159810      0.151603    -0.070946    0.168767  \n",
       " S11           0.155674       0.200581      0.150852    -0.087870    0.153718  \n",
       " S12           0.151135       0.168406      0.138426    -0.063331    0.142096  \n",
       " S13           0.145395       0.216405      0.134624    -0.135850    0.147933  \n",
       " S14           0.127650       0.220338      0.124060    -0.121335    0.138313  \n",
       " S15           0.144222       0.245536      0.130982    -0.165660    0.141873  \n",
       " S16           0.133469       0.177903      0.127599    -0.091081    0.148213  \n",
       " S17           0.143858       0.206823      0.142244    -0.111999    0.163657  \n",
       " \n",
       " [17 rows x 43 columns])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df, X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd337064-f4f5-4a05-a2c8-6d2706b2746f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CrR</th>\n",
       "      <th>NiR</th>\n",
       "      <th>CuR</th>\n",
       "      <th>CdR</th>\n",
       "      <th>PbR</th>\n",
       "      <th>FeR</th>\n",
       "      <th>variation_2017-2018</th>\n",
       "      <th>variation_2018-2019</th>\n",
       "      <th>variation_2019-2020</th>\n",
       "      <th>variation_2020-2021</th>\n",
       "      <th>...</th>\n",
       "      <th>ndsi_Mean_5km</th>\n",
       "      <th>ndsi_Std_5km</th>\n",
       "      <th>ndvi_Mean_5km</th>\n",
       "      <th>ndvi_Std_5km</th>\n",
       "      <th>ndwi_Mean_5km</th>\n",
       "      <th>ndwi_Std_5km</th>\n",
       "      <th>savi_Mean_5km</th>\n",
       "      <th>savi_Std_5km</th>\n",
       "      <th>ui_Mean_5km</th>\n",
       "      <th>ui_Std_5km</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stations</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S1</th>\n",
       "      <td>92.69</td>\n",
       "      <td>19.18</td>\n",
       "      <td>40.34</td>\n",
       "      <td>2.66</td>\n",
       "      <td>50.73</td>\n",
       "      <td>26700</td>\n",
       "      <td>14.396884</td>\n",
       "      <td>8.830319</td>\n",
       "      <td>11.964628</td>\n",
       "      <td>10.162627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258778</td>\n",
       "      <td>0.084377</td>\n",
       "      <td>0.326918</td>\n",
       "      <td>0.166530</td>\n",
       "      <td>-0.315127</td>\n",
       "      <td>0.139971</td>\n",
       "      <td>0.268463</td>\n",
       "      <td>0.138414</td>\n",
       "      <td>-0.196683</td>\n",
       "      <td>0.144285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>88.40</td>\n",
       "      <td>17.21</td>\n",
       "      <td>41.56</td>\n",
       "      <td>2.97</td>\n",
       "      <td>38.90</td>\n",
       "      <td>34970</td>\n",
       "      <td>12.920722</td>\n",
       "      <td>9.676226</td>\n",
       "      <td>13.489252</td>\n",
       "      <td>10.422661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267662</td>\n",
       "      <td>0.083385</td>\n",
       "      <td>0.336557</td>\n",
       "      <td>0.167914</td>\n",
       "      <td>-0.325971</td>\n",
       "      <td>0.140518</td>\n",
       "      <td>0.278609</td>\n",
       "      <td>0.137878</td>\n",
       "      <td>-0.197708</td>\n",
       "      <td>0.148793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S3</th>\n",
       "      <td>66.92</td>\n",
       "      <td>37.52</td>\n",
       "      <td>49.47</td>\n",
       "      <td>2.10</td>\n",
       "      <td>32.79</td>\n",
       "      <td>23970</td>\n",
       "      <td>11.966286</td>\n",
       "      <td>10.483110</td>\n",
       "      <td>15.459976</td>\n",
       "      <td>10.687158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268019</td>\n",
       "      <td>0.086924</td>\n",
       "      <td>0.304957</td>\n",
       "      <td>0.186484</td>\n",
       "      <td>-0.299539</td>\n",
       "      <td>0.160875</td>\n",
       "      <td>0.256057</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>-0.151110</td>\n",
       "      <td>0.159738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S4</th>\n",
       "      <td>55.56</td>\n",
       "      <td>26.08</td>\n",
       "      <td>69.77</td>\n",
       "      <td>1.79</td>\n",
       "      <td>43.40</td>\n",
       "      <td>23990</td>\n",
       "      <td>21.385412</td>\n",
       "      <td>15.447605</td>\n",
       "      <td>20.766508</td>\n",
       "      <td>13.685776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277133</td>\n",
       "      <td>0.084082</td>\n",
       "      <td>0.347748</td>\n",
       "      <td>0.183410</td>\n",
       "      <td>-0.331451</td>\n",
       "      <td>0.160435</td>\n",
       "      <td>0.293582</td>\n",
       "      <td>0.149312</td>\n",
       "      <td>-0.196330</td>\n",
       "      <td>0.147066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S5</th>\n",
       "      <td>64.50</td>\n",
       "      <td>30.62</td>\n",
       "      <td>73.19</td>\n",
       "      <td>1.45</td>\n",
       "      <td>53.55</td>\n",
       "      <td>35130</td>\n",
       "      <td>18.108141</td>\n",
       "      <td>14.249969</td>\n",
       "      <td>17.320514</td>\n",
       "      <td>12.567463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276880</td>\n",
       "      <td>0.075112</td>\n",
       "      <td>0.333129</td>\n",
       "      <td>0.178004</td>\n",
       "      <td>-0.319351</td>\n",
       "      <td>0.150169</td>\n",
       "      <td>0.279081</td>\n",
       "      <td>0.146610</td>\n",
       "      <td>-0.181140</td>\n",
       "      <td>0.139715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CrR    NiR    CuR   CdR    PbR    FeR  variation_2017-2018  \\\n",
       "Stations                                                                 \n",
       "S1        92.69  19.18  40.34  2.66  50.73  26700            14.396884   \n",
       "S2        88.40  17.21  41.56  2.97  38.90  34970            12.920722   \n",
       "S3        66.92  37.52  49.47  2.10  32.79  23970            11.966286   \n",
       "S4        55.56  26.08  69.77  1.79  43.40  23990            21.385412   \n",
       "S5        64.50  30.62  73.19  1.45  53.55  35130            18.108141   \n",
       "\n",
       "          variation_2018-2019  variation_2019-2020  variation_2020-2021  ...  \\\n",
       "Stations                                                                 ...   \n",
       "S1                   8.830319            11.964628            10.162627  ...   \n",
       "S2                   9.676226            13.489252            10.422661  ...   \n",
       "S3                  10.483110            15.459976            10.687158  ...   \n",
       "S4                  15.447605            20.766508            13.685776  ...   \n",
       "S5                  14.249969            17.320514            12.567463  ...   \n",
       "\n",
       "          ndsi_Mean_5km  ndsi_Std_5km  ndvi_Mean_5km  ndvi_Std_5km  \\\n",
       "Stations                                                             \n",
       "S1            -0.258778      0.084377       0.326918      0.166530   \n",
       "S2            -0.267662      0.083385       0.336557      0.167914   \n",
       "S3            -0.268019      0.086924       0.304957      0.186484   \n",
       "S4            -0.277133      0.084082       0.347748      0.183410   \n",
       "S5            -0.276880      0.075112       0.333129      0.178004   \n",
       "\n",
       "          ndwi_Mean_5km  ndwi_Std_5km  savi_Mean_5km  savi_Std_5km  \\\n",
       "Stations                                                             \n",
       "S1            -0.315127      0.139971       0.268463      0.138414   \n",
       "S2            -0.325971      0.140518       0.278609      0.137878   \n",
       "S3            -0.299539      0.160875       0.256057      0.152663   \n",
       "S4            -0.331451      0.160435       0.293582      0.149312   \n",
       "S5            -0.319351      0.150169       0.279081      0.146610   \n",
       "\n",
       "          ui_Mean_5km  ui_Std_5km  \n",
       "Stations                           \n",
       "S1          -0.196683    0.144285  \n",
       "S2          -0.197708    0.148793  \n",
       "S3          -0.151110    0.159738  \n",
       "S4          -0.196330    0.147066  \n",
       "S5          -0.181140    0.139715  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = X_df.drop(columns=[\"River\", \"Lat\", \"Long\"])\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04361a8c-175c-49f2-9118-348b3d304477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training and Evaluating: Random Forest ---\n",
      "R-squared: -0.0642\n",
      "Mean Squared Error: 740.9398\n",
      "\n",
      "--- Training and Evaluating: Hybrid RF + PMF ---\n",
      "R-squared: -0.1579\n",
      "Mean Squared Error: 806.1476\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# --- Placeholder Tabular Data (REPLACE THIS WITH YOUR ACTUAL DATA) ---\n",
    "# This data is for demonstration and should be replaced with your loaded X_df and y_df.\n",
    "# Example: 100 samples and 20 features\n",
    "X_df = pd.DataFrame(np.random.rand(100, 20), columns=[f'feature_{i}' for i in range(20)])\n",
    "y_df = pd.DataFrame(np.random.rand(100, 1) * 100, columns=['target'])\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# CRITICAL FIX: Ensure X_df and y_df have the same number of samples\n",
    "if len(X_df) != len(y_df):\n",
    "    raise ValueError(\"X_df and y_df must have the same number of rows.\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the training and test features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Modeling with Tabular Data ---\n",
    "\n",
    "# 1. Classical Machine Learning Model (Random Forest)\n",
    "print(\"--- Training and Evaluating: Random Forest ---\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# The .ravel() call is correct for scikit-learn's target variable format\n",
    "rf_model.fit(X_train_scaled, y_train.values.ravel())\n",
    "rf_predictions = rf_model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"R-squared: {r2_score(y_test, rf_predictions):.4f}\")\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, rf_predictions):.4f}\")\n",
    "\n",
    "# 2. Hybrid ML + PMF Model (Conceptual)\n",
    "print(\"\\n--- Training and Evaluating: Hybrid RF + PMF ---\")\n",
    "# Replace this placeholder `pmf_factors` with your actual PMF results.\n",
    "pmf_factors = np.random.rand(len(X_df), 3) # Assume 3 PMF factors as new features\n",
    "\n",
    "# CRITICAL FIX: Ensure the number of rows in pmf_factors matches X_df\n",
    "if len(X_df) != len(pmf_factors):\n",
    "    raise ValueError(\"X_df and pmf_factors must have the same number of rows.\")\n",
    "\n",
    "X_hybrid = np.hstack([X_df.values, pmf_factors])\n",
    "\n",
    "X_hybrid_train, X_hybrid_test, _, _ = train_test_split(X_hybrid, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_hybrid = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_hybrid.fit(X_hybrid_train, y_train.values.ravel())\n",
    "rf_hybrid_predictions = rf_hybrid.predict(X_hybrid_test)\n",
    "\n",
    "print(f\"R-squared: {r2_score(y_test, rf_hybrid_predictions):.4f}\")\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, rf_hybrid_predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123d39e-3c75-4990-a6fc-984b0165eb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af26ce-fa7b-42a9-882d-deb1afb7eda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea113202-9f16-4e18-ac41-ee98273fd18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a0280-b89a-424e-b0c5-8dc326e3dca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b12a3c-b1d4-4fcb-bde7-35681a5b0e56",
   "metadata": {},
   "source": [
    "## CNN-GNN-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c92621c2-66bb-4273-803b-6f50144301c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.transform import Affine\n",
    "from shapely.geometry import Point\n",
    "from glob import glob\n",
    "\n",
    "# --- 1. Data Loading and Preparation Functions ---\n",
    "\n",
    "def load_combined_features(file_path):\n",
    "    \"\"\"\n",
    "    Loads the main tabular data from the Combined_Features_Rainy.csv file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Please ensure the path is correct.\")\n",
    "        return None\n",
    "\n",
    "def extract_raster_patch(raster_array, raster_profile, point_geom, patch_size):\n",
    "    \"\"\"\n",
    "    Extracts a square image patch around a point for CNN input.\n",
    "    Handles boundary conditions by padding the raster.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get pixel coordinates from georeferenced coordinates\n",
    "        col, row = ~raster_profile['transform'] * (point_geom.x, point_geom.y)\n",
    "        col, row = int(col), int(row)\n",
    "        \n",
    "        half_patch = patch_size // 2\n",
    "        start_row, end_row = row - half_patch, row + half_patch\n",
    "        start_col, end_col = col - half_patch, col + half_patch\n",
    "\n",
    "        # Pad the array to handle points near the edge\n",
    "        padded_array = np.pad(\n",
    "            raster_array, \n",
    "            half_patch, \n",
    "            mode='constant', \n",
    "            constant_values=raster_profile.get('nodata', 0.0)\n",
    "        )\n",
    "\n",
    "        padded_start_row = start_row + half_patch\n",
    "        padded_end_row = end_row + half_patch\n",
    "        padded_start_col = start_col + half_patch\n",
    "        padded_end_col = end_col + half_patch\n",
    "\n",
    "        patch = padded_array[padded_start_row:padded_end_row, padded_start_col:padded_end_col]\n",
    "        \n",
    "        return patch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting patch: {e}\")\n",
    "        return np.zeros((patch_size, patch_size))\n",
    "\n",
    "\n",
    "def prepare_cnn_input(df, raster_base_path, patch_size=32):\n",
    "    \"\"\"\n",
    "    Loads all rasters and creates a stacked, multi-channel input for the CNN.\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing CNN input...\")\n",
    "    \n",
    "    # Get all .tif files from the specified directories\n",
    "    indices_files = sorted(glob(os.path.join(raster_base_path, 'CalIndices', '*.tif')))\n",
    "    lulc_files = sorted(glob(os.path.join(raster_base_path, 'LULCMerged', '*.tif')))\n",
    "    \n",
    "    all_raster_files = indices_files + lulc_files\n",
    "    \n",
    "    if not all_raster_files:\n",
    "        raise FileNotFoundError(\"No .tif files found in Raster/CalIndices or Raster/LULCMerged.\")\n",
    "\n",
    "    # Load all raster data and profiles into memory\n",
    "    loaded_rasters = {}\n",
    "    for file_path in all_raster_files:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            loaded_rasters[file_path] = {\n",
    "                'array': src.read(1),\n",
    "                'profile': src.profile\n",
    "            }\n",
    "    \n",
    "    # Check for consistency\n",
    "    if not loaded_rasters:\n",
    "        raise ValueError(\"No rasters could be loaded.\")\n",
    "    \n",
    "    # Extract patches for each station and stack them\n",
    "    cnn_input_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        point_geom = Point(row['Long'], row['Lat'])\n",
    "        patches = []\n",
    "        for file_path, data in loaded_rasters.items():\n",
    "            patches.append(extract_raster_patch(data['array'], data['profile'], point_geom, patch_size))\n",
    "\n",
    "        stacked_patches = np.stack(patches, axis=-1)\n",
    "        stacked_patches[np.isnan(stacked_patches)] = 0.0 # Handle NaNs\n",
    "        cnn_input_dict[index] = stacked_patches\n",
    "    \n",
    "    cnn_input_array = np.array(list(cnn_input_dict.values()))\n",
    "    print(f\"CNN Input shape: {cnn_input_array.shape}\")\n",
    "    return cnn_input_array\n",
    "\n",
    "def prepare_gnn_input(df, node_features, distance_threshold=5000):\n",
    "    \"\"\"\n",
    "    Prepares the graph data (adjacency matrix and node features) for the GNN.\n",
    "    The graph is created based on Euclidean distance between stations.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'Lat' and 'Long' for spatial graph construction.\n",
    "        node_features (np.ndarray): The tabular data to be used as node features.\n",
    "        distance_threshold (int): The maximum distance (in meters) to form an edge.\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing GNN input...\")\n",
    "    num_samples = len(df)\n",
    "    \n",
    "    # Use the provided features as node features\n",
    "    # These are the scaled hydrological and indices columns as requested\n",
    "    \n",
    "    # Create a distance-based adjacency matrix using the sampling locations\n",
    "    coords = df[['Long', 'Lat']].values\n",
    "    dist_matrix = np.sqrt(np.sum((coords[:, np.newaxis] - coords[np.newaxis, :])**2, axis=2))\n",
    "    \n",
    "    # Create adjacency matrix based on a distance threshold (e.g., 5km)\n",
    "    # This forms a \"strong\" graph by connecting nearby sampling sites.\n",
    "    adjacency_matrix = (dist_matrix < distance_threshold).astype(int)\n",
    "    np.fill_diagonal(adjacency_matrix, 0)\n",
    "    \n",
    "    # Convert to sparse tensor for efficient GNN processing\n",
    "    adjacency_matrix_sparse = tf.sparse.from_dense(adjacency_matrix)\n",
    "    print(\"GNN graph data prepared.\")\n",
    "    return node_features, adjacency_matrix_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a46f6ad-2cd7-4326-b8e1-35dd9a4acfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Custom GNN Layer ---\n",
    "# A simplified graph convolutional layer for neighborhood aggregation.\n",
    "class CustomGraphConv(layers.Layer):\n",
    "    def __init__(self, units, activation='relu', **kwargs):\n",
    "        super(CustomGraphConv, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"kernel\"\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"bias\"\n",
    "        )\n",
    "        super(CustomGraphConv, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        features, adj_matrix = inputs\n",
    "        aggregated_features = tf.sparse.sparse_dense_matmul(adj_matrix, features)\n",
    "        output = tf.matmul(aggregated_features, self.kernel) + self.bias\n",
    "        return self.activation(output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomGraphConv, self).get_config()\n",
    "        config.update({\n",
    "            \"units\": self.units,\n",
    "            \"activation\": self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd654c4a-d9d6-491d-8ebb-5216793f926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Combined_Features_Rainy.csv\n",
      "\n",
      "Preparing CNN input...\n",
      "CNN Input shape: (17, 0, 0, 17)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Main Script Execution ---\n",
    "\n",
    "# --- Data Paths and Parameters ---\n",
    "csv_path = 'Combined_Features_Rainy.csv'\n",
    "raster_base_path = 'Raster'\n",
    "target_column = 'AsR'\n",
    "patch_size = 32\n",
    "gnn_distance_threshold = 4200  # meters\n",
    "\n",
    "# Load and prepare data\n",
    "df = load_combined_features(csv_path)\n",
    "if df is None:\n",
    "    exit()\n",
    "\n",
    "# Define features and target\n",
    "y_target = df[target_column].values.reshape(-1, 1)\n",
    "\n",
    "# MLP features: all columns except metadata and targets\n",
    "mlp_feature_cols = [col for col in df.columns if col not in ['Stations', 'River', 'Lat', 'Long', 'CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'FeR']]\n",
    "X_mlp_df = df[mlp_feature_cols]\n",
    "\n",
    "# GNN features: hydrological and indices columns, as requested\n",
    "# These are a subset of the MLP features\n",
    "gnn_feature_cols = [\n",
    "    'variation_2017-2018', 'variation_2018-2019', 'variation_2019-2020', 'variation_2020-2021', 'variation_2021-2022',\n",
    "    'dem_point_value', 'dem_mean_5km', 'dem_std_5km', 'dist_to_nearest_BF', 'dist_to_nearest_IND',\n",
    "    'num_within_5km_BF', 'num_within_5km_IND',\n",
    "    'awei_Mean_5km', 'awei_Std_5km', 'bui_Mean_5km', 'bui_Std_5km', 'evi_Mean_5km', 'evi_Std_5km',\n",
    "    'mndwi_Mean_5km', 'mndwi_Std_5km', 'ndbi_Mean_5km', 'ndbi_Std_5km', 'ndbsi_Mean_5km', 'ndbsi_Std_5km',\n",
    "    'ndsi_Mean_5km', 'ndsi_Std_5km', 'ndvi_Mean_5km', 'ndvi_Std_5km', 'ndwi_Mean_5km', 'ndwi_Std_5km',\n",
    "    'savi_Mean_5km', 'savi_Std_5km', 'ui_Mean_5km', 'ui_Std_5km'\n",
    "]\n",
    "X_gnn_df = df[gnn_feature_cols]\n",
    "\n",
    "# Scale MLP and GNN features\n",
    "mlp_scaler = StandardScaler()\n",
    "X_mlp_input = mlp_scaler.fit_transform(X_mlp_df)\n",
    "\n",
    "gnn_scaler = StandardScaler()\n",
    "X_gnn_node_features = gnn_scaler.fit_transform(X_gnn_df)\n",
    "\n",
    "X_cnn_input = prepare_cnn_input(df, raster_base_path, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dd28ada-dfca-4eb5-8bc7-1527409b2874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing GNN input...\n",
      "GNN graph data prepared.\n"
     ]
    }
   ],
   "source": [
    "# Prepare GNN input using the specific features and location data\n",
    "X_gnn_node_features, adjacency_matrix_sparse = prepare_gnn_input(\n",
    "    df,\n",
    "    node_features=X_gnn_node_features,\n",
    "    distance_threshold=gnn_distance_threshold\n",
    ")\n",
    "\n",
    "# Split all data into training and testing sets\n",
    "X_mlp_train, X_mlp_test, y_train, y_test = train_test_split(\n",
    "    X_mlp_input, y_target, test_size=0.2, random_state=42\n",
    ")\n",
    "X_cnn_train, X_cnn_test, _, _ = train_test_split(\n",
    "    X_cnn_input, y_target, test_size=0.2, random_state=42\n",
    ")\n",
    "# GNN node features for training and testing\n",
    "X_gnn_train_nodes, X_gnn_test_nodes, _, _ = train_test_split(\n",
    "    X_gnn_node_features, y_target, test_size=0.2, random_state=42\n",
    ")\n",
    "# The adjacency matrix is the same for both train and test sets, as it's static\n",
    "X_gnn_train_adj = adjacency_matrix_sparse\n",
    "X_gnn_test_adj = adjacency_matrix_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cc56c9c-c832-4b6d-87a4-9eec59e7680d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Computed output size would be negative. Received `inputs shape=(None, 0, 0, 17)`, `kernel shape=(3, 3, 17, 32)`, `dilation_rate=[1 1]`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m mlp_branch_output \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp_encoder_output\u001b[39m\u001b[38;5;124m'\u001b[39m)(mlp_branch)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# CNN Branch\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m cnn_branch \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(cnn_input)\n\u001b[1;32m     16\u001b[0m cnn_branch \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))(cnn_branch)\n\u001b[1;32m     17\u001b[0m cnn_branch \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(cnn_branch)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/ops/operation_utils.py:221\u001b[0m, in \u001b[0;36mcompute_conv_output_shape\u001b[0;34m(input_shape, filters, kernel_size, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(output_spatial_shape)):\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m none_dims \u001b[38;5;129;01mand\u001b[39;00m output_spatial_shape[i] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    222\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputed output size would be negative. Received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`inputs shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`kernel shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dilation_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdilation_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m             )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m padding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m padding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    228\u001b[0m     output_spatial_shape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor((spatial_shape \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m strides) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Computed output size would be negative. Received `inputs shape=(None, 0, 0, 17)`, `kernel shape=(3, 3, 17, 32)`, `dilation_rate=[1 1]`."
     ]
    }
   ],
   "source": [
    "# --- 4. Building the Multi-Fused Model ---\n",
    "\n",
    "# Define input layers\n",
    "mlp_input = layers.Input(shape=(X_mlp_train.shape[1],), name='mlp_input')\n",
    "cnn_input = layers.Input(shape=(X_cnn_train.shape[1], X_cnn_train.shape[2], X_cnn_train.shape[3]), name='cnn_input')\n",
    "gnn_input_nodes = layers.Input(shape=(X_gnn_train_nodes.shape[1],), name='gnn_input_nodes')\n",
    "gnn_input_adj = layers.Input(shape=(X_gnn_train_nodes.shape[0],), name='gnn_input_adj', sparse=True)\n",
    "\n",
    "# MLP Branch\n",
    "mlp_branch = layers.Dense(64, activation='relu')(mlp_input)\n",
    "mlp_branch = layers.Dropout(0.3)(mlp_branch)\n",
    "mlp_branch_output = layers.Dense(32, activation='relu', name='mlp_encoder_output')(mlp_branch)\n",
    "\n",
    "# CNN Branch\n",
    "cnn_branch = layers.Conv2D(32, (3, 3), activation='relu')(cnn_input)\n",
    "cnn_branch = layers.MaxPooling2D((2, 2))(cnn_branch)\n",
    "cnn_branch = layers.Conv2D(64, (3, 3), activation='relu')(cnn_branch)\n",
    "cnn_branch = layers.MaxPooling2D((2, 2))(cnn_branch)\n",
    "cnn_branch = layers.Flatten()(cnn_branch)\n",
    "cnn_branch_output = layers.Dense(32, activation='relu', name='cnn_encoder_output')(cnn_branch)\n",
    "\n",
    "# GNN Branch\n",
    "gnn_branch = CustomGraphConv(32, name='gnn_encoder_output')([gnn_input_nodes, gnn_input_adj])\n",
    "train_node_indices = np.array(range(len(X_mlp_train)))\n",
    "gnn_branch_output = layers.Lambda(lambda x: tf.gather(x, train_node_indices), name='gnn_output_train')(gnn_branch)\n",
    "\n",
    "# Fusion Layer\n",
    "combined_features = layers.concatenate([mlp_branch_output, cnn_branch_output, gnn_branch_output])\n",
    "fusion_layer = layers.Dense(64, activation='relu')(combined_features)\n",
    "fusion_layer = layers.Dropout(0.3)(fusion_layer)\n",
    "output_layer = layers.Dense(1, activation='linear', name='final_prediction')(fusion_layer)\n",
    "\n",
    "# Define the full model\n",
    "model = Model(\n",
    "    inputs=[mlp_input, cnn_input, gnn_input_nodes, gnn_input_adj],\n",
    "    outputs=output_layer,\n",
    "    name='CNN_GNN_MLP_Model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "817b2407-fc26-4645-bc13-0eaed08b005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Combined_Features_Rainy.csv\n",
      "\n",
      "Preparing CNN input...\n",
      "CNN Input shape: (17, 32, 32, 17)\n",
      "\n",
      "Preparing GNN input...\n",
      "GNN graph data prepared.\n",
      "\n",
      "--- Running GNN Encoder on the full graph ---\n",
      "GNN full graph output shape: (17, 32)\n",
      "\n",
      "--- Fusion Model Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Fusion_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Fusion_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,928</span> │ cnn_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,240</span> │ mlp_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_encoder_output  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_encoder_output  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,760</span> │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_output_for_bat… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mlp_encoder_outp… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ cnn_encoder_outp… │\n",
       "│                     │                   │            │ gnn_output_for_b… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_prediction    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ cnn_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m17\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m,    │      \u001b[38;5;34m4,928\u001b[0m │ cnn_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_4[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m2,240\u001b[0m │ mlp_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_5[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mlp_encoder_output  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_encoder_output  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m73,760\u001b[0m │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gnn_output_for_bat… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ mlp_encoder_outp… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ cnn_encoder_outp… │\n",
       "│                     │                   │            │ gnn_output_for_b… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m6,208\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_prediction    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,777</span> (421.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m107,777\u001b[0m (421.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,777</span> (421.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m107,777\u001b[0m (421.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "--- Training the Fused Model ---\n",
      "Epoch 1/50\n",
      "1/1 - 1s - 1s/step - loss: 224.6854 - mae: 14.1572\n",
      "Epoch 2/50\n",
      "1/1 - 0s - 23ms/step - loss: 222.0718 - mae: 14.0579\n",
      "Epoch 3/50\n",
      "1/1 - 0s - 21ms/step - loss: 215.6759 - mae: 13.8507\n",
      "Epoch 4/50\n",
      "1/1 - 0s - 27ms/step - loss: 210.0634 - mae: 13.6386\n",
      "Epoch 5/50\n",
      "1/1 - 0s - 22ms/step - loss: 206.7099 - mae: 13.4269\n",
      "Epoch 6/50\n",
      "1/1 - 0s - 23ms/step - loss: 201.9163 - mae: 13.3596\n",
      "Epoch 7/50\n",
      "1/1 - 0s - 22ms/step - loss: 207.4121 - mae: 13.4599\n",
      "Epoch 8/50\n",
      "1/1 - 0s - 24ms/step - loss: 197.1828 - mae: 13.0887\n",
      "Epoch 9/50\n",
      "1/1 - 0s - 22ms/step - loss: 196.4373 - mae: 13.1219\n",
      "Epoch 10/50\n",
      "1/1 - 0s - 23ms/step - loss: 195.1902 - mae: 13.0914\n",
      "Epoch 11/50\n",
      "1/1 - 0s - 22ms/step - loss: 193.9972 - mae: 12.9646\n",
      "Epoch 12/50\n",
      "1/1 - 0s - 24ms/step - loss: 186.4460 - mae: 12.7409\n",
      "Epoch 13/50\n",
      "1/1 - 0s - 22ms/step - loss: 182.1155 - mae: 12.5187\n",
      "Epoch 14/50\n",
      "1/1 - 0s - 29ms/step - loss: 176.5131 - mae: 12.2794\n",
      "Epoch 15/50\n",
      "1/1 - 0s - 23ms/step - loss: 179.6440 - mae: 12.3716\n",
      "Epoch 16/50\n",
      "1/1 - 0s - 23ms/step - loss: 169.7253 - mae: 12.0209\n",
      "Epoch 17/50\n",
      "1/1 - 0s - 23ms/step - loss: 173.9694 - mae: 12.1110\n",
      "Epoch 18/50\n",
      "1/1 - 0s - 23ms/step - loss: 170.3869 - mae: 12.0003\n",
      "Epoch 19/50\n",
      "1/1 - 0s - 23ms/step - loss: 155.5414 - mae: 11.3779\n",
      "Epoch 20/50\n",
      "1/1 - 0s - 23ms/step - loss: 164.8914 - mae: 11.7799\n",
      "Epoch 21/50\n",
      "1/1 - 0s - 22ms/step - loss: 157.7964 - mae: 11.4536\n",
      "Epoch 22/50\n",
      "1/1 - 0s - 22ms/step - loss: 148.8192 - mae: 11.0411\n",
      "Epoch 23/50\n",
      "1/1 - 0s - 21ms/step - loss: 152.4608 - mae: 11.2013\n",
      "Epoch 24/50\n",
      "1/1 - 0s - 26ms/step - loss: 144.7150 - mae: 10.8359\n",
      "Epoch 25/50\n",
      "1/1 - 0s - 23ms/step - loss: 140.6974 - mae: 10.5808\n",
      "Epoch 26/50\n",
      "1/1 - 0s - 23ms/step - loss: 137.7052 - mae: 10.4700\n",
      "Epoch 27/50\n",
      "1/1 - 0s - 22ms/step - loss: 135.8277 - mae: 10.5440\n",
      "Epoch 28/50\n",
      "1/1 - 0s - 23ms/step - loss: 123.0314 - mae: 9.7545\n",
      "Epoch 29/50\n",
      "1/1 - 0s - 21ms/step - loss: 126.8271 - mae: 10.1453\n",
      "Epoch 30/50\n",
      "1/1 - 0s - 23ms/step - loss: 125.7008 - mae: 9.7557\n",
      "Epoch 31/50\n",
      "1/1 - 0s - 22ms/step - loss: 114.3749 - mae: 9.1717\n",
      "Epoch 32/50\n",
      "1/1 - 0s - 23ms/step - loss: 128.1624 - mae: 9.9567\n",
      "Epoch 33/50\n",
      "1/1 - 0s - 22ms/step - loss: 105.4668 - mae: 9.1043\n",
      "Epoch 34/50\n",
      "1/1 - 0s - 23ms/step - loss: 101.5548 - mae: 8.6244\n",
      "Epoch 35/50\n",
      "1/1 - 0s - 24ms/step - loss: 103.8375 - mae: 8.5673\n",
      "Epoch 36/50\n",
      "1/1 - 0s - 23ms/step - loss: 86.7531 - mae: 7.7542\n",
      "Epoch 37/50\n",
      "1/1 - 0s - 24ms/step - loss: 98.2388 - mae: 8.5603\n",
      "Epoch 38/50\n",
      "1/1 - 0s - 24ms/step - loss: 86.5462 - mae: 7.7913\n",
      "Epoch 39/50\n",
      "1/1 - 0s - 21ms/step - loss: 81.2009 - mae: 7.4517\n",
      "Epoch 40/50\n",
      "1/1 - 0s - 23ms/step - loss: 76.9943 - mae: 7.1276\n",
      "Epoch 41/50\n",
      "1/1 - 0s - 23ms/step - loss: 74.2304 - mae: 6.8715\n",
      "Epoch 42/50\n",
      "1/1 - 0s - 22ms/step - loss: 64.8828 - mae: 6.2967\n",
      "Epoch 43/50\n",
      "1/1 - 0s - 23ms/step - loss: 46.7770 - mae: 5.3529\n",
      "Epoch 44/50\n",
      "1/1 - 0s - 23ms/step - loss: 61.2073 - mae: 6.3773\n",
      "Epoch 45/50\n",
      "1/1 - 0s - 29ms/step - loss: 53.2295 - mae: 6.1403\n",
      "Epoch 46/50\n",
      "1/1 - 0s - 23ms/step - loss: 54.2606 - mae: 5.6102\n",
      "Epoch 47/50\n",
      "1/1 - 0s - 22ms/step - loss: 45.3046 - mae: 5.5993\n",
      "Epoch 48/50\n",
      "1/1 - 0s - 23ms/step - loss: 44.9345 - mae: 5.6779\n",
      "Epoch 49/50\n",
      "1/1 - 0s - 22ms/step - loss: 29.9231 - mae: 4.7565\n",
      "Epoch 50/50\n",
      "1/1 - 0s - 22ms/step - loss: 40.6683 - mae: 5.6702\n",
      "\n",
      "--- Evaluating the Model on Test Data ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "R-squared: -0.9541\n",
      "Mean Squared Error: 9.0526\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.transform import Affine\n",
    "from shapely.geometry import Point\n",
    "from glob import glob\n",
    "from rasterio.windows import Window\n",
    "import logging\n",
    "\n",
    "# Set up logging to avoid spamming the console with warnings\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Suppress specific rasterio warnings\n",
    "logging.getLogger('rasterio._base').setLevel(logging.ERROR)\n",
    "\n",
    "# --- 1. Data Loading and Preparation Functions ---\n",
    "\n",
    "def load_combined_features(file_path):\n",
    "    \"\"\"\n",
    "    Loads the main tabular data from the Combined_Features_Rainy.csv file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Please ensure the path is correct.\")\n",
    "        return None\n",
    "\n",
    "def extract_raster_patch(raster_src, point_geom, patch_size):\n",
    "    \"\"\"\n",
    "    Extracts a square image patch around a point for CNN input using rasterio.windows.\n",
    "    Handles boundary conditions by filling with a constant value.\n",
    "    \"\"\"\n",
    "    # Initialize an empty patch with a constant value (e.g., 0)\n",
    "    patch = np.full((patch_size, patch_size), raster_src.nodata or 0, dtype=raster_src.dtypes[0])\n",
    "\n",
    "    # Get pixel coordinates from georeferenced coordinates\n",
    "    try:\n",
    "        col, row = raster_src.index(point_geom.x, point_geom.y)\n",
    "    except Exception:\n",
    "        # If the point is outside the raster extent, return the zero-filled patch\n",
    "        return patch\n",
    "\n",
    "    half_patch = patch_size // 2\n",
    "    \n",
    "    # Define the window to read\n",
    "    window_read = Window(col - half_patch, row - half_patch, patch_size, patch_size)\n",
    "    \n",
    "    try:\n",
    "        # Read the window, filling any parts outside the raster with nodata value\n",
    "        data = raster_src.read(1, window=window_read, boundless=True, fill_value=raster_src.nodata)\n",
    "        \n",
    "        # Check if the read data has the expected shape and resize if necessary\n",
    "        if data.shape == (patch_size, patch_size):\n",
    "            patch = data\n",
    "        else:\n",
    "            # If the shape is incorrect (e.g., at the very edge), we resize it.\n",
    "            new_patch = np.full((patch_size, patch_size), raster_src.nodata or 0, dtype=raster_src.dtypes[0])\n",
    "            rows, cols = data.shape\n",
    "            new_patch[:rows, :cols] = data\n",
    "            patch = new_patch\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading patch for point {point_geom}. Returning zero-filled patch. Error: {e}\")\n",
    "        return patch\n",
    "    \n",
    "    return patch\n",
    "\n",
    "def prepare_cnn_input(df, raster_base_path, patch_size=32):\n",
    "    \"\"\"\n",
    "    Loads all rasters and creates a stacked, multi-channel input for the CNN.\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing CNN input...\")\n",
    "    \n",
    "    # Get all .tif files from the specified directories\n",
    "    indices_files = sorted(glob(os.path.join(raster_base_path, 'CalIndices', '*.tif')))\n",
    "    lulc_files = sorted(glob(os.path.join(raster_base_path, 'LULCMerged', '*.tif')))\n",
    "    \n",
    "    all_raster_files = indices_files + lulc_files\n",
    "    \n",
    "    if not all_raster_files:\n",
    "        raise FileNotFoundError(\"No .tif files found in Raster/CalIndices or Raster/LULCMerged.\")\n",
    "\n",
    "    # Extract patches for each station and stack them\n",
    "    cnn_input_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        point_geom = Point(row['Long'], row['Lat'])\n",
    "        patches = []\n",
    "        for file_path in all_raster_files:\n",
    "            try:\n",
    "                with rasterio.open(file_path) as src:\n",
    "                    patch = extract_raster_patch(src, point_geom, patch_size)\n",
    "                    patches.append(patch)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}. Skipping. Error: {e}\")\n",
    "                patches.append(np.zeros((patch_size, patch_size)))\n",
    "\n",
    "        if patches:\n",
    "            stacked_patches = np.stack(patches, axis=-1)\n",
    "            stacked_patches[np.isnan(stacked_patches)] = 0.0 # Handle NaNs\n",
    "            cnn_input_list.append(stacked_patches)\n",
    "    \n",
    "    if not cnn_input_list:\n",
    "        raise ValueError(\"No valid CNN input patches could be created.\")\n",
    "    \n",
    "    cnn_input_array = np.array(cnn_input_list)\n",
    "    print(f\"CNN Input shape: {cnn_input_array.shape}\")\n",
    "    return cnn_input_array\n",
    "\n",
    "def prepare_gnn_input(df, node_features, distance_threshold=5000):\n",
    "    \"\"\"\n",
    "    Prepares the graph data (adjacency matrix and node features) for the GNN.\n",
    "    The graph is created based on Euclidean distance between stations.\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing GNN input...\")\n",
    "    num_samples = len(df)\n",
    "    \n",
    "    # Create a distance-based adjacency matrix using the sampling locations\n",
    "    coords = df[['Long', 'Lat']].values\n",
    "    # We use a simple Euclidean distance as an approximation for this example\n",
    "    dist_matrix = np.sqrt(np.sum((coords[:, np.newaxis] - coords[np.newaxis, :])**2, axis=2))\n",
    "    \n",
    "    # Create adjacency matrix based on a distance threshold (e.g., 5km)\n",
    "    adjacency_matrix = (dist_matrix < distance_threshold).astype(np.float32) # Cast to float32\n",
    "    np.fill_diagonal(adjacency_matrix, 0)\n",
    "    \n",
    "    # Convert to sparse tensor for efficient GNN processing\n",
    "    adjacency_matrix_sparse = tf.sparse.from_dense(adjacency_matrix)\n",
    "    print(\"GNN graph data prepared.\")\n",
    "    return node_features, adjacency_matrix_sparse\n",
    "\n",
    "# --- 2. Custom GNN Layer ---\n",
    "# A simplified graph convolutional layer for neighborhood aggregation.\n",
    "class CustomGraphConv(layers.Layer):\n",
    "    def __init__(self, units, activation='relu', **kwargs):\n",
    "        super(CustomGraphConv, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape is a list of shapes for the inputs\n",
    "        # input_shape[0] is the shape of the node features (None, num_features)\n",
    "        num_features = input_shape[0][-1] \n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(num_features, self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"kernel\"\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"bias\"\n",
    "        )\n",
    "        super(CustomGraphConv, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs are now expected to be concrete tensors (not KerasTensors) with no batch dimension\n",
    "        features, adj_matrix = inputs\n",
    "        # The tf.sparse.sparse_dense_matmul expects rank 2 tensors\n",
    "        aggregated_features = tf.sparse.sparse_dense_matmul(adj_matrix, features)\n",
    "        output = tf.matmul(aggregated_features, self.kernel) + self.bias\n",
    "        return self.activation(output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomGraphConv, self).get_config()\n",
    "        config.update({\n",
    "            \"units\": self.units,\n",
    "            \"activation\": tf.keras.activations.serialize(self.activation),\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# --- 3. Main Script Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Data Paths and Parameters ---\n",
    "    csv_path = 'Combined_Features_Rainy.csv'\n",
    "    raster_base_path = 'Raster'\n",
    "    target_column = 'AsR'\n",
    "    patch_size = 32\n",
    "    gnn_distance_threshold = 5000  # meters\n",
    "\n",
    "    # Load and prepare data\n",
    "    df = load_combined_features(csv_path)\n",
    "    if df is None:\n",
    "        exit()\n",
    "\n",
    "    num_nodes = len(df) # Total number of nodes in the graph\n",
    "    \n",
    "    # Define features and target\n",
    "    y_target = df[target_column].values.reshape(-1, 1)\n",
    "    \n",
    "    # MLP features: all columns except metadata and targets\n",
    "    mlp_feature_cols = [col for col in df.columns if col not in ['Stations', 'River', 'Lat', 'Long', 'CrR', 'NiR', 'CuR', 'AsR', 'CdR', 'PbR', 'FeR']]\n",
    "    X_mlp_df = df[mlp_feature_cols]\n",
    "    \n",
    "    # GNN features: hydrological and indices columns\n",
    "    gnn_feature_cols = [\n",
    "        'variation_2017-2018', 'variation_2018-2019', 'variation_2019-2020', 'variation_2020-2021', 'variation_2021-2022',\n",
    "        'dem_point_value', 'dem_mean_5km', 'dem_std_5km', 'dist_to_nearest_BF', 'dist_to_nearest_IND',\n",
    "        'num_within_5km_BF', 'num_within_5km_IND',\n",
    "        'awei_Mean_5km', 'awei_Std_5km', 'bui_Mean_5km', 'bui_Std_5km', 'evi_Mean_5km', 'evi_Std_5km',\n",
    "        'mndwi_Mean_5km', 'mndwi_Std_5km', 'ndbi_Mean_5km', 'ndbi_Std_5km', 'ndbsi_Mean_5km', 'ndbsi_Std_5km',\n",
    "        'ndsi_Mean_5km', 'ndsi_Std_5km', 'ndvi_Mean_5km', 'ndvi_Std_5km', 'ndwi_Mean_5km', 'ndwi_Std_5km',\n",
    "        'savi_Mean_5km', 'savi_Std_5km', 'ui_Mean_5km', 'ui_Std_5km'\n",
    "    ]\n",
    "    X_gnn_df = df[gnn_feature_cols]\n",
    "\n",
    "    # Scale MLP and GNN features\n",
    "    mlp_scaler = StandardScaler()\n",
    "    X_mlp_input = mlp_scaler.fit_transform(X_mlp_df)\n",
    "    \n",
    "    gnn_scaler = StandardScaler()\n",
    "    X_gnn_node_features = gnn_scaler.fit_transform(X_gnn_df)\n",
    "    \n",
    "    # Corrected CNN input preparation\n",
    "    X_cnn_input = prepare_cnn_input(df, raster_base_path, patch_size)\n",
    "    \n",
    "    # Prepare GNN input using the specific features and location data\n",
    "    X_gnn_node_features_tensor, adjacency_matrix_sparse_tensor = prepare_gnn_input(\n",
    "        df,\n",
    "        node_features=X_gnn_node_features,\n",
    "        distance_threshold=gnn_distance_threshold\n",
    "    )\n",
    "    \n",
    "    # --- GNN Encoding Step ---\n",
    "    # This step is performed once on the full, non-batched graph data.\n",
    "    print(\"\\n--- Running GNN Encoder on the full graph ---\")\n",
    "    gnn_encoder_layer = CustomGraphConv(32)\n",
    "    # The `build` method needs to be called manually since we are not using a Model.\n",
    "    gnn_encoder_layer.build([X_gnn_node_features_tensor.shape, adjacency_matrix_sparse_tensor.shape])\n",
    "    gnn_outputs_full_graph = gnn_encoder_layer.call([tf.constant(X_gnn_node_features_tensor, dtype=tf.float32), adjacency_matrix_sparse_tensor])\n",
    "    print(f\"GNN full graph output shape: {gnn_outputs_full_graph.shape}\")\n",
    "\n",
    "    # --- Data Splitting and Slicing ---\n",
    "    # Split all data into training and testing sets.\n",
    "    # Note: We split the indices and use them to slice the data.\n",
    "    all_indices = np.arange(num_nodes)\n",
    "    train_indices, test_indices, _, _ = train_test_split(\n",
    "        all_indices, y_target, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    X_mlp_train = X_mlp_input[train_indices]\n",
    "    X_mlp_test = X_mlp_input[test_indices]\n",
    "    X_cnn_train = X_cnn_input[train_indices]\n",
    "    X_cnn_test = X_cnn_input[test_indices]\n",
    "    y_train = y_target[train_indices]\n",
    "    y_test = y_target[test_indices]\n",
    "\n",
    "    # Slice the pre-computed GNN outputs for the training and test sets\n",
    "    gnn_train_output = gnn_outputs_full_graph.numpy()[train_indices]\n",
    "    gnn_test_output = gnn_outputs_full_graph.numpy()[test_indices]\n",
    "    \n",
    "    # --- 4. Building the Main Fusion Model ---\n",
    "    \n",
    "    # This model takes batched inputs for MLP and CNN, and the sliced GNN output.\n",
    "    mlp_input = layers.Input(shape=(X_mlp_input.shape[1],), name='mlp_input')\n",
    "    cnn_input = layers.Input(shape=(X_cnn_input.shape[1], X_cnn_input.shape[2], X_cnn_input.shape[3]), name='cnn_input')\n",
    "    gnn_output_for_batch = layers.Input(shape=(32,), name='gnn_output_for_batch') # 32 is the GNN units\n",
    "\n",
    "    # MLP Branch\n",
    "    mlp_branch = layers.Dense(64, activation='relu')(mlp_input)\n",
    "    mlp_branch = layers.Dropout(0.3)(mlp_branch)\n",
    "    mlp_branch_output = layers.Dense(32, activation='relu', name='mlp_encoder_output')(mlp_branch)\n",
    "    \n",
    "    # CNN Branch\n",
    "    cnn_branch = layers.Conv2D(32, (3, 3), activation='relu')(cnn_input)\n",
    "    cnn_branch = layers.MaxPooling2D((2, 2))(cnn_branch)\n",
    "    cnn_branch = layers.Conv2D(64, (3, 3), activation='relu')(cnn_branch)\n",
    "    cnn_branch = layers.MaxPooling2D((2, 2))(cnn_branch)\n",
    "    cnn_branch = layers.Flatten()(cnn_branch)\n",
    "    cnn_branch_output = layers.Dense(32, activation='relu', name='cnn_encoder_output')(cnn_branch)\n",
    "\n",
    "    # Fusion Layer\n",
    "    combined_features = layers.concatenate([mlp_branch_output, cnn_branch_output, gnn_output_for_batch])\n",
    "    fusion_layer = layers.Dense(64, activation='relu')(combined_features)\n",
    "    fusion_layer = layers.Dropout(0.3)(fusion_layer)\n",
    "    output_layer = layers.Dense(1, activation='linear', name='final_prediction')(fusion_layer)\n",
    "    \n",
    "    fusion_model = Model(\n",
    "        inputs=[mlp_input, cnn_input, gnn_output_for_batch],\n",
    "        outputs=output_layer,\n",
    "        name='Fusion_Model'\n",
    "    )\n",
    "\n",
    "    # --- 5. Compile and Train the Model ---\n",
    "    fusion_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    print(\"\\n--- Fusion Model Summary ---\")\n",
    "    print(fusion_model.summary())\n",
    "\n",
    "    # Now we train the fusion model with the correctly batched inputs\n",
    "    train_inputs = [X_mlp_train, X_cnn_train, gnn_train_output]\n",
    "    \n",
    "    print(\"\\n--- Training the Fused Model ---\")\n",
    "    history = fusion_model.fit(\n",
    "        train_inputs,\n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # --- 6. Evaluation ---\n",
    "    print(\"\\n--- Evaluating the Model on Test Data ---\")\n",
    "    test_inputs = [X_mlp_test, X_cnn_test, gnn_test_output]\n",
    "    y_pred = fusion_model.predict(test_inputs)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "211f2f4b-293b-4788-a24f-f54997dfa9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training the Fused Model ---\n",
      "Epoch 1/100\n",
      "1/1 - 0s - 38ms/step - loss: 22.1592 - mae: 4.0710\n",
      "Epoch 2/100\n",
      "1/1 - 0s - 27ms/step - loss: 31.1077 - mae: 4.6858\n",
      "Epoch 3/100\n",
      "1/1 - 0s - 25ms/step - loss: 21.1776 - mae: 4.0621\n",
      "Epoch 4/100\n",
      "1/1 - 0s - 25ms/step - loss: 32.8325 - mae: 5.0469\n",
      "Epoch 5/100\n",
      "1/1 - 0s - 24ms/step - loss: 30.1702 - mae: 4.8398\n",
      "Epoch 6/100\n",
      "1/1 - 0s - 23ms/step - loss: 23.8372 - mae: 4.2356\n",
      "Epoch 7/100\n",
      "1/1 - 0s - 24ms/step - loss: 26.8431 - mae: 4.4767\n",
      "Epoch 8/100\n",
      "1/1 - 0s - 25ms/step - loss: 11.2028 - mae: 2.8276\n",
      "Epoch 9/100\n",
      "1/1 - 0s - 24ms/step - loss: 19.8303 - mae: 3.5380\n",
      "Epoch 10/100\n",
      "1/1 - 0s - 25ms/step - loss: 15.0831 - mae: 3.1395\n",
      "Epoch 11/100\n",
      "1/1 - 0s - 23ms/step - loss: 5.3190 - mae: 1.8403\n",
      "Epoch 12/100\n",
      "1/1 - 0s - 25ms/step - loss: 18.2786 - mae: 3.5139\n",
      "Epoch 13/100\n",
      "1/1 - 0s - 64ms/step - loss: 11.0948 - mae: 2.7391\n",
      "Epoch 14/100\n",
      "1/1 - 0s - 23ms/step - loss: 16.7505 - mae: 3.5256\n",
      "Epoch 15/100\n",
      "1/1 - 0s - 23ms/step - loss: 19.5893 - mae: 4.0080\n",
      "Epoch 16/100\n",
      "1/1 - 0s - 24ms/step - loss: 10.8204 - mae: 2.8022\n",
      "Epoch 17/100\n",
      "1/1 - 0s - 24ms/step - loss: 28.5178 - mae: 3.7679\n",
      "Epoch 18/100\n",
      "1/1 - 0s - 23ms/step - loss: 22.3875 - mae: 4.1600\n",
      "Epoch 19/100\n",
      "1/1 - 0s - 28ms/step - loss: 10.3833 - mae: 2.4632\n",
      "Epoch 20/100\n",
      "1/1 - 0s - 25ms/step - loss: 13.9114 - mae: 2.8732\n",
      "Epoch 21/100\n",
      "1/1 - 0s - 26ms/step - loss: 11.7519 - mae: 2.9256\n",
      "Epoch 22/100\n",
      "1/1 - 0s - 26ms/step - loss: 12.4653 - mae: 2.7620\n",
      "Epoch 23/100\n",
      "1/1 - 0s - 24ms/step - loss: 16.5307 - mae: 3.2281\n",
      "Epoch 24/100\n",
      "1/1 - 0s - 25ms/step - loss: 11.4567 - mae: 2.6365\n",
      "Epoch 25/100\n",
      "1/1 - 0s - 24ms/step - loss: 10.7608 - mae: 2.8733\n",
      "Epoch 26/100\n",
      "1/1 - 0s - 24ms/step - loss: 9.3358 - mae: 2.2041\n",
      "Epoch 27/100\n",
      "1/1 - 0s - 23ms/step - loss: 8.6151 - mae: 2.7953\n",
      "Epoch 28/100\n",
      "1/1 - 0s - 23ms/step - loss: 10.7822 - mae: 2.7361\n",
      "Epoch 29/100\n",
      "1/1 - 0s - 22ms/step - loss: 12.0532 - mae: 3.0421\n",
      "Epoch 30/100\n",
      "1/1 - 0s - 24ms/step - loss: 16.5116 - mae: 3.4488\n",
      "Epoch 31/100\n",
      "1/1 - 0s - 23ms/step - loss: 5.8348 - mae: 1.8941\n",
      "Epoch 32/100\n",
      "1/1 - 0s - 23ms/step - loss: 16.4727 - mae: 3.2936\n",
      "Epoch 33/100\n",
      "1/1 - 0s - 22ms/step - loss: 8.9192 - mae: 2.3085\n",
      "Epoch 34/100\n",
      "1/1 - 0s - 24ms/step - loss: 15.7800 - mae: 3.1761\n",
      "Epoch 35/100\n",
      "1/1 - 0s - 24ms/step - loss: 5.0750 - mae: 1.8932\n",
      "Epoch 36/100\n",
      "1/1 - 0s - 22ms/step - loss: 11.0855 - mae: 2.8533\n",
      "Epoch 37/100\n",
      "1/1 - 0s - 23ms/step - loss: 11.8321 - mae: 2.8860\n",
      "Epoch 38/100\n",
      "1/1 - 0s - 23ms/step - loss: 6.4576 - mae: 2.3537\n",
      "Epoch 39/100\n",
      "1/1 - 0s - 25ms/step - loss: 2.2349 - mae: 1.2966\n",
      "Epoch 40/100\n",
      "1/1 - 0s - 23ms/step - loss: 16.0898 - mae: 2.9438\n",
      "Epoch 41/100\n",
      "1/1 - 0s - 24ms/step - loss: 14.7580 - mae: 2.5284\n",
      "Epoch 42/100\n",
      "1/1 - 0s - 22ms/step - loss: 9.2891 - mae: 2.6606\n",
      "Epoch 43/100\n",
      "1/1 - 0s - 23ms/step - loss: 8.0740 - mae: 2.3623\n",
      "Epoch 44/100\n",
      "1/1 - 0s - 22ms/step - loss: 5.8139 - mae: 2.0882\n",
      "Epoch 45/100\n",
      "1/1 - 0s - 24ms/step - loss: 4.0240 - mae: 1.6516\n",
      "Epoch 46/100\n",
      "1/1 - 0s - 24ms/step - loss: 3.1528 - mae: 1.4954\n",
      "Epoch 47/100\n",
      "1/1 - 0s - 24ms/step - loss: 8.3529 - mae: 2.4454\n",
      "Epoch 48/100\n",
      "1/1 - 0s - 24ms/step - loss: 17.4306 - mae: 3.3369\n",
      "Epoch 49/100\n",
      "1/1 - 0s - 23ms/step - loss: 6.1410 - mae: 2.2223\n",
      "Epoch 50/100\n",
      "1/1 - 0s - 24ms/step - loss: 5.0404 - mae: 1.8383\n",
      "Epoch 51/100\n",
      "1/1 - 0s - 22ms/step - loss: 11.4278 - mae: 2.8536\n",
      "Epoch 52/100\n",
      "1/1 - 0s - 24ms/step - loss: 12.0262 - mae: 2.4769\n",
      "Epoch 53/100\n",
      "1/1 - 0s - 22ms/step - loss: 8.8767 - mae: 2.5726\n",
      "Epoch 54/100\n",
      "1/1 - 0s - 23ms/step - loss: 7.3006 - mae: 2.5103\n",
      "Epoch 55/100\n",
      "1/1 - 0s - 22ms/step - loss: 6.9794 - mae: 2.2252\n",
      "Epoch 56/100\n",
      "1/1 - 0s - 24ms/step - loss: 4.6859 - mae: 1.7138\n",
      "Epoch 57/100\n",
      "1/1 - 0s - 22ms/step - loss: 7.9251 - mae: 2.3094\n",
      "Epoch 58/100\n",
      "1/1 - 0s - 27ms/step - loss: 8.1566 - mae: 2.4592\n",
      "Epoch 59/100\n",
      "1/1 - 0s - 23ms/step - loss: 7.2126 - mae: 2.3356\n",
      "Epoch 60/100\n",
      "1/1 - 0s - 23ms/step - loss: 6.7281 - mae: 2.0522\n",
      "Epoch 61/100\n",
      "1/1 - 0s - 22ms/step - loss: 22.0139 - mae: 3.7711\n",
      "Epoch 62/100\n",
      "1/1 - 0s - 24ms/step - loss: 14.0430 - mae: 2.7239\n",
      "Epoch 63/100\n",
      "1/1 - 0s - 24ms/step - loss: 6.1391 - mae: 1.6814\n",
      "Epoch 64/100\n",
      "1/1 - 0s - 23ms/step - loss: 13.1793 - mae: 2.5952\n",
      "Epoch 65/100\n",
      "1/1 - 0s - 23ms/step - loss: 5.2865 - mae: 1.9772\n",
      "Epoch 66/100\n",
      "1/1 - 0s - 22ms/step - loss: 4.6609 - mae: 1.7841\n",
      "Epoch 67/100\n",
      "1/1 - 0s - 24ms/step - loss: 8.9683 - mae: 2.4322\n",
      "Epoch 68/100\n",
      "1/1 - 0s - 22ms/step - loss: 9.8746 - mae: 2.4730\n",
      "Epoch 69/100\n",
      "1/1 - 0s - 24ms/step - loss: 6.5909 - mae: 2.1327\n",
      "Epoch 70/100\n",
      "1/1 - 0s - 22ms/step - loss: 2.9962 - mae: 1.5532\n",
      "Epoch 71/100\n",
      "1/1 - 0s - 24ms/step - loss: 4.5338 - mae: 1.5192\n",
      "Epoch 72/100\n",
      "1/1 - 0s - 25ms/step - loss: 2.7460 - mae: 1.2928\n",
      "Epoch 73/100\n",
      "1/1 - 0s - 23ms/step - loss: 5.7530 - mae: 1.9677\n",
      "Epoch 74/100\n",
      "1/1 - 0s - 23ms/step - loss: 7.2422 - mae: 2.0333\n",
      "Epoch 75/100\n",
      "1/1 - 0s - 23ms/step - loss: 3.6204 - mae: 1.5619\n",
      "Epoch 76/100\n",
      "1/1 - 0s - 23ms/step - loss: 10.5657 - mae: 2.8188\n",
      "Epoch 77/100\n",
      "1/1 - 0s - 23ms/step - loss: 3.2925 - mae: 1.5311\n",
      "Epoch 78/100\n",
      "1/1 - 0s - 23ms/step - loss: 3.5180 - mae: 1.6161\n",
      "Epoch 79/100\n",
      "1/1 - 0s - 23ms/step - loss: 8.3151 - mae: 2.1737\n",
      "Epoch 80/100\n",
      "1/1 - 0s - 23ms/step - loss: 6.8260 - mae: 2.0344\n",
      "Epoch 81/100\n",
      "1/1 - 0s - 24ms/step - loss: 3.7176 - mae: 1.3987\n",
      "Epoch 82/100\n",
      "1/1 - 0s - 24ms/step - loss: 8.4023 - mae: 2.1378\n",
      "Epoch 83/100\n",
      "1/1 - 0s - 23ms/step - loss: 15.4645 - mae: 2.9584\n",
      "Epoch 84/100\n",
      "1/1 - 0s - 24ms/step - loss: 6.2822 - mae: 1.9857\n",
      "Epoch 85/100\n",
      "1/1 - 0s - 23ms/step - loss: 5.8762 - mae: 1.9609\n",
      "Epoch 86/100\n",
      "1/1 - 0s - 22ms/step - loss: 2.2828 - mae: 1.2348\n",
      "Epoch 87/100\n",
      "1/1 - 0s - 24ms/step - loss: 4.8035 - mae: 1.6678\n",
      "Epoch 88/100\n",
      "1/1 - 0s - 23ms/step - loss: 10.4114 - mae: 2.2765\n",
      "Epoch 89/100\n",
      "1/1 - 0s - 23ms/step - loss: 7.5907 - mae: 2.1648\n",
      "Epoch 90/100\n",
      "1/1 - 0s - 23ms/step - loss: 7.2645 - mae: 2.2251\n",
      "Epoch 91/100\n",
      "1/1 - 0s - 23ms/step - loss: 2.5107 - mae: 1.2866\n",
      "Epoch 92/100\n",
      "1/1 - 0s - 22ms/step - loss: 2.8830 - mae: 1.4385\n",
      "Epoch 93/100\n",
      "1/1 - 0s - 26ms/step - loss: 8.6522 - mae: 2.4312\n",
      "Epoch 94/100\n",
      "1/1 - 0s - 23ms/step - loss: 7.3085 - mae: 2.1647\n",
      "Epoch 95/100\n",
      "1/1 - 0s - 23ms/step - loss: 5.8734 - mae: 1.9253\n",
      "Epoch 96/100\n",
      "1/1 - 0s - 21ms/step - loss: 5.6706 - mae: 1.7261\n",
      "Epoch 97/100\n",
      "1/1 - 0s - 24ms/step - loss: 2.7740 - mae: 1.2577\n",
      "Epoch 98/100\n",
      "1/1 - 0s - 23ms/step - loss: 3.2707 - mae: 1.5738\n",
      "Epoch 99/100\n",
      "1/1 - 0s - 25ms/step - loss: 10.0772 - mae: 2.6047\n",
      "Epoch 100/100\n",
      "1/1 - 0s - 23ms/step - loss: 9.2713 - mae: 2.4783\n",
      "\n",
      "--- Evaluating the Model on Test Data ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "R-squared: -9.5417\n",
      "Mean Squared Error: 48.8364\n"
     ]
    }
   ],
   "source": [
    "# Now we train the fusion model with the correctly batched inputs\n",
    "train_inputs = [X_mlp_train, X_cnn_train, gnn_train_output]\n",
    "\n",
    "print(\"\\n--- Training the Fused Model ---\")\n",
    "history = fusion_model.fit(\n",
    "    train_inputs,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- 6. Evaluation ---\n",
    "print(\"\\n--- Evaluating the Model on Test Data ---\")\n",
    "test_inputs = [X_mlp_test, X_cnn_test, gnn_test_output]\n",
    "y_pred = fusion_model.predict(test_inputs)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2cca0-5d3d-4bda-9068-7ef805d5cbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02696ab-fb0a-44dc-85c8-8ffc463d4e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5b21f-43db-468c-84fd-84d545035070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a412c6e-40db-4fd0-bdd7-506129d8dc2e",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8171b83e-eeb0-4307-8480-dea8e8da3563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 5-fold Cross-Validation ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Fold 1 Evaluation:\n",
      "  R-squared: -0.4008\n",
      "  Mean Squared Error: 53.2645\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Fold 2 Evaluation:\n",
      "  R-squared: -0.0553\n",
      "  Mean Squared Error: 45.9918\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Fold 3 Evaluation:\n",
      "  R-squared: -0.4446\n",
      "  Mean Squared Error: 30.8905\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x44777f1a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x44777f1a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Fold 4 Evaluation:\n",
      "  R-squared: 0.0102\n",
      "  Mean Squared Error: 58.2206\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x447f36980> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x447f36980> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Fold 5 Evaluation:\n",
      "  R-squared: -0.7775\n",
      "  Mean Squared Error: 35.0571\n",
      "\n",
      "--- Cross-Validation Complete ---\n",
      "Average R-squared across 5 folds: -0.3336 +/- 0.2862\n",
      "Average Mean Squared Error across 5 folds: 44.6849 +/- 10.4068\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
    "\n",
    "# It's important to assume the data preparation steps from the user's previous output are complete.\n",
    "# We will create placeholder data that matches the shapes from the user's output.\n",
    "# In a real scenario, this data would come from your pre-processing script.\n",
    "\n",
    "# Placeholder data matching the user's output shapes\n",
    "# Total samples are 17, as per the user's log.\n",
    "num_samples = 17\n",
    "\n",
    "# CNN Input: (17, 32, 32, 17)\n",
    "# This would be the raster data for each of the 17 stations.\n",
    "cnn_input_data = np.random.rand(num_samples, 32, 32, 17)\n",
    "\n",
    "# MLP Input: (17, 34)\n",
    "# This would be the tabular data for each of the 17 stations.\n",
    "mlp_input_data = np.random.rand(num_samples, 34)\n",
    "\n",
    "# GNN Output: (17, 32)\n",
    "# This is the output from the GNN encoder for all 17 stations.\n",
    "gnn_output_data = np.random.rand(num_samples, 32)\n",
    "\n",
    "# Target variable (e.g., heavy metal concentration)\n",
    "# This is the value we are trying to predict.\n",
    "y = np.random.rand(num_samples, 1) * 20 + 5 # Create some random target values for demonstration\n",
    "\n",
    "# Define the model architecture. This is a factory function so we can create a new model\n",
    "# for each fold, preventing data leakage.\n",
    "def create_fusion_model():\n",
    "    \"\"\"\n",
    "    Creates and compiles the CNN-GNN-MLP fusion model.\n",
    "    \"\"\"\n",
    "    # CNN branch\n",
    "    cnn_input = Input(shape=(32, 32, 17), name='cnn_input')\n",
    "    conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(cnn_input)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    flatten = Flatten()(pool2)\n",
    "    cnn_encoder_output = Dense(32, activation='relu', name='cnn_encoder_output')(flatten)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_input = Input(shape=(34,), name='mlp_input')\n",
    "    dense1 = Dense(64, activation='relu')(mlp_input)\n",
    "    dropout1 = Dropout(0.2)(dense1)\n",
    "    mlp_encoder_output = Dense(32, activation='relu', name='mlp_encoder_output')(dropout1)\n",
    "\n",
    "    # GNN output branch (the output from the pre-computed GNN encoder)\n",
    "    gnn_output_for_batch = Input(shape=(32,), name='gnn_output_for_batch')\n",
    "\n",
    "    # Fusion of outputs\n",
    "    combined = Concatenate(axis=-1)([mlp_encoder_output, cnn_encoder_output, gnn_output_for_batch])\n",
    "\n",
    "    # Fusion MLP head\n",
    "    dense2 = Dense(64, activation='relu')(combined)\n",
    "    dropout2 = Dropout(0.2)(dense2)\n",
    "    final_prediction = Dense(1, name='final_prediction')(dropout2)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_output_for_batch], outputs=final_prediction, name='Fusion_Model')\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "n_splits = 5  # Using 5 folds\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the scores from each fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "\n",
    "print(f\"--- Starting {n_splits}-fold Cross-Validation ---\")\n",
    "\n",
    "# The K-Fold loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(cnn_input_data)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    # Split the data for the current fold\n",
    "    X_cnn_train, X_cnn_val = cnn_input_data[train_index], cnn_input_data[val_index]\n",
    "    X_mlp_train, X_mlp_val = mlp_input_data[train_index], mlp_input_data[val_index]\n",
    "    X_gnn_train, X_gnn_val = gnn_output_data[train_index], gnn_output_data[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Create a fresh instance of the model for this fold\n",
    "    model = create_fusion_model()\n",
    "\n",
    "    # Train the model\n",
    "    # Use a small number of epochs for demonstration given the small dataset\n",
    "    history = model.fit(\n",
    "        [X_cnn_train, X_mlp_train, X_gnn_train],\n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        batch_size=1, # batch_size=1 is necessary for small datasets\n",
    "        verbose=0 # Turn off verbose output to keep the log clean\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    predictions = model.predict([X_cnn_val, X_mlp_val, X_gnn_val])\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    r2 = r2_score(y_val, predictions)\n",
    "    mse = mean_squared_error(y_val, predictions)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Evaluation:\")\n",
    "    print(f\"  R-squared: {r2:.4f}\")\n",
    "    print(f\"  Mean Squared Error: {mse:.4f}\")\n",
    "    \n",
    "    # Store the scores\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Calculate and print the final average scores\n",
    "print(\"\\n--- Cross-Validation Complete ---\")\n",
    "print(f\"Average R-squared across {n_splits} folds: {np.mean(r2_scores):.4f} +/- {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average Mean Squared Error across {n_splits} folds: {np.mean(mse_scores):.4f} +/- {np.std(mse_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b4c498f-6c49-4652-93e3-c08fc702c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 5-fold Cross-Validation with Simplified Model ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Fold 1 Evaluation:\n",
      "  R-squared: 0.0317\n",
      "  Mean Squared Error: 40.6759\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Fold 2 Evaluation:\n",
      "  R-squared: -2.0545\n",
      "  Mean Squared Error: 49.7319\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Fold 3 Evaluation:\n",
      "  R-squared: -0.3538\n",
      "  Mean Squared Error: 16.8414\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Fold 4 Evaluation:\n",
      "  R-squared: 0.0275\n",
      "  Mean Squared Error: 43.8860\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Fold 5 Evaluation:\n",
      "  R-squared: -0.4175\n",
      "  Mean Squared Error: 1.9751\n",
      "\n",
      "--- Cross-Validation Complete ---\n",
      "Average R-squared across 5 folds: -0.5533 +/- 0.7735\n",
      "Average Mean Squared Error across 5 folds: 30.6220 +/- 18.1811\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, Flatten\n",
    "\n",
    "# Re-using the placeholder data from the previous run.\n",
    "num_samples = 17\n",
    "cnn_input_data = np.random.rand(num_samples, 32, 32, 17)\n",
    "mlp_input_data = np.random.rand(num_samples, 34)\n",
    "gnn_output_data = np.random.rand(num_samples, 32)\n",
    "y = np.random.rand(num_samples, 1) * 20 + 5\n",
    "\n",
    "# Define the new, simplified model architecture. This factory function\n",
    "# will be called for each fold.\n",
    "def create_simplified_fusion_model():\n",
    "    \"\"\"\n",
    "    Creates a simplified fusion model with fewer parameters to prevent overfitting\n",
    "    on a small dataset.\n",
    "    \"\"\"\n",
    "    # CNN branch (simplified to just a flatten and a dense layer)\n",
    "    cnn_input = Input(shape=(32, 32, 17), name='cnn_input')\n",
    "    flatten_cnn = Flatten()(cnn_input)\n",
    "    cnn_output_simplified = Dense(16, activation='relu', name='cnn_simplified_output')(flatten_cnn)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_input = Input(shape=(34,), name='mlp_input')\n",
    "    mlp_encoder_output = Dense(16, activation='relu', name='mlp_encoder_output')(mlp_input)\n",
    "\n",
    "    # GNN output branch\n",
    "    gnn_output_for_batch = Input(shape=(32,), name='gnn_output_for_batch')\n",
    "\n",
    "    # Fusion of all three branches\n",
    "    combined = Concatenate(axis=-1)([mlp_encoder_output, cnn_output_simplified, gnn_output_for_batch])\n",
    "\n",
    "    # Simplified fusion MLP head with fewer neurons\n",
    "    dense_combined = Dense(32, activation='relu')(combined)\n",
    "    final_prediction = Dense(1, name='final_prediction')(dense_combined)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_output_for_batch], outputs=final_prediction, name='Simplified_Fusion_Model')\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the scores from each fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "\n",
    "print(f\"--- Starting {n_splits}-fold Cross-Validation with Simplified Model ---\")\n",
    "\n",
    "# The K-Fold loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(cnn_input_data)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    # Split the data for the current fold\n",
    "    X_cnn_train, X_cnn_val = cnn_input_data[train_index], cnn_input_data[val_index]\n",
    "    X_mlp_train, X_mlp_val = mlp_input_data[train_index], mlp_input_data[val_index]\n",
    "    X_gnn_train, X_gnn_val = gnn_output_data[train_index], gnn_output_data[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Create a fresh instance of the SIMPLIFIED model for this fold\n",
    "    model = create_simplified_fusion_model()\n",
    "\n",
    "    # Train the model\n",
    "    # Fewer epochs are used here as well to prevent overfitting\n",
    "    history = model.fit(\n",
    "        [X_cnn_train, X_mlp_train, X_gnn_train],\n",
    "        y_train,\n",
    "        epochs=30,\n",
    "        batch_size=1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    predictions = model.predict([X_cnn_val, X_mlp_val, X_gnn_val])\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    r2 = r2_score(y_val, predictions)\n",
    "    mse = mean_squared_error(y_val, predictions)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Evaluation:\")\n",
    "    print(f\"  R-squared: {r2:.4f}\")\n",
    "    print(f\"  Mean Squared Error: {mse:.4f}\")\n",
    "    \n",
    "    # Store the scores\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Calculate and print the final average scores\n",
    "print(\"\\n--- Cross-Validation Complete ---\")\n",
    "print(f\"Average R-squared across {n_splits} folds: {np.mean(r2_scores):.4f} +/- {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average Mean Squared Error across {n_splits} folds: {np.mean(mse_scores):.4f} +/- {np.std(mse_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98302086-29b7-40c1-af6c-8461ccb225ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e0ec4-00d5-4ebb-8a8b-3d38b0ad8a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315312b-349c-43c8-bba2-8bdd10cd8e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14caf3-f3a2-4864-829f-cb9e13487761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "537ecc40-d36b-483c-bed7-6bdfdbe7ebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 5-fold Cross-Validation for Source Apportionment ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 22.22%\n",
      "  MLP_Importance: 22.87%\n",
      "  GNN_Importance: 54.91%\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 25.31%\n",
      "  MLP_Importance: 24.93%\n",
      "  GNN_Importance: 49.76%\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 17.87%\n",
      "  MLP_Importance: 24.27%\n",
      "  GNN_Importance: 57.87%\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 10.12%\n",
      "  MLP_Importance: 21.11%\n",
      "  GNN_Importance: 68.77%\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 14.89%\n",
      "  MLP_Importance: 23.80%\n",
      "  GNN_Importance: 61.31%\n",
      "\n",
      "--- Cross-Validation Complete ---\n",
      "Average Source Apportionment across all 5 folds:\n",
      "  CNN (LULC Data): 18.08%\n",
      "  MLP (Hydrological Data): 23.39%\n",
      "  GNN (Spatial Network Data): 58.52%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Placeholder for the dataset. We'll use the same dummy data as before\n",
    "# for this demonstration. In a real-world scenario, you would load your\n",
    "# actual LULC, Hydrological, and chemical data here.\n",
    "num_samples = 17\n",
    "cnn_input_data = np.random.rand(num_samples, 32, 32, 17) # Represents LULC variations\n",
    "mlp_input_data = np.random.rand(num_samples, 34) # Represents Hydrological Properties\n",
    "gnn_output_data = np.random.rand(num_samples, 32) # Represents GNN embeddings\n",
    "y = np.random.rand(num_samples, 1) * 20 + 5 # The target heavy metal concentration\n",
    "\n",
    "def create_simplified_fusion_model():\n",
    "    \"\"\"\n",
    "    Creates a simplified fusion model with fewer parameters to prevent overfitting\n",
    "    on a small dataset.\n",
    "    \"\"\"\n",
    "    # CNN branch for LULC data\n",
    "    cnn_input = Input(shape=(32, 32, 17), name='cnn_input')\n",
    "    flatten_cnn = Flatten()(cnn_input)\n",
    "    cnn_output_simplified = Dense(16, activation='relu', name='cnn_simplified_output')(flatten_cnn)\n",
    "\n",
    "    # MLP branch for Hydrological Properties\n",
    "    mlp_input = Input(shape=(34,), name='mlp_input')\n",
    "    mlp_encoder_output = Dense(16, activation='relu', name='mlp_encoder_output')(mlp_input)\n",
    "\n",
    "    # GNN output branch for spatial network features\n",
    "    gnn_output_for_batch = Input(shape=(32,), name='gnn_output_for_batch')\n",
    "\n",
    "    # Fusion of all three branches\n",
    "    combined = Concatenate(axis=-1)([mlp_encoder_output, cnn_output_simplified, gnn_output_for_batch])\n",
    "\n",
    "    # Simplified fusion MLP head\n",
    "    dense_combined = Dense(32, activation='relu')(combined)\n",
    "    final_prediction = Dense(1, name='final_prediction')(dense_combined)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_output_for_batch], outputs=final_prediction, name='Simplified_Fusion_Model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "def analyze_feature_importance(model, cnn_input, mlp_input, gnn_output, y):\n",
    "    \"\"\"\n",
    "    Analyzes the relative importance of each input branch (CNN, MLP, GNN)\n",
    "    by training a simple linear model on the latent features.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained deep learning model.\n",
    "        cnn_input: The input data for the CNN branch.\n",
    "        mlp_input: The input data for the MLP branch.\n",
    "        gnn_output: The input data for the GNN output branch.\n",
    "        y: The target values.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the percentage importance of each branch.\n",
    "    \"\"\"\n",
    "    # Extract the latent feature models\n",
    "    latent_cnn_model = Model(inputs=model.input, outputs=model.get_layer('cnn_simplified_output').output)\n",
    "    latent_mlp_model = Model(inputs=model.input, outputs=model.get_layer('mlp_encoder_output').output)\n",
    "    \n",
    "    # Get the latent features from the trained model\n",
    "    latent_cnn_features = latent_cnn_model.predict([cnn_input, mlp_input, gnn_output])\n",
    "    latent_mlp_features = latent_mlp_model.predict([cnn_input, mlp_input, gnn_output])\n",
    "    latent_gnn_features = gnn_output  # GNN output is already a latent feature\n",
    "\n",
    "    # Combine the latent features\n",
    "    X_latent = np.concatenate([latent_cnn_features, latent_mlp_features, latent_gnn_features], axis=-1)\n",
    "\n",
    "    # Define a simple linear model to find the importance of each feature group\n",
    "    # We will use this to get the weights and interpret them as importance scores.\n",
    "    input_latent = Input(shape=(X_latent.shape[1],))\n",
    "    output_latent = Dense(1, activation='linear')(input_latent)\n",
    "    importance_model = Model(inputs=input_latent, outputs=output_latent)\n",
    "    \n",
    "    importance_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the importance model to get the weights\n",
    "    importance_model.fit(X_latent, y, epochs=100, verbose=0)\n",
    "    \n",
    "    # Extract the weights from the trained linear model\n",
    "    weights = importance_model.get_weights()[0].flatten()\n",
    "\n",
    "    # Calculate the total importance for each branch based on the weights\n",
    "    # The weights for each branch correspond to the size of its latent space\n",
    "    cnn_weight_sum = np.sum(np.abs(weights[:latent_cnn_features.shape[1]]))\n",
    "    mlp_weight_sum = np.sum(np.abs(weights[latent_cnn_features.shape[1] : latent_cnn_features.shape[1] + latent_mlp_features.shape[1]]))\n",
    "    gnn_weight_sum = np.sum(np.abs(weights[latent_cnn_features.shape[1] + latent_mlp_features.shape[1]:]))\n",
    "\n",
    "    total_weight_sum = cnn_weight_sum + mlp_weight_sum + gnn_weight_sum\n",
    "\n",
    "    # Calculate the percentage contribution of each branch\n",
    "    if total_weight_sum > 0:\n",
    "        cnn_importance = (cnn_weight_sum / total_weight_sum) * 100\n",
    "        mlp_importance = (mlp_weight_sum / total_weight_sum) * 100\n",
    "        gnn_importance = (gnn_weight_sum / total_weight_sum) * 100\n",
    "    else:\n",
    "        cnn_importance = mlp_importance = gnn_importance = 0\n",
    "        \n",
    "    return {\n",
    "        \"CNN_Importance\": cnn_importance,\n",
    "        \"MLP_Importance\": mlp_importance,\n",
    "        \"GNN_Importance\": gnn_importance\n",
    "    }\n",
    "\n",
    "\n",
    "# Set up k-fold cross-validation (same as before)\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "all_importance_scores = []\n",
    "\n",
    "print(f\"--- Starting {n_splits}-fold Cross-Validation for Source Apportionment ---\")\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(cnn_input_data)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    X_cnn_train, X_cnn_val = cnn_input_data[train_index], cnn_input_data[val_index]\n",
    "    X_mlp_train, X_mlp_val = mlp_input_data[train_index], mlp_input_data[val_index]\n",
    "    X_gnn_train, X_gnn_val = gnn_output_data[train_index], gnn_output_data[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Create and train a fresh model for this fold\n",
    "    model = create_simplified_fusion_model()\n",
    "    model.fit(\n",
    "        [X_cnn_train, X_mlp_train, X_gnn_train],\n",
    "        y_train,\n",
    "        epochs=30,\n",
    "        batch_size=1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Analyze the feature importance for this fold's model\n",
    "    importance_scores = analyze_feature_importance(model, X_cnn_val, X_mlp_val, X_gnn_val, y_val)\n",
    "    all_importance_scores.append(importance_scores)\n",
    "    \n",
    "    print(\"Source Apportionment for this Fold:\")\n",
    "    for key, value in importance_scores.items():\n",
    "        print(f\"  {key}: {value:.2f}%\")\n",
    "\n",
    "# Calculate and print the final average scores\n",
    "print(\"\\n--- Cross-Validation Complete ---\")\n",
    "avg_cnn_importance = np.mean([s['CNN_Importance'] for s in all_importance_scores])\n",
    "avg_mlp_importance = np.mean([s['MLP_Importance'] for s in all_importance_scores])\n",
    "avg_gnn_importance = np.mean([s['GNN_Importance'] for s in all_importance_scores])\n",
    "\n",
    "print(\"Average Source Apportionment across all 5 folds:\")\n",
    "print(f\"  CNN (LULC Data): {avg_cnn_importance:.2f}%\")\n",
    "print(f\"  MLP (Hydrological Data): {avg_mlp_importance:.2f}%\")\n",
    "print(f\"  GNN (Spatial Network Data): {avg_gnn_importance:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1ad2f04-b770-4bd3-bfeb-f2f7d5fa0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 5-fold Cross-Validation for Source Apportionment ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 10.14%\n",
      "  MLP_Importance: 31.61%\n",
      "  GNN_Importance: 58.25%\n",
      "\n",
      "  Top MLP Variables for this Fold:\n",
      "                   Feature  Importance\n",
      "distance_from_brick_fields    4.490038\n",
      "           soil_type_index    4.392446\n",
      "            land_use_index    4.170262\n",
      "   distance_from_factories    4.098063\n",
      "      hydrological_index_1    3.626208\n",
      "      hydrological_index_2    3.313287\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 18.98%\n",
      "  MLP_Importance: 19.39%\n",
      "  GNN_Importance: 61.63%\n",
      "\n",
      "  Top MLP Variables for this Fold:\n",
      "                   Feature  Importance\n",
      "distance_from_brick_fields    4.965214\n",
      "            land_use_index    4.744360\n",
      "      hydrological_index_2    4.739936\n",
      "   distance_from_factories    4.336761\n",
      "           soil_type_index    4.029285\n",
      "      hydrological_index_1    3.451914\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 11.69%\n",
      "  MLP_Importance: 33.11%\n",
      "  GNN_Importance: 55.20%\n",
      "\n",
      "  Top MLP Variables for this Fold:\n",
      "                   Feature  Importance\n",
      "      hydrological_index_2    5.292892\n",
      "            land_use_index    5.054132\n",
      "           soil_type_index    5.026363\n",
      "   distance_from_factories    4.699852\n",
      "distance_from_brick_fields    3.464331\n",
      "      hydrological_index_1    3.134218\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 22.59%\n",
      "  MLP_Importance: 29.34%\n",
      "  GNN_Importance: 48.07%\n",
      "\n",
      "  Top MLP Variables for this Fold:\n",
      "                   Feature  Importance\n",
      "            land_use_index    4.778516\n",
      "      hydrological_index_2    4.747976\n",
      "distance_from_brick_fields    4.631099\n",
      "   distance_from_factories    4.547859\n",
      "           soil_type_index    4.421392\n",
      "      hydrological_index_1    4.255725\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Source Apportionment for this Fold:\n",
      "  CNN_Importance: 16.55%\n",
      "  MLP_Importance: 31.88%\n",
      "  GNN_Importance: 51.57%\n",
      "\n",
      "  Top MLP Variables for this Fold:\n",
      "                   Feature  Importance\n",
      "            land_use_index    4.761050\n",
      "           soil_type_index    4.241211\n",
      "      hydrological_index_1    3.943067\n",
      "distance_from_brick_fields    3.896088\n",
      "      hydrological_index_2    3.827098\n",
      "   distance_from_factories    3.791059\n",
      "\n",
      "--- Cross-Validation Complete ---\n",
      "Average Source Apportionment across all 5 folds:\n",
      "  CNN (LULC Data): 15.99%\n",
      "  MLP (Hydrological Data): 29.07%\n",
      "  GNN (Spatial Network Data): 54.94%\n",
      "\n",
      "Average Importance of Individual MLP Variables:\n",
      "                            Importance\n",
      "Feature                               \n",
      "land_use_index                4.701664\n",
      "soil_type_index               4.422139\n",
      "hydrological_index_2          4.384238\n",
      "distance_from_factories       4.294719\n",
      "distance_from_brick_fields    4.289354\n",
      "hydrological_index_1          3.682226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# --- 1. Enhanced Placeholder Dataset with Specific Variables ---\n",
    "num_samples = 17\n",
    "\n",
    "# CNN input data: LULC variations. No change from the previous version.\n",
    "cnn_input_data = np.random.rand(num_samples, 32, 32, 17)\n",
    "\n",
    "# GNN output data: GNN embeddings. No change from the previous version.\n",
    "gnn_output_data = np.random.rand(num_samples, 32)\n",
    "\n",
    "# MLP input data: Now a pandas DataFrame with specific variable names.\n",
    "# This makes the variables explicit and easier to analyze.\n",
    "mlp_data_columns = [\n",
    "    'distance_from_factories',\n",
    "    'distance_from_brick_fields',\n",
    "    'hydrological_index_1',\n",
    "    'hydrological_index_2',\n",
    "    'land_use_index',\n",
    "    'soil_type_index'\n",
    "]\n",
    "# Create a dummy DataFrame with more specific and varied data.\n",
    "mlp_input_data = pd.DataFrame(np.random.rand(num_samples, len(mlp_data_columns)), columns=mlp_data_columns)\n",
    "\n",
    "# The target heavy metal concentration\n",
    "y = np.random.rand(num_samples, 1) * 20 + 5\n",
    "\n",
    "# --- 2. Simplified Fusion Model (Same as before) ---\n",
    "def create_simplified_fusion_model():\n",
    "    \"\"\"\n",
    "    Creates a simplified fusion model with a clear structure to aid interpretability.\n",
    "    \"\"\"\n",
    "    # CNN branch for LULC data\n",
    "    cnn_input = Input(shape=(32, 32, 17), name='cnn_input')\n",
    "    flatten_cnn = Flatten()(cnn_input)\n",
    "    cnn_output_simplified = Dense(16, activation='relu', name='cnn_simplified_output')(flatten_cnn)\n",
    "\n",
    "    # MLP branch for Hydrological Properties (now with a defined input size)\n",
    "    mlp_input = Input(shape=(mlp_input_data.shape[1],), name='mlp_input')\n",
    "    mlp_encoder_output = Dense(16, activation='relu', name='mlp_encoder_output')(mlp_input)\n",
    "\n",
    "    # GNN output branch for spatial network features\n",
    "    gnn_output_for_batch = Input(shape=(32,), name='gnn_output_for_batch')\n",
    "\n",
    "    # Fusion of all three branches\n",
    "    combined = Concatenate(axis=-1)([mlp_encoder_output, cnn_output_simplified, gnn_output_for_batch])\n",
    "\n",
    "    # Simplified fusion MLP head\n",
    "    dense_combined = Dense(32, activation='relu')(combined)\n",
    "    final_prediction = Dense(1, name='final_prediction')(dense_combined)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[cnn_input, mlp_input, gnn_output_for_batch], outputs=final_prediction, name='Simplified_Fusion_Model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- 3. Refined Importance Analysis Function ---\n",
    "def analyze_feature_importance(model, cnn_input, mlp_input, gnn_output, y, mlp_feature_names):\n",
    "    \"\"\"\n",
    "    Analyzes the relative importance of each input branch and also the\n",
    "    individual features within the MLP branch.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained deep learning model.\n",
    "        cnn_input: The input data for the CNN branch.\n",
    "        mlp_input: The input data for the MLP branch.\n",
    "        gnn_output: The input data for the GNN output branch.\n",
    "        y: The target values.\n",
    "        mlp_feature_names: A list of names for the MLP features.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the percentage importance of each branch and\n",
    "        a list of the top MLP features.\n",
    "    \"\"\"\n",
    "    # Extract the latent feature models\n",
    "    latent_cnn_model = Model(inputs=model.input, outputs=model.get_layer('cnn_simplified_output').output)\n",
    "    latent_mlp_model = Model(inputs=model.input, outputs=model.get_layer('mlp_encoder_output').output)\n",
    "    \n",
    "    # Get the latent features from the trained model\n",
    "    latent_cnn_features = latent_cnn_model.predict([cnn_input, mlp_input, gnn_output])\n",
    "    latent_mlp_features = latent_mlp_model.predict([cnn_input, mlp_input, gnn_output])\n",
    "    latent_gnn_features = gnn_output  # GNN output is already a latent feature\n",
    "\n",
    "    # Combine the latent features for the final linear model\n",
    "    X_latent = np.concatenate([latent_cnn_features, latent_mlp_features, latent_gnn_features], axis=-1)\n",
    "\n",
    "    # Define a simple linear model to find the importance of each latent feature group\n",
    "    input_latent = Input(shape=(X_latent.shape[1],))\n",
    "    output_latent = Dense(1, activation='linear')(input_latent)\n",
    "    importance_model = Model(inputs=input_latent, outputs=output_latent)\n",
    "    \n",
    "    importance_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    importance_model.fit(X_latent, y, epochs=100, verbose=0)\n",
    "    \n",
    "    weights = importance_model.get_weights()[0].flatten()\n",
    "\n",
    "    # Calculate the total importance for each branch\n",
    "    cnn_weight_sum = np.sum(np.abs(weights[:latent_cnn_features.shape[1]]))\n",
    "    mlp_weight_sum = np.sum(np.abs(weights[latent_cnn_features.shape[1] : latent_cnn_features.shape[1] + latent_mlp_features.shape[1]]))\n",
    "    gnn_weight_sum = np.sum(np.abs(weights[latent_cnn_features.shape[1] + latent_mlp_features.shape[1]:]))\n",
    "\n",
    "    total_weight_sum = cnn_weight_sum + mlp_weight_sum + gnn_weight_sum\n",
    "\n",
    "    if total_weight_sum > 0:\n",
    "        cnn_importance = (cnn_weight_sum / total_weight_sum) * 100\n",
    "        mlp_importance = (mlp_weight_sum / total_weight_sum) * 100\n",
    "        gnn_importance = (gnn_weight_sum / total_weight_sum) * 100\n",
    "    else:\n",
    "        cnn_importance = mlp_importance = gnn_importance = 0\n",
    "    \n",
    "    # --- New section: Analyze individual MLP feature importance ---\n",
    "    # We get the weights of the first dense layer of the MLP branch.\n",
    "    # The magnitude of these weights indicates how strongly each input feature\n",
    "    # influences the hidden neurons of that branch.\n",
    "    mlp_weights = model.get_layer('mlp_encoder_output').get_weights()[0]\n",
    "    \n",
    "    # Sum the absolute values of the weights for each input feature.\n",
    "    # The result is a single importance score for each original MLP feature.\n",
    "    feature_importance_scores = np.sum(np.abs(mlp_weights), axis=1)\n",
    "\n",
    "    # Create a DataFrame for easy sorting and display\n",
    "    mlp_importance_df = pd.DataFrame({\n",
    "        'Feature': mlp_feature_names,\n",
    "        'Importance': feature_importance_scores\n",
    "    })\n",
    "\n",
    "    # Sort the features by their importance\n",
    "    top_mlp_features = mlp_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        \"CNN_Importance\": cnn_importance,\n",
    "        \"MLP_Importance\": mlp_importance,\n",
    "        \"GNN_Importance\": gnn_importance,\n",
    "        \"Top_MLP_Features\": top_mlp_features\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 4. Main execution loop with new analysis function ---\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "all_importance_scores = []\n",
    "all_mlp_importance_dfs = []\n",
    "\n",
    "print(f\"--- Starting {n_splits}-fold Cross-Validation for Source Apportionment ---\")\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(cnn_input_data)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    X_cnn_train, X_cnn_val = cnn_input_data[train_index], cnn_input_data[val_index]\n",
    "    X_mlp_train, X_mlp_val = mlp_input_data.iloc[train_index], mlp_input_data.iloc[val_index]\n",
    "    X_gnn_train, X_gnn_val = gnn_output_data[train_index], gnn_output_data[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    model = create_simplified_fusion_model()\n",
    "    model.fit(\n",
    "        [X_cnn_train, X_mlp_train, X_gnn_train],\n",
    "        y_train,\n",
    "        epochs=30,\n",
    "        batch_size=1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    importance_scores = analyze_feature_importance(\n",
    "        model, \n",
    "        X_cnn_val, \n",
    "        X_mlp_val, \n",
    "        X_gnn_val, \n",
    "        y_val, \n",
    "        mlp_input_data.columns.tolist()\n",
    "    )\n",
    "    all_importance_scores.append(importance_scores)\n",
    "    all_mlp_importance_dfs.append(importance_scores['Top_MLP_Features'])\n",
    "    \n",
    "    print(\"Source Apportionment for this Fold:\")\n",
    "    for key, value in importance_scores.items():\n",
    "        if key != \"Top_MLP_Features\":\n",
    "            print(f\"  {key}: {value:.2f}%\")\n",
    "    print(\"\\n  Top MLP Variables for this Fold:\")\n",
    "    print(importance_scores['Top_MLP_Features'].to_string(index=False))\n",
    "\n",
    "# Calculate and print the final average scores\n",
    "print(\"\\n--- Cross-Validation Complete ---\")\n",
    "avg_cnn_importance = np.mean([s['CNN_Importance'] for s in all_importance_scores])\n",
    "avg_mlp_importance = np.mean([s['MLP_Importance'] for s in all_importance_scores])\n",
    "avg_gnn_importance = np.mean([s['GNN_Importance'] for s in all_importance_scores])\n",
    "\n",
    "# Average the MLP feature importances across all folds\n",
    "avg_mlp_importance_df = pd.concat(all_mlp_importance_dfs).groupby('Feature').mean().sort_values(by='Importance', ascending=False)\n",
    "\n",
    "\n",
    "print(\"Average Source Apportionment across all 5 folds:\")\n",
    "print(f\"  CNN (LULC Data): {avg_cnn_importance:.2f}%\")\n",
    "print(f\"  MLP (Hydrological Data): {avg_mlp_importance:.2f}%\")\n",
    "print(f\"  GNN (Spatial Network Data): {avg_gnn_importance:.2f}%\")\n",
    "\n",
    "print(\"\\nAverage Importance of Individual MLP Variables:\")\n",
    "print(avg_mlp_importance_df.to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e99167-6b92-44ac-afbe-3d29c872a539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
